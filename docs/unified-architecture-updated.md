# Cyclical Learning Agentic AI Platform
## Talos Bare Metal + Local/Cloud Hybrid + Vector Knowledge Base

---

## Executive Summary

This architecture delivers a **self-improving homelab AI agent** that learns from every interaction, builds institutional knowledge, and progressively earns autonomy. It combines local inference for speed and privacy with cloud escalation for complex reasoning, unified by a vector knowledge base that enables genuine learning.

**Core Capabilities:**
- **Flexible inference**: Local Ollama, Cloud Gemini/Claude, or hybrid - switchable per-request
- **Persistent knowledge**: Vector DB stores decisions, outcomes, and learnings
- **Cyclical learning**: Every action feeds back into the knowledge base
- **Progressive autonomy**: System earns trust through demonstrated reliability
- **Human-in-the-loop**: Telegram Forum with topic-based organization and inline keyboard approvals

**Philosophy**: The AI doesn't just execute tasksâ€”it remembers what worked, learns your preferences, and gets smarter over time.

---

## Part I: The Learning Loop

### The Cyclical Intelligence Model

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         THE LEARNING CYCLE                                â”‚
â”‚                                                                           â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚    â”‚  DETECT  â”‚                                      â”‚  LEARN   â”‚        â”‚
â”‚    â”‚  Event   â”‚                                      â”‚  Update  â”‚        â”‚
â”‚    â”‚  occurs  â”‚                                      â”‚  vectors â”‚        â”‚
â”‚    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                                      â””â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”˜        â”‚
â”‚         â”‚                                                 â”‚              â”‚
â”‚         â–¼                                                 â”‚              â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚              â”‚
â”‚    â”‚ RETRIEVE â”‚      â”‚  REASON  â”‚      â”‚   ACT    â”‚      â”‚              â”‚
â”‚    â”‚ Similar  â”‚â”€â”€â”€â”€â”€â–¶â”‚  Decide  â”‚â”€â”€â”€â”€â”€â–¶â”‚ Execute  â”‚â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚    â”‚ contexts â”‚      â”‚  action  â”‚      â”‚  + log   â”‚                     â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚         â–²                                                                â”‚
â”‚         â”‚                                                                â”‚
â”‚    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”                                                         â”‚
â”‚    â”‚  VECTOR  â”‚  â—„â”€â”€ Runbooks, decisions, outcomes, preferences         â”‚
â”‚    â”‚    DB    â”‚  â—„â”€â”€ System state snapshots, documentation              â”‚
â”‚    â”‚ (Qdrant) â”‚  â—„â”€â”€ Conversation history, human feedback               â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                         â”‚
â”‚                                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### What Gets Learned

| Knowledge Type | Source | How It's Used |
|----------------|--------|---------------|
| **Runbooks** | Approved fixes | "I've seen this before, solution X worked" |
| **Outcomes** | Post-action monitoring | "Last time this fix caused Y" |
| **Preferences** | Human approvals/rejections | "Charlie prefers permanent fixes" |
| **Context** | System state at decision time | "This happened during high load" |
| **Documentation** | Ingested docs, READMEs | "The Sonarr API works like this" |
| **Conversations** | Past interactions | "We discussed this architecture before" |

---

## Part II: Hybrid Inference Layer

### Inference Modes

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        INFERENCE MODE SELECTION                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚ LOCAL_FIRST â”‚     â”‚ CLOUD_ONLY  â”‚     â”‚ LOCAL_ONLY  â”‚                â”‚
â”‚  â”‚  (Default)  â”‚     â”‚  (Bypass)   â”‚     â”‚  (Offline)  â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚         â”‚                   â”‚                   â”‚                        â”‚
â”‚         â–¼                   â–¼                   â–¼                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚   Ollama    â”‚     â”‚   Gemini    â”‚     â”‚   Ollama    â”‚                â”‚
â”‚  â”‚  qwen2.5:7b â”‚     â”‚    Flash    â”‚     â”‚  qwen2.5:7b â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚         â”‚                                                                â”‚
â”‚         â–¼                                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                        â”‚
â”‚  â”‚ Confidence  â”‚                                                        â”‚
â”‚  â”‚   < 0.7?    â”‚                                                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                                        â”‚
â”‚    Yes  â”‚  No                                                           â”‚
â”‚         â–¼                                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”‚
â”‚  â”‚  Escalate   â”‚     â”‚   Return    â”‚                                    â”‚
â”‚  â”‚  to Cloud   â”‚     â”‚   Result    â”‚                                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Mode Configuration

| Mode | Behavior | Use Case |
|------|----------|----------|
| `local_first` | Try Ollama â†’ escalate if confidence < threshold | Default operation |
| `cloud_only` | Skip local, go directly to Gemini/Claude | Complex tasks, bypass local |
| `local_only` | Never use cloud APIs | Offline, privacy-critical |
| `cloud_first` | Try cloud â†’ fallback to local if API fails | When quality is priority |

### Mode Switching

```bash
# Set globally via ConfigMap
kubectl patch configmap ai-config -n ai-platform \
  --patch '{"data": {"INFERENCE_MODE": "local_first"}}'

# Override per-request via header
curl -X POST http://langgraph:8000/invoke \
  -H "X-Inference-Mode: cloud_only" \
  -d '{"query": "complex architecture question"}'

# Telegram command to switch mode
# In any topic, send: /mode cloud_only
```

### LiteLLM Routing Configuration

```yaml
# litellm-config.yaml
model_list:
  # Local Models (Ollama)
  - model_name: local/qwen2.5:7b
    litellm_params:
      model: ollama/qwen2.5:7b
      api_base: http://ollama:11434
      
  - model_name: local/qwen2.5:3b
    litellm_params:
      model: ollama/qwen2.5:3b
      api_base: http://ollama:11434

  # Cloud Models (Gemini - Primary)
  - model_name: cloud/gemini-flash
    litellm_params:
      model: gemini/gemini-2.0-flash
      api_key: os.environ/GEMINI_API_KEY
      
  - model_name: cloud/gemini-pro
    litellm_params:
      model: gemini/gemini-1.5-pro
      api_key: os.environ/GEMINI_API_KEY

  # Cloud Models (Claude - Premium)
  - model_name: cloud/claude-sonnet
    litellm_params:
      model: anthropic/claude-sonnet-4-20250514
      api_key: os.environ/ANTHROPIC_API_KEY

router_settings:
  routing_strategy: simple-shuffle
  redis_host: redis
  redis_port: 6379
```

### Inference Router Logic

```python
"""Inference router with mode selection and confidence-based escalation."""

from enum import Enum
from dataclasses import dataclass
import litellm

class InferenceMode(Enum):
    LOCAL_FIRST = "local_first"
    CLOUD_ONLY = "cloud_only"
    LOCAL_ONLY = "local_only"
    CLOUD_FIRST = "cloud_first"

@dataclass
class InferenceResult:
    response: str
    model_used: str
    confidence: float
    escalated: bool

class InferenceRouter:
    """Routes inference requests based on mode and confidence."""
    
    def __init__(
        self,
        local_model: str = "local/qwen2.5:7b",
        cloud_model: str = "cloud/gemini-flash",
        escalation_threshold: float = 0.7,
        default_mode: InferenceMode = InferenceMode.LOCAL_FIRST
    ):
        self.local_model = local_model
        self.cloud_model = cloud_model
        self.threshold = escalation_threshold
        self.default_mode = default_mode
    
    async def infer(
        self,
        prompt: str,
        mode: InferenceMode = None,
        force_model: str = None
    ) -> InferenceResult:
        """Execute inference with mode-based routing."""
        
        mode = mode or self.default_mode
        
        # Direct model override
        if force_model:
            return await self._call_model(force_model, prompt)
        
        # Mode-based routing
        if mode == InferenceMode.CLOUD_ONLY:
            return await self._call_model(self.cloud_model, prompt)
        
        if mode == InferenceMode.LOCAL_ONLY:
            return await self._call_model(self.local_model, prompt)
        
        if mode == InferenceMode.CLOUD_FIRST:
            try:
                return await self._call_model(self.cloud_model, prompt)
            except Exception:
                return await self._call_model(self.local_model, prompt)
        
        # LOCAL_FIRST (default): Try local, escalate if needed
        result = await self._call_model(self.local_model, prompt)
        
        if result.confidence < self.threshold:
            cloud_result = await self._call_model(self.cloud_model, prompt)
            cloud_result.escalated = True
            return cloud_result
        
        return result
    
    async def _call_model(self, model: str, prompt: str) -> InferenceResult:
        """Call a specific model via LiteLLM."""
        response = await litellm.acompletion(
            model=model,
            messages=[{"role": "user", "content": prompt}]
        )
        
        # Extract confidence from response metadata or estimate
        confidence = self._estimate_confidence(response)
        
        return InferenceResult(
            response=response.choices[0].message.content,
            model_used=model,
            confidence=confidence,
            escalated=False
        )
    
    def _estimate_confidence(self, response) -> float:
        """Estimate confidence based on response characteristics."""
        text = response.choices[0].message.content.lower()
        
        # Low confidence indicators
        uncertain_phrases = [
            "i'm not sure", "i think", "possibly", "might be",
            "i don't know", "unclear", "uncertain"
        ]
        
        if any(phrase in text for phrase in uncertain_phrases):
            return 0.5
        
        # Check for hedging
        if text.count("maybe") > 1 or text.count("perhaps") > 1:
            return 0.6
        
        return 0.85  # Default confidence
```

---

## Part III: Human-in-the-Loop Layer

### Telegram Forum Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      TELEGRAM FORUM STRUCTURE                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  ğŸ  Homelab Ops (Forum Supergroup)                                       â”‚
â”‚  â”œâ”€â”€ ğŸ“Œ General                    # Default topic                       â”‚
â”‚  â”œâ”€â”€ ğŸ”´ Critical Alerts            # Standing - high priority            â”‚
â”‚  â”œâ”€â”€ ğŸŸ¡ Arr Suite                  # Standing - *arr domain              â”‚
â”‚  â”œâ”€â”€ ğŸ”µ Infrastructure             # Standing - K8s/storage/network      â”‚
â”‚  â”œâ”€â”€ ğŸ  Home Assistant             # Standing - HA domain                â”‚
â”‚  â”œâ”€â”€ ğŸ“Š Weekly Reports             # Standing - scheduled digests        â”‚
â”‚  â”œâ”€â”€ ğŸ”§ Incident #47               # Dynamic - agent-created             â”‚
â”‚  â””â”€â”€ âœ… Resolved                    # Standing - archive                  â”‚
â”‚                                                                          â”‚
â”‚  Agent Capabilities:                                                     â”‚
â”‚  â€¢ Create/close/reopen topics dynamically                                â”‚
â”‚  â€¢ Route messages to appropriate topics based on domain                  â”‚
â”‚  â€¢ Present inline keyboard buttons for approvals                         â”‚
â”‚  â€¢ Track conversation context per topic                                  â”‚
â”‚  â€¢ Learn routing patterns from human corrections                         â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Approval Workflow

```
Detection (Coroot/Prometheus/Renovate)
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ  Homelab Ops Forum                                            â”‚
â”‚  â””â”€â”€ ğŸŸ¡ Arr Suite                                                â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚      â”‚ ğŸ”” Radarr memory at 95%                                 â”‚â”‚
â”‚      â”‚                                                          â”‚â”‚
â”‚      â”‚ Similar to: runbook-mem-001 (89%)                       â”‚â”‚
â”‚      â”‚ Model: local/qwen2.5:7b (confidence: 0.82)              â”‚â”‚
â”‚      â”‚                                                          â”‚â”‚
â”‚      â”‚ Solutions:                                               â”‚â”‚
â”‚      â”‚ 1. Increase memory 1GB â†’ 2GB                            â”‚â”‚
â”‚      â”‚ 2. Restart pod (temporary)                              â”‚â”‚
â”‚      â”‚ 3. Investigate leak (diagnostic)                        â”‚â”‚
â”‚      â”‚                                                          â”‚â”‚
â”‚      â”‚ [1ï¸âƒ£ Increase] [2ï¸âƒ£ Restart] [3ï¸âƒ£ Investigate]            â”‚â”‚
â”‚      â”‚ [âŒ Ignore]   [ğŸ” Details] [â˜ï¸ Re-analyze]              â”‚â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ Button: [1ï¸âƒ£ Increase]
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Execute via MCP infrastructure tools                            â”‚
â”‚  kubectl patch deployment/radarr ...                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸŸ¡ Arr Suite                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ âœ… Radarr memory increased to 2GB                           â”‚â”‚
â”‚  â”‚                                                              â”‚â”‚
â”‚  â”‚ Status: Healthy (15 min)                                    â”‚â”‚
â”‚  â”‚ Memory: 45% (was 95%)                                       â”‚â”‚
â”‚  â”‚                                                              â”‚â”‚
â”‚  â”‚ ğŸ“š Runbook updated: runbook-mem-001                         â”‚â”‚
â”‚  â”‚ Next occurrence: [ğŸ”„ Auto] [â“ Ask] [ğŸ” Investigate]         â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Telegram Commands

| Input | Action |
|-------|--------|
| Button: `1ï¸âƒ£`, `2ï¸âƒ£`, `3ï¸âƒ£` | Select numbered solution |
| Button: `Approve` | Approve current action/PR |
| Button: `Ignore` | Take no action, record preference |
| Button: `Details` | Show extended diagnostics |
| Button: `â˜ï¸ Re-analyze` | Bypass local, re-run with cloud model |
| Text: `status` | Show pending approvals |
| Text: `weekly` | Trigger weekly report now |
| Text: `/mode cloud_only` | Switch inference mode |
| Text: `custom: restart all arr` | Execute custom instruction |

### Topic Routing Rules

```yaml
# config/telegram-topics.yaml
standing_topics:
  - key: critical
    name: "ğŸ”´ Critical Alerts"
    route_when:
      - severity: critical
      - alertname: ".*OOM.*|.*Crash.*|.*Down.*"
      
  - key: arr_suite
    name: "ğŸŸ¡ Arr Suite"
    domains: ["sonarr", "radarr", "prowlarr", "sabnzbd", "plex"]
    
  - key: infrastructure
    name: "ğŸ”µ Infrastructure"
    domains: ["k8s", "argocd", "storage", "network", "proxmox", "talos"]
    
  - key: home_assistant
    name: "ğŸ  Home Assistant"
    domains: ["homeassistant", "tasmota", "mqtt", "zigbee"]
    
  - key: weekly_reports
    name: "ğŸ“Š Weekly Reports"
    
  - key: resolved
    name: "âœ… Resolved"

dynamic_topics:
  incident:
    name_template: "ğŸ”§ {title} #{id}"
    create_when:
      - severity: critical
      - estimated_complexity: high
    auto_close_after_hours: 168
    archive_to: resolved
```

---

## Part IV: Vector Knowledge Base

### Qdrant Collections

```python
# Collection schemas for learning system

collections = {
    "runbooks": {
        "vector_size": 768,  # nomic-embed-text
        "distance": "Cosine",
        "payload_schema": {
            "id": "keyword",
            "trigger_pattern": "text",
            "solution": "text",
            "success_rate": "float",
            "approval_count": "integer",
            "automation_level": "keyword",  # manual, prompted, standard
            "last_used": "datetime"
        }
    },
    
    "decisions": {
        "vector_size": 768,
        "distance": "Cosine", 
        "payload_schema": {
            "trigger": "text",
            "context": "text",
            "action_taken": "text",
            "outcome": "keyword",  # success, failure, partial
            "human_feedback": "keyword",  # approved, rejected, modified
            "model_used": "keyword",
            "confidence": "float",
            "timestamp": "datetime"
        }
    },
    
    "documentation": {
        "vector_size": 768,
        "distance": "Cosine",
        "payload_schema": {
            "source": "keyword",
            "title": "text",
            "content": "text",
            "doc_type": "keyword",  # readme, api, config, runbook
            "last_updated": "datetime"
        }
    }
}
```

### RAG Pipeline

```python
"""Retrieval-Augmented Generation for context-aware decisions."""

class KnowledgeRetriever:
    """Retrieves relevant context from vector DB."""
    
    async def get_context(self, query: str, alert: dict) -> dict:
        """Retrieve all relevant context for a decision."""
        
        # Embed the query
        query_vector = await self.embed(query)
        
        # Search each collection
        runbooks = await self.qdrant.search(
            collection_name="runbooks",
            query_vector=query_vector,
            limit=3,
            query_filter=Filter(
                must=[FieldCondition(
                    key="automation_level",
                    match=MatchAny(any=["manual", "prompted"])
                )]
            )
        )
        
        past_decisions = await self.qdrant.search(
            collection_name="decisions",
            query_vector=query_vector,
            limit=5,
            query_filter=Filter(
                must=[FieldCondition(
                    key="outcome",
                    match=MatchValue(value="success")
                )]
            )
        )
        
        documentation = await self.qdrant.search(
            collection_name="documentation",
            query_vector=query_vector,
            limit=3
        )
        
        return {
            "runbooks": [r.payload for r in runbooks],
            "past_decisions": [d.payload for d in past_decisions],
            "documentation": [d.payload for d in documentation],
            "query": query
        }
```

---

## Part V: MCP Server Layer

### Server Architecture

```
LangGraph Orchestrator
         â”‚
         â”œâ”€â”€â–¶ home-assistant-mcp â”€â”€â–¶ Home Assistant API
         â”‚                           â”œâ”€â”€ Lights (Tasmota)
         â”‚                           â”œâ”€â”€ Climate
         â”‚                           â””â”€â”€ Automations
         â”‚
         â”œâ”€â”€â–¶ arr-suite-mcp â”€â”€â–¶ *arr APIs
         â”‚                      â”œâ”€â”€ Sonarr
         â”‚                      â”œâ”€â”€ Radarr
         â”‚                      â”œâ”€â”€ Prowlarr
         â”‚                      â””â”€â”€ SABnzbd
         â”‚
         â”œâ”€â”€â–¶ infrastructure-mcp â”€â”€â–¶ K8s / System
         â”‚                           â”œâ”€â”€ kubectl
         â”‚                           â”œâ”€â”€ ArgoCD
         â”‚                           â””â”€â”€ Proxmox
         â”‚
         â”œâ”€â”€â–¶ network-mcp â”€â”€â–¶ Network Infra
         â”‚                    â”œâ”€â”€ OPNsense
         â”‚                    â”œâ”€â”€ Cloudflare
         â”‚                    â””â”€â”€ UniFi
         â”‚
         â””â”€â”€â–¶ knowledge-mcp â”€â”€â–¶ Knowledge Base
                               â”œâ”€â”€ Qdrant search
                               â”œâ”€â”€ Runbook lookup
                               â””â”€â”€ Documentation
```

---

## Part VI: Observability & Learning Dashboard

### Key Metrics

```yaml
metrics:
  # Inference metrics
  - inference_requests_total{model, mode, escalated}
  - inference_latency_seconds{model}
  - inference_confidence_histogram{model}
  
  # Decision metrics  
  - decisions_total{outcome, human_feedback}
  - decision_response_time_seconds
  
  # Learning metrics
  - runbooks_total{automation_level}
  - runbook_promotions_total
  - knowledge_base_size{collection}
  
  # Telegram metrics
  - telegram_messages_sent{topic}
  - telegram_callbacks_received{action}
  - telegram_response_time_seconds
```

### Weekly Report (Telegram ğŸ“Š Topic)

```
ğŸ“Š Weekly Homelab Report (Dec 23-30)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“ˆ INFERENCE SUMMARY
â”œâ”€â”€ Local (qwen2.5:7b): 142 calls (78%)
â”œâ”€â”€ Cloud escalations: 28 calls (15%)
â”œâ”€â”€ Cloud bypass: 12 calls (7%)
â””â”€â”€ Avg local confidence: 0.81

âš¡ ACTIONS TAKEN
â”œâ”€â”€ Auto-executed (standard): 8
â”‚   â””â”€â”€ Pod restarts (3), cache clears (5)
â”œâ”€â”€ With approval: 12
â”‚   â””â”€â”€ Memory increases (4), PR merges (6), config (2)
â””â”€â”€ Declined: 2
    â””â”€â”€ Suggested changes you said no to

ğŸ“ LEARNING
â”œâ”€â”€ New runbooks: 3
â”œâ”€â”€ Promoted to standard: 1
â”œâ”€â”€ Patterns learned: 5
â””â”€â”€ Total knowledge items: 247

ğŸ“± TELEGRAM ACTIVITY
â”œâ”€â”€ Messages sent: 89
â”œâ”€â”€ Topics created: 2
â”œâ”€â”€ Avg response time: 4.2 min
â””â”€â”€ Most active: ğŸŸ¡ Arr Suite (34 msgs)

ğŸ”® NEXT WEEK
â”œâ”€â”€ SSL renewal due Dec 31
â”œâ”€â”€ Monthly UniFi check Jan 1
â””â”€â”€ 2 PRs awaiting review

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## Part VII: Implementation Timeline

### Phase 1: Infrastructure (Week 1)
- [ ] Talos cluster provisioned via Terraform
- [ ] Storage configured (Mayastor/local-path)
- [ ] ArgoCD + SOPS deployed
- [ ] Renovate configured on GitOps repo

### Phase 2: Inference Layer (Week 2)
- [ ] Deploy Ollama with qwen2.5:7b
- [ ] Deploy LiteLLM with hybrid routing config
- [ ] Test local inference
- [ ] Configure cloud API keys (Gemini, Claude)
- [ ] Test escalation logic

### Phase 3: Knowledge Base (Week 3)
- [ ] Deploy Qdrant
- [ ] Create collection schemas
- [ ] Deploy nomic-embed-text model
- [ ] Build embedding pipeline
- [ ] Ingest initial documentation

### Phase 4: MCP Servers (Week 4)
- [ ] Build home-assistant-mcp
- [ ] Build infrastructure-mcp
- [ ] Build arr-suite-mcp
- [ ] Build knowledge-mcp
- [ ] Test tool execution

### Phase 5: Human-in-the-Loop (Week 5)
- [ ] Create Telegram bot via @BotFather
- [ ] Create Forum supergroup
- [ ] Deploy telegram-service
- [ ] Initialize standing topics
- [ ] Register webhook
- [ ] Test approval workflow with inline keyboards

### Phase 6: Observability (Week 6)
- [ ] Deploy Prometheus + Grafana
- [ ] Deploy Coroot
- [ ] Build learning dashboard
- [ ] Configure alert â†’ AI pipeline

### Phase 7: Go Live (Week 7)
- [ ] Enable VERBOSE MODE (ask everything)
- [ ] Monitor all decisions
- [ ] Tune confidence thresholds
- [ ] Create initial runbooks from approvals

### Phase 8: Progressive Autonomy (Month 2+)
- [ ] Track approval patterns
- [ ] Promote high-success runbooks to standard
- [ ] Reduce notification frequency
- [ ] Quarterly trust reviews

---

## Appendix: Configuration Reference

### Environment Variables

```bash
# Ollama (Local)
OLLAMA_HOST=http://ollama:11434
OLLAMA_KEEP_ALIVE=10m

# Cloud APIs
GEMINI_API_KEY=your-key
ANTHROPIC_API_KEY=your-key

# Inference Routing
INFERENCE_MODE=local_first          # local_first, cloud_only, local_only, cloud_first
LOCAL_CONFIDENCE_THRESHOLD=0.7
CLOUD_ESCALATION_MODEL=cloud/gemini-flash

# Telegram
TELEGRAM_BOT_TOKEN=your-bot-token
TELEGRAM_FORUM_CHAT_ID=-100xxxxxxxxxx
TELEGRAM_WEBHOOK_URL=https://telegram-webhook.yourdomain.com/webhook

# Learning
PROMOTION_MIN_APPROVALS=5
PROMOTION_MIN_SUCCESS_RATE=0.95
```

### Port Reference

| Service | Port | Protocol |
|---------|------|----------|
| Ollama | 11434 | HTTP |
| Qdrant REST | 6333 | HTTP |
| Qdrant gRPC | 6334 | gRPC |
| LiteLLM | 4000 | HTTP |
| LangGraph | 8000 | HTTP |
| Telegram Service | 8080 | HTTP |
| MCP Servers | 8001-8010 | HTTP/SSE |
| Prometheus | 9090 | HTTP |
| Grafana | 3000 | HTTP |
| Coroot | 8081 | HTTP |
| ArgoCD | 8443 | HTTPS |

---

## Conclusion

This architecture creates a **genuinely learning system** that:

1. **Remembers** every decision, outcome, and your feedback
2. **Retrieves** relevant context before acting via RAG
3. **Reasons** using local models first, cloud when needed
4. **Acts** through MCP tools with human approval via Telegram
5. **Learns** from outcomes and updates its knowledge

**The key insight**: Autonomy is earned, not configured. The system starts by asking permission for everything, then gradually proves it can be trusted with more independence.

**Flexibility built-in**: Switch between local-only, cloud-only, or hybrid inference based on your needsâ€”privacy, offline operation, or maximum quality. Override per-request when needed.

**The learning loop is the differentiator**: Every interaction makes the system smarter. In six months, it will know your homelab better than any generic AI ever could.
