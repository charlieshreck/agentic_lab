apiVersion: v1
kind: ConfigMap
metadata:
  name: knowledge-mcp-code
  namespace: ai-platform
  labels:
    app: knowledge-mcp
data:
  main.py: |
    #!/usr/bin/env python3
    """Knowledge MCP server for Qdrant vector database operations."""
    import os
    import logging
    import httpx
    from typing import List, Optional
    from datetime import datetime, timezone
    from fastmcp import FastMCP
    from pydantic import BaseModel
    from starlette.applications import Starlette
    from starlette.routing import Route, Mount
    from starlette.responses import JSONResponse
    from starlette.requests import Request
    import uvicorn

    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

    QDRANT_URL = os.environ.get("QDRANT_URL", "http://qdrant:6333")
    LITELLM_URL = os.environ.get("LITELLM_URL", "http://litellm:4000")
    EMBEDDING_MODEL = os.environ.get("EMBEDDING_MODEL", "embeddings")

    mcp = FastMCP(name="knowledge-mcp", instructions="MCP server for knowledge base operations.")

    class SearchResult(BaseModel):
        id: str
        score: float
        title: str
        content: str

    async def get_embedding(text: str) -> List[float]:
        """Get embeddings via LiteLLM (Gemini text-embedding-004)."""
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                f"{LITELLM_URL}/v1/embeddings",
                json={"model": EMBEDDING_MODEL, "input": text}
            )
            response.raise_for_status()
            return response.json()["data"][0]["embedding"]

    async def qdrant_search(collection: str, vector: List[float], limit: int = 5) -> List[dict]:
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(f"{QDRANT_URL}/collections/{collection}/points/search", json={"vector": vector, "limit": limit, "with_payload": True})
            response.raise_for_status()
            return response.json().get("result", [])

    async def qdrant_upsert(collection: str, points: List[dict]) -> bool:
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.put(f"{QDRANT_URL}/collections/{collection}/points", json={"points": points})
            return response.status_code == 200

    @mcp.tool()
    async def search_runbooks(query: str, limit: int = 5) -> List[SearchResult]:
        """Search runbooks for solutions to issues."""
        try:
            vector = await get_embedding(query)
            results = await qdrant_search("runbooks", vector, limit)
            return [SearchResult(id=str(r.get("id")), score=r.get("score", 0), title=r.get("payload", {}).get("title", ""), content=r.get("payload", {}).get("solution", "")) for r in results if r.get("score", 0) >= 0.7]
        except Exception as e:
            logger.error(f"Search failed: {e}")
            return []

    @mcp.tool()
    async def search_documentation(query: str, limit: int = 5) -> List[SearchResult]:
        """Search documentation for information."""
        try:
            vector = await get_embedding(query)
            results = await qdrant_search("documentation", vector, limit)
            return [SearchResult(id=str(r.get("id")), score=r.get("score", 0), title=r.get("payload", {}).get("title", ""), content=r.get("payload", {}).get("content", "")) for r in results]
        except Exception as e:
            logger.error(f"Search failed: {e}")
            return []

    @mcp.tool()
    async def add_runbook(title: str, trigger_pattern: str, solution: str) -> dict:
        """Add a new runbook to the knowledge base."""
        try:
            import uuid
            point_id = str(uuid.uuid4())
            vector = await get_embedding(f"{title}\n{solution}")
            point = {"id": point_id, "vector": vector, "payload": {"title": title, "trigger_pattern": trigger_pattern, "solution": solution, "created_at": datetime.now(timezone.utc).isoformat()}}
            success = await qdrant_upsert("runbooks", [point])
            return {"success": success, "id": point_id}
        except Exception as e:
            return {"success": False, "error": str(e)}

    @mcp.tool()
    async def get_similar_events(event_description: str, limit: int = 5) -> List[SearchResult]:
        """Find similar historical events."""
        try:
            vector = await get_embedding(event_description)
            results = await qdrant_search("agent_events", vector, limit)
            return [SearchResult(id=str(r.get("id")), score=r.get("score", 0), title=r.get("payload", {}).get("event_type", ""), content=r.get("payload", {}).get("description", "")) for r in results if r.get("score", 0) >= 0.75]
        except Exception as e:
            logger.error(f"Search failed: {e}")
            return []

    # ============================================================================
    # REST API (for langgraph context building)
    # ============================================================================

    async def rest_health(request: Request):
        """Health check endpoint."""
        return JSONResponse({"status": "healthy"})

    async def rest_api_search(request: Request):
        """Search knowledge base for langgraph context."""
        try:
            query = request.query_params.get("q", "")
            collection = request.query_params.get("collection", "runbooks")
            limit = int(request.query_params.get("limit", "5"))
            if not query:
                return JSONResponse({"status": "error", "error": "Missing query parameter 'q'"}, status_code=400)
            vector = await get_embedding(query)
            results = await qdrant_search(collection, vector, limit)
            return JSONResponse({"status": "ok", "data": results, "count": len(results)})
        except Exception as e:
            logger.error(f"REST api_search error: {e}")
            return JSONResponse({"status": "error", "error": str(e)}, status_code=500)

    # ============================================================================
    # MAIN
    # ============================================================================

    def main():
        port = int(os.environ.get("PORT", "8000"))
        logger.info(f"Starting knowledge MCP on port {port}")

        rest_routes = [
            Route("/health", rest_health, methods=["GET"]),
            Route("/api/search", rest_api_search, methods=["GET"]),
        ]

        mcp_app = mcp.http_app()
        app = Starlette(routes=rest_routes + [Mount("/mcp", app=mcp_app)])
        uvicorn.run(app, host="0.0.0.0", port=port)

    if __name__ == "__main__":
        main()

  requirements.txt: |
    fastmcp>=2.7.0
    pydantic>=2.11.0
    httpx>=0.28.0
    uvicorn>=0.34.0
    starlette>=0.40.0
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: knowledge-mcp
  namespace: ai-platform
  labels:
    app: knowledge-mcp
    component: mcp
spec:
  replicas: 1
  selector:
    matchLabels:
      app: knowledge-mcp
  template:
    metadata:
      labels:
        app: knowledge-mcp
        component: mcp
    spec:
      initContainers:
        - name: install-deps
          image: python:3.11-slim
          command: ['sh', '-c', 'pip install --target=/app/deps -r /code/requirements.txt']
          volumeMounts:
            - name: code
              mountPath: /code
            - name: deps
              mountPath: /app/deps
      containers:
        - name: mcp-server
          image: python:3.11-slim
          command: ['sh', '-c', 'cd /app && PYTHONPATH=/app/deps python /code/main.py']
          ports:
            - containerPort: 8000
              name: http
          env:
            - name: PORT
              value: "8000"
            - name: QDRANT_URL
              value: "http://qdrant:6333"
            - name: LITELLM_URL
              value: "http://litellm:4000"
            - name: EMBEDDING_MODEL
              value: "embeddings"
          volumeMounts:
            - name: code
              mountPath: /code
            - name: deps
              mountPath: /app/deps
          resources:
            requests:
              memory: "128Mi"
              cpu: "100m"
            limits:
              memory: "256Mi"
              cpu: "500m"
          readinessProbe:
            tcpSocket:
              port: 8000
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
          livenessProbe:
            tcpSocket:
              port: 8000
            initialDelaySeconds: 60
            periodSeconds: 30
            timeoutSeconds: 5
      volumes:
        - name: code
          configMap:
            name: knowledge-mcp-code
        - name: deps
          emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: knowledge-mcp
  namespace: ai-platform
  labels:
    app: knowledge-mcp
    component: mcp
spec:
  selector:
    app: knowledge-mcp
  ports:
    - port: 8000
      targetPort: 8000
      name: http
