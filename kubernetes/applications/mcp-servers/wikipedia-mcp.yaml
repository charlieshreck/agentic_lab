---
apiVersion: v1
kind: ConfigMap
metadata:
  name: wikipedia-mcp-code
  namespace: ai-platform
data:
  main.py: |
    #!/usr/bin/env python3
    """Wikipedia MCP server for knowledge retrieval and research."""
    import os
    import logging
    from typing import Optional, List
    from fastmcp import FastMCP
    from pydantic import BaseModel, Field
    import httpx

    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

    WIKIPEDIA_API = "https://en.wikipedia.org/api/rest_v1"
    WIKIPEDIA_ACTION_API = "https://en.wikipedia.org/w/api.php"

    mcp = FastMCP(
        name="wikipedia-mcp",
        instructions="Wikipedia MCP server for searching articles, getting summaries, and retrieving knowledge."
    )

    async def _wiki_rest(endpoint: str) -> dict:
        """Call Wikipedia REST API."""
        async with httpx.AsyncClient(timeout=30.0) as client:
            headers = {"User-Agent": "KernowHomelabMCP/1.0"}
            response = await client.get(f"{WIKIPEDIA_API}{endpoint}", headers=headers)
            response.raise_for_status()
            return response.json()

    async def _wiki_action(params: dict) -> dict:
        """Call Wikipedia Action API."""
        params["format"] = "json"
        async with httpx.AsyncClient(timeout=30.0) as client:
            headers = {"User-Agent": "KernowHomelabMCP/1.0"}
            response = await client.get(WIKIPEDIA_ACTION_API, params=params, headers=headers)
            response.raise_for_status()
            return response.json()

    def _handle_error(e: Exception) -> str:
        if isinstance(e, httpx.HTTPStatusError):
            return f"Error: Wikipedia API returned {e.response.status_code}"
        return f"Error: {type(e).__name__}: {str(e)}"

    # ============================================================================
    # SEARCH
    # ============================================================================

    @mcp.tool(annotations={"readOnlyHint": True})
    async def wikipedia_search(query: str, limit: int = 10) -> str:
        """Search Wikipedia for articles matching a query."""
        try:
            params = {
                "action": "query",
                "list": "search",
                "srsearch": query,
                "srlimit": min(limit, 20),
                "srprop": "snippet|titlesnippet"
            }
            result = await _wiki_action(params)
            items = result.get("query", {}).get("search", [])

            lines = [f"# Wikipedia Search: {query}", "", f"Found {len(items)} results", ""]
            for item in items:
                title = item.get("title", "")
                snippet = item.get("snippet", "").replace('<span class="searchmatch">', '**').replace('</span>', '**')
                # Clean HTML
                import re
                snippet = re.sub(r'<[^>]+>', '', snippet)
                lines.append(f"## {title}")
                lines.append(f"{snippet[:200]}...")
                lines.append("")
            return "\n".join(lines)
        except Exception as e:
            return _handle_error(e)

    # ============================================================================
    # ARTICLE RETRIEVAL
    # ============================================================================

    @mcp.tool(annotations={"readOnlyHint": True})
    async def wikipedia_summary(title: str) -> str:
        """Get a summary of a Wikipedia article."""
        try:
            # URL encode the title
            import urllib.parse
            encoded_title = urllib.parse.quote(title.replace(" ", "_"))
            result = await _wiki_rest(f"/page/summary/{encoded_title}")

            extract = result.get("extract", "No summary available.")
            page_title = result.get("title", title)
            description = result.get("description", "")
            url = result.get("content_urls", {}).get("desktop", {}).get("page", "")

            return (f"# {page_title}\n\n"
                    f"*{description}*\n\n"
                    f"{extract}\n\n"
                    f"**Read more:** {url}")
        except httpx.HTTPStatusError as e:
            if e.response.status_code == 404:
                return f"Article not found: {title}. Try searching with wikipedia_search first."
            return _handle_error(e)
        except Exception as e:
            return _handle_error(e)

    @mcp.tool(annotations={"readOnlyHint": True})
    async def wikipedia_article(title: str, max_length: int = 5000) -> str:
        """Get the full text content of a Wikipedia article."""
        try:
            import urllib.parse
            encoded_title = urllib.parse.quote(title.replace(" ", "_"))

            # Get mobile-html which has cleaner content
            async with httpx.AsyncClient(timeout=30.0) as client:
                headers = {"User-Agent": "KernowHomelabMCP/1.0"}
                response = await client.get(
                    f"{WIKIPEDIA_API}/page/mobile-html/{encoded_title}",
                    headers=headers
                )
                response.raise_for_status()
                html = response.text

            # Extract text from HTML (basic extraction)
            import re
            # Remove script and style elements
            html = re.sub(r'<script[^>]*>.*?</script>', '', html, flags=re.DOTALL)
            html = re.sub(r'<style[^>]*>.*?</style>', '', html, flags=re.DOTALL)
            # Remove HTML tags
            text = re.sub(r'<[^>]+>', ' ', html)
            # Clean whitespace
            text = re.sub(r'\s+', ' ', text).strip()

            # Truncate
            if len(text) > max_length:
                text = text[:max_length] + "...\n\n[Content truncated]"

            return f"# {title}\n\n{text}"
        except httpx.HTTPStatusError as e:
            if e.response.status_code == 404:
                return f"Article not found: {title}. Try searching with wikipedia_search first."
            return _handle_error(e)
        except Exception as e:
            return _handle_error(e)

    @mcp.tool(annotations={"readOnlyHint": True})
    async def wikipedia_sections(title: str) -> str:
        """Get the table of contents (sections) of a Wikipedia article."""
        try:
            params = {
                "action": "parse",
                "page": title,
                "prop": "sections"
            }
            result = await _wiki_action(params)
            sections = result.get("parse", {}).get("sections", [])

            lines = [f"# Sections: {title}", ""]
            for s in sections:
                level = int(s.get("toclevel", 1))
                indent = "  " * (level - 1)
                number = s.get("number", "")
                name = s.get("line", "")
                lines.append(f"{indent}- {number} {name}")

            return "\n".join(lines) if len(lines) > 2 else "No sections found."
        except Exception as e:
            return _handle_error(e)

    @mcp.tool(annotations={"readOnlyHint": True})
    async def wikipedia_section_content(title: str, section: int) -> str:
        """Get the content of a specific section of a Wikipedia article."""
        try:
            params = {
                "action": "parse",
                "page": title,
                "prop": "text",
                "section": str(section)
            }
            result = await _wiki_action(params)
            html = result.get("parse", {}).get("text", {}).get("*", "")

            # Extract text from HTML
            import re
            html = re.sub(r'<script[^>]*>.*?</script>', '', html, flags=re.DOTALL)
            html = re.sub(r'<style[^>]*>.*?</style>', '', html, flags=re.DOTALL)
            text = re.sub(r'<[^>]+>', ' ', html)
            text = re.sub(r'\s+', ' ', text).strip()

            section_title = result.get("parse", {}).get("title", title)
            return f"# {section_title} - Section {section}\n\n{text[:5000]}"
        except Exception as e:
            return _handle_error(e)

    # ============================================================================
    # RELATED CONTENT
    # ============================================================================

    @mcp.tool(annotations={"readOnlyHint": True})
    async def wikipedia_links(title: str, limit: int = 20) -> str:
        """Get links from a Wikipedia article to other articles."""
        try:
            params = {
                "action": "query",
                "titles": title,
                "prop": "links",
                "pllimit": min(limit, 50),
                "plnamespace": "0"  # Main namespace only
            }
            result = await _wiki_action(params)
            pages = result.get("query", {}).get("pages", {})

            lines = [f"# Links from: {title}", ""]
            for page_id, page_data in pages.items():
                links = page_data.get("links", [])
                for link in links:
                    lines.append(f"- {link.get('title', '')}")

            return "\n".join(lines) if len(lines) > 2 else "No links found."
        except Exception as e:
            return _handle_error(e)

    @mcp.tool(annotations={"readOnlyHint": True})
    async def wikipedia_related(title: str) -> str:
        """Get related articles for a Wikipedia article."""
        try:
            import urllib.parse
            encoded_title = urllib.parse.quote(title.replace(" ", "_"))
            result = await _wiki_rest(f"/page/related/{encoded_title}")

            pages = result.get("pages", [])
            lines = [f"# Related to: {title}", ""]
            for p in pages[:15]:
                p_title = p.get("title", "")
                desc = p.get("description", "")[:80]
                lines.append(f"- **{p_title}** - {desc}")

            return "\n".join(lines) if len(lines) > 2 else "No related articles found."
        except Exception as e:
            return _handle_error(e)

    @mcp.tool(annotations={"readOnlyHint": True})
    async def wikipedia_categories(title: str) -> str:
        """Get categories of a Wikipedia article."""
        try:
            params = {
                "action": "query",
                "titles": title,
                "prop": "categories",
                "cllimit": "50"
            }
            result = await _wiki_action(params)
            pages = result.get("query", {}).get("pages", {})

            lines = [f"# Categories: {title}", ""]
            for page_id, page_data in pages.items():
                categories = page_data.get("categories", [])
                for cat in categories:
                    cat_title = cat.get("title", "").replace("Category:", "")
                    lines.append(f"- {cat_title}")

            return "\n".join(lines) if len(lines) > 2 else "No categories found."
        except Exception as e:
            return _handle_error(e)

    # ============================================================================
    # SPECIAL PAGES
    # ============================================================================

    @mcp.tool(annotations={"readOnlyHint": True})
    async def wikipedia_random() -> str:
        """Get a random Wikipedia article summary."""
        try:
            result = await _wiki_rest("/page/random/summary")
            return await wikipedia_summary(result.get("title", ""))
        except Exception as e:
            return _handle_error(e)

    @mcp.tool(annotations={"readOnlyHint": True})
    async def wikipedia_on_this_day() -> str:
        """Get notable events that happened on this day in history."""
        try:
            from datetime import datetime
            today = datetime.now()
            result = await _wiki_rest(f"/feed/onthisday/events/{today.month}/{today.day}")

            events = result.get("events", [])[:10]
            lines = [f"# On This Day: {today.strftime('%B %d')}", ""]
            for e in events:
                year = e.get("year", "")
                text = e.get("text", "")[:150]
                lines.append(f"- **{year}**: {text}")

            return "\n".join(lines)
        except Exception as e:
            return _handle_error(e)

    # ============================================================================
    # REST API & HEALTH
    # ============================================================================

    from starlette.applications import Starlette
    from starlette.routing import Route, Mount
    from starlette.responses import JSONResponse

    async def rest_health(request):
        try:
            # Quick connectivity test
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.get(f"{WIKIPEDIA_API}/page/summary/Wikipedia",
                                           headers={"User-Agent": "KernowHomelabMCP/1.0"})
                response.raise_for_status()
            return JSONResponse({"status": "healthy", "api": "wikipedia"})
        except Exception as e:
            return JSONResponse({"status": "degraded", "error": str(e)[:100]})

    if __name__ == "__main__":
        import uvicorn
        rest_routes = [Route("/health", rest_health, methods=["GET"])]
        mcp_app = mcp.http_app()
        app = Starlette(routes=rest_routes + [Mount("/", app=mcp_app)], lifespan=mcp_app.lifespan)
        uvicorn.run(app, host="0.0.0.0", port=8000)
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: wikipedia-mcp
  namespace: ai-platform
  labels:
    app: wikipedia-mcp
    component: mcp
spec:
  replicas: 1
  selector:
    matchLabels:
      app: wikipedia-mcp
  template:
    metadata:
      labels:
        app: wikipedia-mcp
        component: mcp
    spec:
      containers:
        - name: wikipedia-mcp
          image: python:3.11-slim
          command: ["sh", "-c"]
          args:
            - pip install --no-cache-dir fastmcp httpx uvicorn starlette pydantic && python /app/main.py
          ports:
            - containerPort: 8000
          volumeMounts:
            - name: code
              mountPath: /app
          resources:
            requests: {memory: "128Mi", cpu: "50m"}
            limits: {memory: "256Mi", cpu: "500m"}
          readinessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 30
            periodSeconds: 10
          livenessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 60
            periodSeconds: 30
      volumes:
        - name: code
          configMap:
            name: wikipedia-mcp-code
---
apiVersion: v1
kind: Service
metadata:
  name: wikipedia-mcp
  namespace: ai-platform
  labels:
    app: wikipedia-mcp
    component: mcp
spec:
  type: NodePort
  selector:
    app: wikipedia-mcp
  ports:
    - port: 8000
      targetPort: 8000
      nodePort: 31112
      name: http
