---
# PostgreSQL Backup CronJob for Agentic Cluster
# Backs up all PostgreSQL databases to Garage S3
apiVersion: batch/v1
kind: CronJob
metadata:
  name: postgresql-backup
  namespace: ai-platform
  labels:
    app.kubernetes.io/name: postgresql-backup
    app.kubernetes.io/component: backup
spec:
  # Daily at 2 AM UTC
  schedule: "0 2 * * *"
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 3
  jobTemplate:
    spec:
      ttlSecondsAfterFinished: 86400  # Clean up after 24h
      template:
        spec:
          restartPolicy: OnFailure
          containers:
            - name: backup
              image: postgres:18-alpine
              envFrom:
                - secretRef:
                    name: backup-s3-credentials
              env:
                - name: PGHOST
                  value: "postgresql"
                - name: PGPORT
                  value: "5432"
                - name: PGUSER
                  valueFrom:
                    secretKeyRef:
                      name: postgresql-credentials
                      key: USERNAME
                - name: PGPASSWORD
                  valueFrom:
                    secretKeyRef:
                      name: postgresql-credentials
                      key: PASSWORD
                - name: BACKUP_BUCKET
                  value: "backrest"
                - name: BACKUP_PREFIX
                  value: "agentic/postgresql"
                - name: S3_ENDPOINT
                  value: "http://10.10.0.103:30188"
              command:
                - /bin/sh
                - -c
                - |
                  set -e

                  # Install AWS CLI (alpine package)
                  apk add --no-cache aws-cli

                  TIMESTAMP=$(date +%Y%m%d-%H%M%S)
                  BACKUP_DIR="/tmp/backup-${TIMESTAMP}"
                  mkdir -p "${BACKUP_DIR}"

                  echo "=== PostgreSQL Backup Started at $(date) ==="

                  # Get list of databases (excluding templates)
                  DATABASES=$(psql -h ${PGHOST} -U ${PGUSER} -d postgres -t -c \
                    "SELECT datname FROM pg_database WHERE datistemplate = false AND datname != 'postgres';")

                  for DB in ${DATABASES}; do
                    DB=$(echo $DB | tr -d ' ')
                    if [ -n "$DB" ]; then
                      echo "Backing up database: ${DB}"
                      DUMP_FILE="${BACKUP_DIR}/${DB}.sql.gz"

                      pg_dump -h ${PGHOST} -U ${PGUSER} -d "${DB}" \
                        --format=custom \
                        --compress=9 \
                        -f "${BACKUP_DIR}/${DB}.dump"

                      if [ $? -eq 0 ]; then
                        echo "  ✓ Dump completed: ${DB}"
                      else
                        echo "  ✗ Dump failed: ${DB}"
                        exit 1
                      fi
                    fi
                  done

                  # Upload to S3
                  echo "=== Uploading to S3 ==="
                  for DUMP in ${BACKUP_DIR}/*.dump; do
                    if [ -f "$DUMP" ]; then
                      FILENAME=$(basename "$DUMP")
                      S3_PATH="s3://${BACKUP_BUCKET}/${BACKUP_PREFIX}/${TIMESTAMP}/${FILENAME}"

                      aws s3 cp "$DUMP" "$S3_PATH" \
                        --endpoint-url "${S3_ENDPOINT}"

                      if [ $? -eq 0 ]; then
                        echo "  ✓ Uploaded: ${S3_PATH}"
                      else
                        echo "  ✗ Upload failed: ${FILENAME}"
                        exit 1
                      fi
                    fi
                  done

                  # Clean up old backups (keep 14 days)
                  echo "=== Cleaning old backups ==="
                  # Calculate cutoff date (14 days ago) using epoch seconds (busybox compatible)
                  CUTOFF_EPOCH=$(($(date +%s) - 86400*14))
                  CUTOFF_DATE=$(date -d "@${CUTOFF_EPOCH}" +%Y%m%d 2>/dev/null || date -D %s -d "${CUTOFF_EPOCH}" +%Y%m%d)

                  aws s3 ls "s3://${BACKUP_BUCKET}/${BACKUP_PREFIX}/" \
                    --endpoint-url "${S3_ENDPOINT}" | while read -r line; do
                    DIR=$(echo "$line" | awk '{print $2}' | tr -d '/')
                    # Extract date portion (YYYYMMDD)
                    DIR_DATE=$(echo "$DIR" | cut -d'-' -f1)
                    if [ -n "$DIR_DATE" ] && [ "$DIR_DATE" -lt "$CUTOFF_DATE" ] 2>/dev/null; then
                      echo "  Deleting old backup: ${DIR}"
                      aws s3 rm "s3://${BACKUP_BUCKET}/${BACKUP_PREFIX}/${DIR}/" \
                        --endpoint-url "${S3_ENDPOINT}" --recursive
                    fi
                  done

                  # Cleanup local files
                  rm -rf "${BACKUP_DIR}"

                  echo "=== PostgreSQL Backup Completed at $(date) ==="
              resources:
                requests:
                  memory: "256Mi"
                  cpu: "100m"
                limits:
                  memory: "512Mi"
                  cpu: "500m"
---
# S3 credentials for backups (synced from Infisical)
apiVersion: secrets.infisical.com/v1alpha1
kind: InfisicalSecret
metadata:
  name: backup-s3-infisical
  namespace: ai-platform
  labels:
    app.kubernetes.io/name: backup-credentials
spec:
  hostAPI: https://app.infisical.com/api
  resyncInterval: 300
  authentication:
    universalAuth:
      credentialsRef:
        secretName: universal-auth-credentials
        secretNamespace: infisical-operator-system
      secretsScope:
        projectSlug: prod-homelab-y-nij
        envSlug: prod
        secretsPath: /backups/garage
  managedSecretReference:
    secretName: backup-s3-credentials
    secretNamespace: ai-platform
    secretType: Opaque
    creationPolicy: Owner
