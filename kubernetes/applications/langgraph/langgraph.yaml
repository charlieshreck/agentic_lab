apiVersion: v1
kind: ConfigMap
metadata:
  name: langgraph-code
  namespace: ai-platform
  labels:
    app: langgraph
data:
  main.py: |
    #!/usr/bin/env python3
    """LangGraph Orchestrator for Agentic AI Platform with Diagnostic Runbooks."""
    import os
    import logging
    import json
    import re
    import uuid as uuid_mod
    from typing import TypedDict, Annotated, Sequence, Optional, List, Dict, Any
    from datetime import datetime, timedelta
    import httpx
    import asyncio
    from textwrap import dedent

    from langgraph.graph import StateGraph, END
    from langchain_core.messages import BaseMessage

    from fastapi import FastAPI, HTTPException, Request
    from pydantic import BaseModel, Field
    import uvicorn

    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

    # Environment configuration
    # Core service URLs
    POSTGRES_URL = os.environ.get("POSTGRES_URL", "postgresql://postgres:postgres@postgres:5432/langgraph")
    REDIS_URL = os.environ.get("REDIS_URL", "redis://redis:6379")
    LITELLM_URL = os.environ.get("LITELLM_URL", "http://litellm:4000")
    QDRANT_URL = os.environ.get("QDRANT_URL", "http://qdrant:6333")

    # Claude services - agent for escalation (validator runs separately as daily cronjob)
    CLAUDE_AGENT_URL = os.environ.get("CLAUDE_AGENT_URL", "http://claude-agent:8000")

    # MCP servers - loaded dynamically from mcp-servers-config ConfigMap
    # These env vars are injected via envFrom in deployment
    def load_mcp_urls():
        """Load all MCP URLs from environment variables."""
        urls = {}
        mcp_names = [
            "knowledge", "infrastructure", "coroot", "proxmox", "opnsense",
            "adguard", "cloudflare", "unifi", "truenas", "home_assistant",
            "arr_suite", "homepage", "web_search", "browser_automation",
            "plex", "vikunja", "neo4j", "tasmota", "monitoring", "keep", "infisical",
            "github", "wikipedia", "reddit", "outline"
        ]
        for name in mcp_names:
            env_key = f"{name.upper()}_MCP_URL"
            url = os.environ.get(env_key)
            if url:
                urls[name.replace("_", "-")] = url
        return urls

    MCP_URLS = load_mcp_urls()
    logger.info(f"Loaded {len(MCP_URLS)} MCP server URLs from environment")

    # Convenience accessors for frequently used MCPs
    KNOWLEDGE_MCP_URL = os.environ.get("KNOWLEDGE_MCP_URL", "http://knowledge-mcp:8000")
    INFRASTRUCTURE_MCP_URL = os.environ.get("INFRASTRUCTURE_MCP_URL", "http://infrastructure-mcp:8000")
    COROOT_MCP_URL = os.environ.get("COROOT_MCP_URL", "http://observability-mcp:8000")
    OPNSENSE_MCP_URL = os.environ.get("OPNSENSE_MCP_URL", "http://opnsense-mcp:8000")
    UNIFI_MCP_URL = os.environ.get("UNIFI_MCP_URL", "http://unifi-mcp:8000")
    TRUENAS_MCP_URL = os.environ.get("TRUENAS_MCP_URL", "http://truenas-mcp:8000")
    PROXMOX_MCP_URL = os.environ.get("PROXMOX_MCP_URL", "http://proxmox-mcp:8000")
    HOME_ASSISTANT_MCP_URL = os.environ.get("HOME_ASSISTANT_MCP_URL", "http://home-assistant-mcp:8000")
    TASMOTA_MCP_URL = os.environ.get("TASMOTA_MCP_URL", "http://tasmota-mcp:8000")
    MONITORING_MCP_URL = os.environ.get("MONITORING_MCP_URL", "http://monitoring-mcp:8000")
    EXTERNAL_MCP_URL = os.environ.get("EXTERNAL_MCP_URL", "http://external-mcp:8000")

    # Matrix bot for notifications (replaces Telegram)
    MATRIX_BOT_URL = os.environ.get("MATRIX_BOT_URL", "http://matrix-bot:8000")

    # Afferent mobile PWA for alert approvals
    AFFERENT_WEBHOOK_URL = os.environ.get("AFFERENT_WEBHOOK_URL", "")

    # Model configuration - Gemini only (via LiteLLM)
    # Using 2.0-flash for everything - best free tier limits (1500 RPD, 10 RPM)
    GEMINI_MODEL = os.environ.get("GEMINI_MODEL", "gemini/gemini-2.0-flash")
    GEMINI_FLASH_MODEL = os.environ.get("GEMINI_FLASH_MODEL", "gemini/gemini-2.0-flash")
    EMBEDDING_MODEL = os.environ.get("EMBEDDING_MODEL", "embeddings")

    # Thresholds and configuration - TIERED RUNBOOK MATCHING
    # >= 0.95: EXACT match - execute runbook as-is
    # >= 0.80: SIMILAR match - execute with tweaks, propose runbook update PR
    # <  0.80: NO_MATCH - escalate to Brain Trust (humans via Outline)
    RUNBOOK_EXACT_THRESHOLD = float(os.environ.get("RUNBOOK_EXACT_THRESHOLD", "0.95"))
    RUNBOOK_SIMILAR_THRESHOLD = float(os.environ.get("RUNBOOK_SIMILAR_THRESHOLD", "0.80"))
    # Legacy threshold for backward compatibility
    RUNBOOK_MATCH_THRESHOLD = RUNBOOK_SIMILAR_THRESHOLD
    MIN_CONFIDENCE_FOR_LOCAL_FIX = float(os.environ.get("MIN_CONFIDENCE_FOR_LOCAL_FIX", "0.7"))
    AUTO_APPROVE_MIN_SUCCESSES = int(os.environ.get("AUTO_APPROVE_MIN_SUCCESSES", "5"))
    CONTEXT_CACHE_TTL = int(os.environ.get("CONTEXT_CACHE_TTL", "3600"))

    # APPROVAL_POLICY controls when human approval is required
    # - VERBOSE: Always request approval (default - safest)
    # - STANDARD: Auto-execute EXACT matches with high confidence (>= 0.9) and proven track record
    # - AGGRESSIVE: Auto-execute EXACT and high-confidence SIMILAR matches
    APPROVAL_POLICY = os.environ.get("APPROVAL_POLICY", "VERBOSE").upper()
    AUTO_EXECUTE_MIN_CONFIDENCE = float(os.environ.get("AUTO_EXECUTE_MIN_CONFIDENCE", "0.9"))

    # Temporal pattern detection - flag recurring alerts
    TEMPORAL_WINDOW_HOURS = int(os.environ.get("TEMPORAL_WINDOW_HOURS", "24"))
    RECURRING_ALERT_THRESHOLD = int(os.environ.get("RECURRING_ALERT_THRESHOLD", "3"))

    # GitHub integration for runbook improvement suggestions
    GITHUB_OWNER = os.environ.get("GITHUB_OWNER", "charlieshreck")
    GITHUB_REPO = os.environ.get("GITHUB_REPO", "agentic_lab")

    # Brain Trust Escalation - Outline collection for human review
    BRAIN_TRUST_COLLECTION_ID = os.environ.get("BRAIN_TRUST_COLLECTION_ID", "9bd8c738-4505-494a-a867-74349a887dc6")
    OUTLINE_MCP_URL = os.environ.get("OUTLINE_MCP_URL", "http://outline-mcp:8000")

    # A2A Orchestrator - handles all reasoning (investigate, plan, validate)
    A2A_URL = os.environ.get("A2A_URL", "http://a2a-orchestrator.ai-platform.svc.cluster.local:8000")
    A2A_ENABLED = os.environ.get("A2A_ENABLED", "true").lower() == "true"
    A2A_TIMEOUT = float(os.environ.get("A2A_TIMEOUT", "30.0"))

    # Skill Router Configuration (Project 05)
    # When enabled, routes requests through skill_router before assess_alert
    SKILL_ROUTER_ENABLED = os.environ.get("SKILL_ROUTER_ENABLED", "false").lower() == "true"
    SKILL_COLLISION_THRESHOLD = float(os.environ.get("SKILL_COLLISION_THRESHOLD", "0.2"))  # Relative threshold
    MAX_MCPS_PER_REQUEST = int(os.environ.get("MAX_MCPS_PER_REQUEST", "8"))

    # Static context cache (refreshed hourly for cost savings)
    _static_context_cache = {}
    _static_context_timestamp = None

    # ============================================================================
    # REDIS PERSISTENCE FOR PENDING APPROVALS
    # ============================================================================
    import redis.asyncio as redis_async

    _redis_client = None

    async def get_redis_client():
        """Get or create Redis client for pending approvals persistence."""
        global _redis_client
        if _redis_client is None:
            _redis_client = await redis_async.from_url(REDIS_URL, decode_responses=True)
        return _redis_client

    async def store_pending_approval(alert_id: str, data: dict) -> None:
        """Store pending approval in Redis with 24h TTL."""
        try:
            client = await get_redis_client()
            key = f"pending_approval:{alert_id}"
            await client.setex(key, 86400, json.dumps(data, default=str))  # 24h TTL
            logger.debug(f"Stored pending approval {alert_id} in Redis")
        except Exception as e:
            logger.error(f"Failed to store pending approval in Redis: {e}")
            # Fallback to in-memory
            pending_approvals[alert_id] = data

    async def get_pending_approval(alert_id: str) -> Optional[dict]:
        """Retrieve pending approval from Redis."""
        try:
            client = await get_redis_client()
            key = f"pending_approval:{alert_id}"
            data = await client.get(key)
            if data:
                return json.loads(data)
            # Fallback to in-memory
            return pending_approvals.get(alert_id)
        except Exception as e:
            logger.error(f"Failed to get pending approval from Redis: {e}")
            return pending_approvals.get(alert_id)

    async def delete_pending_approval(alert_id: str) -> None:
        """Delete pending approval from Redis."""
        try:
            client = await get_redis_client()
            key = f"pending_approval:{alert_id}"
            await client.delete(key)
            # Also remove from in-memory if present
            pending_approvals.pop(alert_id, None)
            logger.debug(f"Deleted pending approval {alert_id} from Redis")
        except Exception as e:
            logger.error(f"Failed to delete pending approval from Redis: {e}")
            pending_approvals.pop(alert_id, None)

    async def get_pending_count() -> int:
        """Get count of pending approvals from Redis."""
        try:
            client = await get_redis_client()
            keys = await client.keys("pending_approval:*")
            return len(keys)
        except Exception as e:
            logger.error(f"Failed to count pending approvals: {e}")
            return len(pending_approvals)

    # ============================================================================
    # TOOL CATALOG - Safe MCP tools for plan execution
    # ============================================================================

    # Auth token for MCP REST bridge
    A2A_API_TOKEN = os.environ.get("A2A_API_TOKEN", "")

    # Tool-to-MCP mapping with required arguments
    TOOL_CATALOG = {
        # Kubernetes Pod Operations
        "kubectl_delete_pod": {"mcp": "infrastructure", "required": ["namespace", "pod_name"], "risk": "medium"},
        "kubectl_get_pods": {"mcp": "infrastructure", "required": ["namespace"], "risk": "low"},
        "kubectl_logs": {"mcp": "infrastructure", "required": ["pod_name", "namespace"], "risk": "low"},

        # Kubernetes Deployment Operations
        "kubectl_restart_deployment": {"mcp": "infrastructure", "required": ["deployment_name", "namespace"], "risk": "medium"},
        "kubectl_scale_deployment": {"mcp": "infrastructure", "required": ["deployment_name", "namespace", "replicas"], "risk": "high"},
        "kubectl_get_deployments": {"mcp": "infrastructure", "required": ["namespace"], "risk": "low"},
        "kubectl_rollout_status": {"mcp": "infrastructure", "required": ["deployment_name", "namespace"], "risk": "low"},

        # Kubernetes Service Operations
        "kubectl_get_services": {"mcp": "infrastructure", "required": ["namespace"], "risk": "low"},
        "kubectl_get_events": {"mcp": "infrastructure", "required": ["namespace"], "risk": "low"},

        # ArgoCD Operations
        "argocd_sync_application": {"mcp": "infrastructure", "required": ["app_name"], "risk": "medium"},
        "argocd_get_applications": {"mcp": "infrastructure", "required": [], "risk": "low"},

        # Alerting Operations
        "create_silence": {"mcp": "observability", "required": ["matchers", "duration"], "risk": "low"},
        "delete_silence": {"mcp": "observability", "required": ["silence_id"], "risk": "low"},
        "list_alerts": {"mcp": "observability", "required": [], "risk": "low"},

        # Metrics Operations
        "query_metrics_instant": {"mcp": "observability", "required": ["query"], "risk": "low"},
    }

    # MCP base URLs by domain
    MCP_DOMAIN_URLS = {
        "infrastructure": INFRASTRUCTURE_MCP_URL,
        "observability": os.environ.get("OBSERVABILITY_MCP_URL", "http://observability-mcp:8000"),
        "knowledge": KNOWLEDGE_MCP_URL,
    }

    # Command-to-tool patterns for backwards compatibility
    COMMAND_TO_TOOL_PATTERNS = [
        # kubectl delete pod <pod> -n <namespace>
        (r"kubectl\s+delete\s+pods?\s+([a-z0-9][a-z0-9\-\.]*)\s+(?:-n\s+|--namespace[=\s])([a-z0-9][a-z0-9\-]*)",
         "kubectl_delete_pod", lambda m: {"pod_name": m.group(1), "namespace": m.group(2)}),
        # kubectl delete pod <pod> (default namespace)
        (r"kubectl\s+delete\s+pods?\s+([a-z0-9][a-z0-9\-\.]*)\s*$",
         "kubectl_delete_pod", lambda m: {"pod_name": m.group(1), "namespace": "default"}),
        # kubectl rollout restart deployment/<name> -n <namespace>
        (r"kubectl\s+rollout\s+restart\s+deployment[/]?([a-z0-9][a-z0-9\-]*)\s+(?:-n\s+|--namespace[=\s])([a-z0-9][a-z0-9\-]*)",
         "kubectl_restart_deployment", lambda m: {"deployment_name": m.group(1), "namespace": m.group(2)}),
        # kubectl scale deployment/<name> --replicas=N -n <namespace>
        (r"kubectl\s+scale\s+deployment[/]?([a-z0-9][a-z0-9\-]*)\s+--replicas[=\s]?(\d+)\s+(?:-n\s+|--namespace[=\s])([a-z0-9][a-z0-9\-]*)",
         "kubectl_scale_deployment", lambda m: {"deployment_name": m.group(1), "replicas": int(m.group(2)), "namespace": m.group(3)}),
        # argocd app sync <app>
        (r"argocd\s+app\s+sync\s+([a-z0-9][a-z0-9\-]*)",
         "argocd_sync_application", lambda m: {"app_name": m.group(1)}),
    ]

    def command_to_tool(command: str) -> tuple:
        """Convert a shell command to a tool call.

        Returns:
            Tuple of (tool_name, arguments) or (None, None) if no match
        """
        command = command.strip()
        for pattern, tool_name, extractor in COMMAND_TO_TOOL_PATTERNS:
            match = re.search(pattern, command, re.IGNORECASE)
            if match:
                try:
                    args = extractor(match)
                    logger.info(f"Mapped command to tool: {tool_name} with args {args}")
                    return tool_name, args
                except Exception as e:
                    logger.warning(f"Failed to extract args from command: {e}")
                    continue
        return None, None

    async def call_mcp_rest(mcp_url: str, tool_name: str, arguments: dict) -> dict:
        """Call MCP tool via REST bridge (/api/call).

        This is the safe execution path for plan steps.

        Args:
            mcp_url: Base URL of the MCP server
            tool_name: Name of the tool to call
            arguments: Tool arguments

        Returns:
            Dict with status, output, and optional error
        """
        headers = {"Content-Type": "application/json"}
        if A2A_API_TOKEN:
            headers["Authorization"] = f"Bearer {A2A_API_TOKEN}"

        async with httpx.AsyncClient(timeout=120.0) as client:
            try:
                response = await client.post(
                    f"{mcp_url}/api/call",
                    json={"tool": tool_name, "arguments": arguments},
                    headers=headers
                )
                result = response.json()

                if response.status_code == 200 and result.get("status") == "success":
                    return {
                        "success": True,
                        "tool": tool_name,
                        "output": result.get("output"),
                    }
                else:
                    return {
                        "success": False,
                        "tool": tool_name,
                        "error": result.get("error", f"HTTP {response.status_code}"),
                    }
            except Exception as e:
                logger.error(f"REST bridge call failed: {tool_name} - {e}")
                return {
                    "success": False,
                    "tool": tool_name,
                    "error": str(e),
                }

    async def capture_pre_state(pre_capture: list, arguments: dict) -> dict:
        """Capture pre-execution state for rollback.

        Args:
            pre_capture: List of state capture definitions
            arguments: Tool arguments (for context like namespace, deployment_name)

        Returns:
            Dict of captured values keyed by capture key
        """
        captured = {}
        for capture in pre_capture:
            capture_tool = capture.get("tool")
            capture_key = capture.get("key")
            extract_path = capture.get("extract")

            if not capture_tool or not capture_key:
                continue

            # Build arguments for the capture tool from the main arguments
            capture_args = {}
            spec = TOOL_CATALOG.get(capture_tool, {})
            for req_arg in spec.get("required", []):
                if req_arg in arguments:
                    capture_args[req_arg] = arguments[req_arg]

            # Get MCP URL for the capture tool
            mcp_domain = spec.get("mcp", "infrastructure")
            mcp_url = MCP_DOMAIN_URLS.get(mcp_domain, INFRASTRUCTURE_MCP_URL)

            result = await call_mcp_rest(mcp_url, capture_tool, capture_args)
            if result.get("success"):
                output = result.get("output", {})
                # Try to extract the specific field
                if isinstance(output, str):
                    try:
                        output = json.loads(output)
                    except:
                        pass

                if isinstance(output, dict) and extract_path:
                    # Simple dot-notation extraction
                    value = output
                    for part in extract_path.split("."):
                        if isinstance(value, dict):
                            value = value.get(part)
                        elif isinstance(value, list) and part.isdigit():
                            value = value[int(part)]
                        else:
                            value = None
                            break
                    captured[capture_key] = value
                else:
                    captured[capture_key] = output

                logger.info(f"Captured state: {capture_key}={captured[capture_key]}")

        return captured

    def substitute_captured(template: dict, captured: dict) -> dict:
        """Substitute {captured.key} placeholders in rollback args.

        Args:
            template: Dict with potential placeholders
            captured: Dict of captured values

        Returns:
            Dict with placeholders substituted
        """
        result = {}
        for key, value in template.items():
            if isinstance(value, str):
                # Replace {captured.X} with actual value
                def replacer(m):
                    cap_key = m.group(1)
                    return str(captured.get(cap_key, m.group(0)))
                result[key] = re.sub(r'\{captured\.(\w+)\}', replacer, value)
            else:
                result[key] = value
        return result

    # ============================================================================
    # SYSTEM PROMPT - Injected into all LLM calls
    # ============================================================================

    AGENT_SYSTEM_PROMPT = """
    You are the AI assistant for Charlie's Agentic Homelab Platform.

    Your Capabilities:

    You have access to real-time data from 25 MCP servers (data is fetched automatically based on your query):

    Infrastructure & Network:
    - infrastructure-mcp - Kubernetes cluster state, pods, deployments, services
    - unifi-mcp - WiFi clients, access points, switches, network health
    - opnsense-mcp - Firewall rules, DHCP leases, gateway status, DNS
    - proxmox-mcp - VMs, LXCs, hypervisor nodes, resource usage
    - truenas-mcp - Storage pools, datasets, ZFS health, shares
    - cloudflare-mcp - DNS records, tunnels, zone management

    Monitoring & Observability:
    - coroot-mcp - Service metrics, anomaly detection, dependencies
    - monitoring-mcp - VictoriaMetrics, AlertManager, VictoriaLogs, Grafana, Gatus
    - adguard-mcp - DNS stats, query logs, blocking rates
    - keep-mcp - Alert aggregation, incidents, deduplication

    Smart Home & IoT:
    - home-assistant-mcp - Lights, sensors, climate, automations
    - tasmota-mcp - 26 Tasmota smart devices (power control, WiFi config)

    Media & Entertainment:
    - arr-suite-mcp - Sonarr, Radarr, Prowlarr, Overseerr, Transmission
    - plex-mcp - Plex server status, libraries, active streams, GPU usage

    Knowledge & Search:
    - knowledge-mcp - Qdrant vector DB, runbooks, docs, entities, Neo4j graph
    - neo4j-mcp - Knowledge graph queries, dependencies, impact analysis
    - web-search-mcp - Internet search via SearXNG
    - browser-automation-mcp - Playwright browser automation
    - wikipedia-mcp - Wikipedia articles, summaries, search

    Information & Research:
    - github-mcp - GitHub repos, issues, PRs, code search
    - reddit-mcp - Reddit browsing, subreddit search, discussions

    Documentation:
    - outline-mcp - Outline wiki document and collection management

    Utilities:
    - vikunja-mcp - Task management, kanban boards
    - homepage-mcp - Dashboard widgets and service status
    - infisical-mcp - Secrets management (read-only)

    Qdrant Collections:
    - runbooks - Operational procedures for common issues
    - documentation - Architecture docs and guides
    - entities - Every device on the network (IP, MAC, hostname, type)
    - decisions - Historical decisions and outcomes
    - agent_events - Forever Learning System event log

    Guidelines:
    - Answer based on ACTUAL DATA provided - never invent device names, IPs, or statuses
    - If data retrieval fails, tell the user what failed and suggest troubleshooting
    - Be concise but thorough - include relevant metrics and specifics
    - When suggesting fixes, provide confidence level and reasoning
    - For complex issues, recommend escalating to Claude Code session

    Network Overview:
    - 10.10.0.0/24 - Production network (Proxmox, K8s prod cluster)
    - 10.20.0.0/24 - Agentic platform (this AI system)
    - 10.30.0.0/24 - Monitoring cluster (Grafana, Prometheus, Coroot)
    """

    # ============================================================================
    # RUNBOOK SCHEMA - Pydantic Models for Diagnostic Workflows
    # ============================================================================

    class ExtractField(BaseModel):
        """Extract a value from command output."""
        field: str = Field(..., description="JSONPath or regex pattern to extract")
        expected: Optional[Any] = Field(None, description="Expected value for confidence")
        store_as: str = Field(..., description="Variable name to store result")

    class LookForPattern(BaseModel):
        """Pattern to look for in command output."""
        pattern: str = Field(..., description="Regex pattern to search for")
        confirms: Optional[str] = Field(None, description="What finding this confirms")
        indicates: Optional[str] = Field(None, description="What finding indicates")

    class DiagnosisCheck(BaseModel):
        """A diagnostic check to verify the error."""
        name: str
        command: str = Field(..., description="Command with {{variable}} placeholders")
        extract: Optional[List[ExtractField]] = None
        look_for: Optional[List[LookForPattern]] = None
        store_as: Optional[str] = Field(None, description="Store raw output as variable")
        timeout: int = 30

    class ConfidenceRule(BaseModel):
        """Rule for calculating diagnostic confidence."""
        condition: str = Field(..., description="Python expression using stored variables")
        confidence: float = Field(..., ge=0, le=1)
        diagnosis: str
        note: Optional[str] = None

    class GatherCommand(BaseModel):
        """Command to gather additional context."""
        name: str
        command: str
        store_as: str
        on_failure: str = "continue"
        timeout: int = 30

    class Precondition(BaseModel):
        """Precondition before applying fix."""
        check: str = Field(..., description="Python expression that must be true")
        reason: str

    class FixStep(BaseModel):
        """A step in the fix process."""
        name: str
        action: str = Field(..., description="command, compute, or wait")
        command: Optional[str] = None
        formula: Optional[str] = None
        store_as: Optional[str] = None
        rollback: Optional[str] = None
        timeout: int = 60
        on_failure: str = "abort"
        max_value: Optional[str] = None

    class ValidationCheck(BaseModel):
        """Validation after fix is applied."""
        name: str
        command: str
        expected: Optional[str] = None
        should_not_contain: Optional[str] = None
        wait: int = 0
        retries: int = 1

    class DecisionRules(BaseModel):
        """Rules for deciding fix locally vs escalate."""
        escalate_if: List[str] = Field(default_factory=list)
        handle_locally_if: List[str] = Field(default_factory=list)

    class TriggerConfig(BaseModel):
        """What triggers this runbook."""
        alert_patterns: List[str] = Field(default_factory=list)
        keywords: List[str] = Field(default_factory=list)
        severity: List[str] = Field(default_factory=lambda: ["warning", "critical"])

    class DiagnosisConfig(BaseModel):
        """Diagnosis phase configuration."""
        description: str = ""
        checks: List[DiagnosisCheck] = Field(default_factory=list)
        confidence_rules: List[ConfidenceRule] = Field(default_factory=list)

    class InfoGatherConfig(BaseModel):
        """Information gathering phase."""
        description: str = ""
        commands: List[GatherCommand] = Field(default_factory=list)

    class FixConfig(BaseModel):
        """Fix phase configuration."""
        description: str = ""
        preconditions: List[Precondition] = Field(default_factory=list)
        steps: List[FixStep] = Field(default_factory=list)
        validation: List[ValidationCheck] = Field(default_factory=list)

    class RunbookMetadata(BaseModel):
        """Runbook metadata for learning."""
        created_by: str = "system"
        created_at: str = Field(default_factory=lambda: datetime.utcnow().isoformat())
        updated_at: Optional[str] = None
        success_count: int = 0
        failure_count: int = 0
        false_positive_count: int = 0
        escalation_count: int = 0
        avg_resolution_time: Optional[float] = None
        auto_approve_eligible: bool = False
        tags: List[str] = Field(default_factory=list)
        related_runbooks: List[str] = Field(default_factory=list)

    class Runbook(BaseModel):
        """Complete diagnostic runbook."""
        id: str = Field(default_factory=lambda: f"rb-{uuid_mod.uuid4().hex[:8]}")
        name: str
        version: int = 1
        description: str = ""
        triggers: TriggerConfig = Field(default_factory=TriggerConfig)
        diagnosis: DiagnosisConfig = Field(default_factory=DiagnosisConfig)
        information_gathering: InfoGatherConfig = Field(default_factory=InfoGatherConfig)
        decision: DecisionRules = Field(default_factory=DecisionRules)
        fix: FixConfig = Field(default_factory=FixConfig)
        escalation_template: str = ""
        metadata: RunbookMetadata = Field(default_factory=RunbookMetadata)

    # ============================================================================
    # RUNBOOK EXECUTOR - Executes diagnostic workflows
    # ============================================================================

    class RunbookExecutor:
        """Executes runbook diagnostic workflows."""

        def __init__(self, runbook: Runbook, alert: dict):
            self.runbook = runbook
            self.alert = alert
            self.variables: Dict[str, Any] = {}
            self.diagnosis_result: str = "unknown"
            self.confidence: float = 0.0
            self.escalation_reason: Optional[str] = None
            self.execution_log: List[str] = []
            self.rollback_stack: List[str] = []

            # Initialize variables from alert
            self.variables["alertname"] = alert.get("alertname", "")
            self.variables["severity"] = alert.get("severity", "warning")
            self.variables["description"] = alert.get("description", "")
            self.variables["namespace"] = alert.get("namespace", "default")
            self.variables["pod_name"] = self._extract_pod_name(alert)
            self.variables["node_name"] = (alert.get("labels") or {}).get("node", "")

        def _extract_pod_name(self, alert: dict) -> str:
            """Extract pod name from alert."""
            labels = alert.get("labels") or {}
            if "pod" in labels:
                return labels["pod"]
            desc = alert.get("description", "")
            match = re.search(r'pod[:\s]+([a-z0-9-]+)', desc, re.I)
            return match.group(1) if match else ""

        def _substitute_variables(self, text: str) -> str:
            """Replace {{variable}} placeholders with values."""
            def replacer(match):
                var_name = match.group(1)
                return str(self.variables.get(var_name, f"{{unknown:{var_name}}}"))
            return re.sub(r'\{\{(\w+)\}\}', replacer, text)

        async def _execute_command(self, command: str, timeout: int = 30) -> tuple:
            """Execute a command via MCP tool (using REST bridge).

            First tries to convert the command to a tool call.
            Falls back to error if command is not supported.
            """
            cmd = self._substitute_variables(command)
            self.execution_log.append(f"EXEC: {cmd}")

            # Try to convert command to tool call
            tool_name, arguments = command_to_tool(cmd)

            if tool_name:
                # Execute via REST bridge
                spec = TOOL_CATALOG.get(tool_name, {})
                mcp_domain = spec.get("mcp", "infrastructure")
                mcp_url = MCP_DOMAIN_URLS.get(mcp_domain, INFRASTRUCTURE_MCP_URL)

                result = await call_mcp_rest(mcp_url, tool_name, arguments)

                if result.get("success"):
                    output = str(result.get("output", ""))
                    self.execution_log.append(f"  -> OK (tool: {tool_name}): {output[:200]}")
                    return True, output
                else:
                    error = result.get("error", "Unknown error")
                    self.execution_log.append(f"  -> FAIL (tool: {tool_name}): {error}")
                    return False, error
            else:
                # Command not supported in tool catalog
                self.execution_log.append(f"  -> UNSUPPORTED: Command not in tool catalog: {cmd[:50]}")
                return False, f"Command not supported (no tool mapping): {cmd[:50]}..."

        def _extract_jsonpath(self, data: str, path: str) -> Any:
            """Extract value using simplified JSONPath."""
            try:
                obj = json.loads(data)
                parts = path.replace("[", ".").replace("]", "").split(".")
                for part in parts:
                    if not part:
                        continue
                    if part.isdigit():
                        obj = obj[int(part)]
                    else:
                        obj = obj.get(part, None)
                    if obj is None:
                        return None
                return obj
            except:
                return None

        def _check_pattern(self, text: str, pattern: str) -> bool:
            """Check if pattern exists in text."""
            try:
                return bool(re.search(pattern, text, re.IGNORECASE | re.MULTILINE))
            except:
                return pattern.lower() in text.lower()

        def _evaluate_condition(self, condition: str) -> bool:
            """Evaluate a condition using stored variables."""
            try:
                # Create safe evaluation context - convert None to empty string for string checks
                safe_vars = {k: (v if v is not None else "") for k, v in self.variables.items()}
                safe_vars["true"] = True
                safe_vars["false"] = False
                # Handle common patterns
                cond = condition
                # Handle 'literal' in variable pattern (e.g., 'OOMKill' in recent_events)
                # Substitute the actual variable value at substitution time
                cond = re.sub(r"'([^']+)'\s+in\s+(\w+)",
                    lambda m: f"'{m.group(1)}' in '{str(self.variables.get(m.group(2), '') or '').replace(chr(39), chr(92)+chr(39))}'", cond)
                cond = re.sub(r"(\w+)\s+in\s+\[([^\]]+)\]",
                    lambda m: f"'{self.variables.get(m.group(1), '')}' in [{m.group(2)}]", cond)
                cond = re.sub(r"(\w+)\s*==\s*'([^']+)'",
                    lambda m: f"'{self.variables.get(m.group(1), '')}' == '{m.group(2)}'", cond)
                cond = re.sub(r"(\w+)\s*!=\s*'([^']+)'",
                    lambda m: f"'{self.variables.get(m.group(1), '')}' != '{m.group(2)}'", cond)
                cond = re.sub(r"(\w+)\s*([<>]=?)\s*(\d+)",
                    lambda m: f"{self.variables.get(m.group(1), 0) or 0} {m.group(2)} {m.group(3)}", cond)
                cond = cond.replace(" AND ", " and ").replace(" OR ", " or ")
                return eval(cond, {"__builtins__": {}}, safe_vars)
            except Exception as e:
                logger.warning(f"Condition eval failed: {condition} - {e}")
                return False

        async def run_diagnosis(self) -> dict:
            """Run diagnostic checks to verify the error."""
            logger.info(f"Running diagnosis for {self.runbook.name}")
            results = {"checks": [], "confirmations": set(), "indications": set()}

            for check in self.runbook.diagnosis.checks:
                logger.info(f"  Check: {check.name}")
                success, output = await self._execute_command(check.command, check.timeout)

                check_result = {"name": check.name, "success": success, "output": output[:500]}

                if check.store_as:
                    self.variables[check.store_as] = output

                if check.extract:
                    for ext in check.extract:
                        value = self._extract_jsonpath(output, ext.field)
                        self.variables[ext.store_as] = value
                        check_result[ext.store_as] = value
                        if ext.expected is not None:
                            matches = str(value) == str(ext.expected)
                            check_result[f"{ext.store_as}_matches"] = matches

                if check.look_for:
                    for lf in check.look_for:
                        found = self._check_pattern(output, lf.pattern)
                        if found:
                            if lf.confirms:
                                results["confirmations"].add(lf.confirms)
                            if lf.indicates:
                                results["indications"].add(lf.indicates)

                results["checks"].append(check_result)

            # Convert sets to lists for JSON serialization
            results["confirmations"] = list(results["confirmations"])
            results["indications"] = list(results["indications"])
            return results

        def calculate_confidence(self) -> tuple:
            """Calculate confidence based on diagnosis results."""
            for rule in self.runbook.diagnosis.confidence_rules:
                if self._evaluate_condition(rule.condition):
                    self.confidence = rule.confidence
                    self.diagnosis_result = rule.diagnosis
                    logger.info(f"Confidence: {self.confidence} ({self.diagnosis_result})")
                    if rule.note:
                        self.execution_log.append(f"NOTE: {rule.note}")
                    return self.confidence, self.diagnosis_result

            # Default if no rules match
            self.confidence = 0.3
            self.diagnosis_result = "unclear"
            return self.confidence, self.diagnosis_result

        async def gather_information(self) -> dict:
            """Gather additional context information."""
            logger.info("Gathering additional information...")
            gathered = {}

            for cmd in self.runbook.information_gathering.commands:
                logger.info(f"  Gather: {cmd.name}")
                success, output = await self._execute_command(cmd.command, cmd.timeout)

                if success or cmd.on_failure == "continue":
                    self.variables[cmd.store_as] = output
                    gathered[cmd.store_as] = output[:1000]
                else:
                    gathered[cmd.store_as] = f"FAILED: {output}"

            return gathered

        def should_escalate(self) -> tuple:
            """Determine if we should escalate to Claude."""
            # Check escalation rules
            for rule in self.runbook.decision.escalate_if:
                if self._evaluate_condition(rule):
                    self.escalation_reason = rule
                    logger.info(f"Escalating due to: {rule}")
                    return True, rule

            # Check if confidence is too low
            if self.confidence < MIN_CONFIDENCE_FOR_LOCAL_FIX:
                self.escalation_reason = f"confidence ({self.confidence}) < threshold ({MIN_CONFIDENCE_FOR_LOCAL_FIX})"
                return True, self.escalation_reason

            # Check handle locally rules
            for rule in self.runbook.decision.handle_locally_if:
                if self._evaluate_condition(rule):
                    logger.info(f"Handling locally: {rule}")
                    return False, None

            # Default: escalate if uncertain
            return True, "no matching local handling rule"

        def check_preconditions(self) -> tuple:
            """Check if preconditions for fix are met."""
            for pre in self.runbook.fix.preconditions:
                if not self._evaluate_condition(pre.check):
                    return False, pre.reason
            return True, None

        async def run_fix(self) -> dict:
            """Execute the fix steps."""
            logger.info("Executing fix steps...")
            results = {"steps": [], "success": True}

            for step in self.runbook.fix.steps:
                logger.info(f"  Step: {step.name}")
                step_result = {"name": step.name, "action": step.action}

                if step.action == "command":
                    success, output = await self._execute_command(step.command, step.timeout)
                    step_result["success"] = success
                    step_result["output"] = output[:500]

                    if step.rollback:
                        self.rollback_stack.append(step.rollback)

                    if not success and step.on_failure == "abort":
                        results["success"] = False
                        results["error"] = f"Step '{step.name}' failed: {output}"
                        results["steps"].append(step_result)
                        break

                elif step.action == "compute":
                    try:
                        value = eval(self._substitute_variables(step.formula),
                                   {"__builtins__": {}}, self.variables)
                        if step.max_value:
                            max_val = self._parse_resource(step.max_value)
                            if self._parse_resource(str(value)) > max_val:
                                value = step.max_value
                        if step.store_as:
                            self.variables[step.store_as] = value
                        step_result["success"] = True
                        step_result["value"] = value
                    except Exception as e:
                        step_result["success"] = False
                        step_result["error"] = str(e)

                elif step.action == "wait":
                    await asyncio.sleep(step.timeout)
                    step_result["success"] = True

                results["steps"].append(step_result)

            return results

        def _parse_resource(self, value: str) -> int:
            """Parse resource string like '4Gi' to bytes."""
            value = str(value).strip()
            units = {"Ki": 1024, "Mi": 1024**2, "Gi": 1024**3, "Ti": 1024**4}
            for suffix, mult in units.items():
                if value.endswith(suffix):
                    return int(float(value[:-2]) * mult)
            return int(value)

        async def run_validation(self) -> dict:
            """Validate the fix worked."""
            logger.info("Running validation...")
            results = {"checks": [], "success": True}

            for val in self.runbook.fix.validation:
                logger.info(f"  Validate: {val.name}")

                if val.wait:
                    await asyncio.sleep(val.wait)

                for attempt in range(val.retries):
                    success, output = await self._execute_command(val.command, 30)
                    passed = success

                    if val.expected:
                        expected = self._substitute_variables(val.expected)
                        passed = passed and (expected in output or output.strip() == expected)

                    if val.should_not_contain:
                        forbidden = self._substitute_variables(val.should_not_contain)
                        passed = passed and (forbidden not in output)

                    if passed:
                        break

                    if attempt < val.retries - 1:
                        await asyncio.sleep(5)

                results["checks"].append({
                    "name": val.name,
                    "success": passed,
                    "output": output[:200]
                })

                if not passed:
                    results["success"] = False

            return results

        async def run_rollback(self) -> dict:
            """Execute rollback commands in reverse order."""
            logger.info("Running rollback...")
            results = []

            for cmd in reversed(self.rollback_stack):
                success, output = await self._execute_command(cmd, 60)
                results.append({"command": cmd, "success": success})

            return {"rollback_results": results}

        def format_escalation(self) -> str:
            """Format escalation message for Claude."""
            template = self.runbook.escalation_template or self._default_escalation_template()
            return self._substitute_variables(template)

        def _default_escalation_template(self) -> str:
            """Default escalation template if none provided."""
            return """## Alert Escalation: {{alertname}}

    ### Original Alert
    - Name: {{alertname}}
    - Severity: {{severity}}
    - Description: {{description}}
    - Namespace: {{namespace}}

    ### Diagnosis Attempted
    - Runbook: """ + self.runbook.name + """
    - Confidence: """ + str(self.confidence) + """
    - Diagnosis: """ + self.diagnosis_result + """
    - Escalation Reason: """ + str(self.escalation_reason) + """

    ### Gathered Information
    """ + "\n".join([f"**{k}:**\n```\n{str(v)[:500]}\n```" for k, v in self.variables.items() if k not in ["alertname", "severity", "description", "namespace"]]) + """

    ### Execution Log
    ```
    """ + "\n".join(self.execution_log[-20:]) + """
    ```

    ### Request
    Please analyze this issue and provide:
    1. Root cause analysis
    2. Recommended fix with commands
    3. A runbook definition for future occurrences

    Return the runbook as JSON at the end with this structure:
    ```json
    {
      "runbook_name": "...",
      "runbook_description": "...",
      "triggers": {"alert_patterns": [...], "keywords": [...]},
      "diagnosis_checks": [{"name": "...", "command": "...", "look_for": [...]}],
      "fix_steps": [{"name": "...", "command": "..."}],
      "validation": [{"name": "...", "command": "...", "expected": "..."}]
    }
    ```"""

        async def execute(self) -> dict:
            """Main execution entry point."""
            start_time = datetime.utcnow()
            result = {
                "runbook_id": self.runbook.id,
                "runbook_name": self.runbook.name,
                "alert": self.alert,
                "started_at": start_time.isoformat()
            }

            try:
                # Phase 1: Diagnosis
                diag_results = await self.run_diagnosis()
                result["diagnosis"] = diag_results
                self.calculate_confidence()
                result["confidence"] = self.confidence
                result["diagnosis_result"] = self.diagnosis_result

                # Phase 2: Gather information (always)
                gathered = await self.gather_information()
                result["gathered_info"] = gathered

                # Phase 3: Decision
                should_esc, reason = self.should_escalate()
                result["escalated"] = should_esc

                if should_esc:
                    result["escalation_reason"] = reason
                    result["escalation_message"] = self.format_escalation()
                    result["action"] = "escalate"
                    return result

                # Phase 4: Check preconditions
                pre_ok, pre_reason = self.check_preconditions()
                if not pre_ok:
                    result["action"] = "escalate"
                    result["escalation_reason"] = f"Precondition failed: {pre_reason}"
                    result["escalation_message"] = self.format_escalation()
                    return result

                # Phase 5: Execute fix (requires approval first)
                result["action"] = "fix_ready"
                result["fix_steps"] = [s.model_dump() for s in self.runbook.fix.steps]
                result["requires_approval"] = not self.runbook.metadata.auto_approve_eligible

                return result

            except Exception as e:
                logger.error(f"Runbook execution error: {e}")
                result["action"] = "error"
                result["error"] = str(e)
                return result

        async def execute_approved_fix(self) -> dict:
            """Execute the fix after approval."""
            result = {"runbook_id": self.runbook.id}

            try:
                fix_result = await self.run_fix()
                result["fix_result"] = fix_result

                if fix_result["success"]:
                    val_result = await self.run_validation()
                    result["validation"] = val_result

                    if val_result["success"]:
                        result["action"] = "completed"
                        result["success"] = True
                    else:
                        # Validation failed, rollback
                        rollback_result = await self.run_rollback()
                        result["rollback"] = rollback_result
                        result["action"] = "rolled_back"
                        result["success"] = False
                else:
                    # Fix failed, rollback
                    rollback_result = await self.run_rollback()
                    result["rollback"] = rollback_result
                    result["action"] = "rolled_back"
                    result["success"] = False

                return result

            except Exception as e:
                result["action"] = "error"
                result["error"] = str(e)
                result["success"] = False
                return result

    class AgentState(TypedDict):
        messages: Annotated[Sequence[BaseMessage], lambda x, y: x + y]
        alert: dict
        assessment: dict
        solutions: list
        selected_solution: dict
        approval_status: str
        execution_result: dict
        runbook_id: str
        topic_id: int
        thread_id: str
        runbook_match: dict  # Matched runbook from Qdrant if found
        handled_locally: bool  # Whether issue was handled with existing runbook
        # Project 05: Skill Router fields
        skill_id: str  # ID of loaded skill (e.g., "infrastructure-skill")
        skill_context: dict  # Skill metadata (system_prompt, runbook_patterns, etc.)
        loaded_mcps: list  # List of loaded MCP server names
        execution_checkpoint: dict  # Checkpoint for rollback support (persisted)

    pending_approvals = {}
    app = FastAPI(title="LangGraph Orchestrator")

    class AlertInput(BaseModel):
        id: str
        alertname: str
        severity: str = "warning"
        namespace: str = "default"
        description: str = ""
        labels: Optional[dict] = None
        annotations: Optional[dict] = None

    class KeepAlert(BaseModel):
        """Keep alert format - maps to AlertInput internally."""
        fingerprint: Optional[str] = None
        id: Optional[str] = None
        name: Optional[str] = None
        alertname: Optional[str] = None  # Alternative field name
        status: str = "firing"
        severity: str = "warning"
        lastReceived: Optional[str] = None
        firingStartTime: Optional[str] = None
        firingStartTimeSinceLastResolved: Optional[str] = None
        firingCounter: Optional[int] = None
        unresolvedCounter: Optional[int] = None
        description: Optional[str] = None
        message: Optional[str] = None
        labels: Optional[dict] = None
        annotations: Optional[dict] = None
        source: Optional[list] = None
        url: Optional[str] = None  # Coroot incident link
        generatorURL: Optional[str] = None  # Prometheus query link
        cluster: Optional[str] = None
        namespace: Optional[str] = None  # Top-level namespace from Keep
        pod: Optional[str] = None
        deployment: Optional[str] = None
        daemonset: Optional[str] = None
        service: Optional[str] = None
        container: Optional[str] = None
        payload: Optional[dict] = None  # Raw payload with startsAt/endsAt
        alert_hash: Optional[str] = None
        startedAt: Optional[str] = None
        firstTimestamp: Optional[str] = None
        # Accept any additional fields Keep might send
        class Config:
            extra = "allow"

    class ApprovalRequest(BaseModel):
        alert_id: str
        solution_index: int
        approved_by: str

    class IgnoreRequest(BaseModel):
        alert_id: str
        ignored_by: str

    async def call_gemini(prompt: str, context: dict = None, model: str = None) -> str:
        """
        Call Gemini via LiteLLM for operational tasks.
        Uses Gemini Pro by default (1M token context window).
        """
        model = model or GEMINI_MODEL
        messages = []

        # Always include comprehensive system prompt
        system_content = AGENT_SYSTEM_PROMPT
        if context:
            system_content += "\n\n## CURRENT CONTEXT:\n" + json.dumps(context, indent=2, default=str)
        messages.append({"role": "system", "content": system_content})

        messages.append({"role": "user", "content": prompt})

        async with httpx.AsyncClient(timeout=120.0) as client:
            try:
                response = await client.post(
                    LITELLM_URL + "/v1/chat/completions",
                    json={"model": model, "messages": messages}
                )
                response.raise_for_status()
                return response.json()["choices"][0]["message"]["content"]
            except Exception as e:
                logger.error("Gemini call failed: %s", e)
                return "Error: " + str(e)

    # Alias for backward compatibility
    async def call_litellm(messages, model=None):
        model = model or GEMINI_MODEL
        async with httpx.AsyncClient(timeout=120.0) as client:
            try:
                response = await client.post(
                    LITELLM_URL + "/v1/chat/completions",
                    json={"model": model, "messages": messages}
                )
                response.raise_for_status()
                return response.json()["choices"][0]["message"]["content"]
            except Exception as e:
                logger.error("LiteLLM call failed: %s", e)
                return "Error: " + str(e)

    async def call_knowledge_mcp(endpoint, method="GET", data=None):
        async with httpx.AsyncClient(timeout=30.0) as client:
            try:
                url = KNOWLEDGE_MCP_URL + "/" + endpoint
                if method == "GET":
                    response = await client.get(url)
                else:
                    response = await client.post(url, json=data)
                response.raise_for_status()
                return response.json()
            except Exception as e:
                logger.error("Knowledge MCP call failed: %s", e)
                return {"error": str(e)}

    async def call_matrix(endpoint: str, data: dict) -> dict:
        """Send notifications and requests to Matrix bot."""
        async with httpx.AsyncClient(timeout=30.0) as client:
            try:
                response = await client.post(MATRIX_BOT_URL + "/" + endpoint, json=data)
                response.raise_for_status()
                return response.json()
            except Exception as e:
                logger.error("Matrix bot call failed: %s", e)
                return {"error": str(e)}

    async def _fetch_coroot_metrics_for_alert(alert: dict) -> dict:
        """Fetch Coroot service metrics for an alert to enrich Afferent display."""
        import re
        labels = alert.get("labels", {})
        alert_url = alert.get("url", "") or ""

        # Build Coroot app_id from alert data
        coroot_app_id = None
        if "coroot" in alert_url:
            url_match = re.search(r'/p/([^/]+)/', alert_url)
            if url_match:
                cluster_id = url_match.group(1)
                ns = labels.get("namespace", alert.get("namespace", ""))
                kind = labels.get("kind", "Deployment")
                app_name = labels.get("application", alert.get("alertname", ""))
                if ns and app_name:
                    coroot_app_id = f"{cluster_id}:{ns}:{kind}:{app_name}"

        if not coroot_app_id:
            return {}

        try:
            metrics = await call_coroot_mcp("coroot_get_service_metrics", params={"app_id": coroot_app_id})
            if metrics:
                return {"app_id": coroot_app_id, "raw": metrics}
        except Exception as e:
            logger.warning(f"Failed to fetch Coroot metrics for {coroot_app_id}: {e}")
        return {}

    async def notify_afferent(alert: dict, solutions: list, assessment: dict) -> dict:
        """Send approval request to Afferent mobile PWA webhook."""
        if not AFFERENT_WEBHOOK_URL:
            return {"skipped": True, "reason": "AFFERENT_WEBHOOK_URL not configured"}

        # Format plan from first solution for mobile display
        plan = {}
        if solutions:
            sol = solutions[0]
            plan = {
                "summary": sol.get("description", "Execute remediation"),
                "risk_level": sol.get("risk", "unknown"),
                "steps": sol.get("steps", []),
                "rollback": sol.get("rollback", "Manual intervention required")
            }

        # Get source - could be list or string from Keep
        source = alert.get("source")
        if isinstance(source, list):
            source_str = source[0] if source else "unknown"
        else:
            source_str = source or "unknown"

        # Fetch Coroot service metrics for enrichment
        coroot_data = await _fetch_coroot_metrics_for_alert(alert)

        payload = {
            "id": alert.get("id"),
            "alert_fingerprint": alert.get("id"),
            "alert_name": alert.get("alertname", alert.get("name", "Unknown Alert")),
            "description": alert.get("description", ""),
            "service": alert.get("namespace", "unknown"),
            "severity": alert.get("severity", "warning"),
            "source": source_str,
            "alert_source": source_str,  # Original source (coroot, prometheus, etc.)
            "labels": alert.get("labels", {}),
            "annotations": alert.get("annotations", {}),
            "plan": plan,
            # Rich context from Keep for detailed mobile display
            "url": alert.get("url"),  # Coroot incident link
            "generatorURL": alert.get("generatorURL"),  # Prometheus query link
            "cluster": alert.get("cluster"),
            "namespace": alert.get("namespace", "unknown"),
            "pod": alert.get("pod"),
            "deployment": alert.get("deployment"),
            "daemonset": alert.get("daemonset"),
            "container": alert.get("container"),
            "firingStartTime": alert.get("firingStartTime"),
            "firingCounter": alert.get("firingCounter"),
            "lastReceived": alert.get("lastReceived"),
            "startedAt": alert.get("startedAt"),
            "payload": alert.get("payload"),
            # Coroot service metrics (CPU, memory, latency, error rate)
            "coroot_metrics": coroot_data if coroot_data else None,
            # Include assessment context for better understanding
            "assessment": {
                "confidence": assessment.get("confidence", 0),
                "similar_runbook": assessment.get("similar_runbook"),
                "similarity_score": assessment.get("similarity_score", 0),
                "recommendation": assessment.get("recommendation", "")
            }
        }

        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                response = await client.post(AFFERENT_WEBHOOK_URL, json=payload)
                if response.status_code < 300:
                    logger.info(f"Afferent notified for alert {alert.get('id')}")
                    return {"success": True}
                else:
                    logger.warning(f"Afferent notification failed: {response.status_code}")
                    return {"error": f"HTTP {response.status_code}"}
            except Exception as e:
                logger.error(f"Afferent notification error: {e}")
                return {"error": str(e)}

    # ============================================================================
    # A2A ORCHESTRATOR - Gemini-powered parallel specialist agents
    # ============================================================================

    async def call_a2a_investigate(alert: dict, request_id: str) -> dict:
        """Call A2A /v1/investigate endpoint for parallel specialist investigation."""
        if not A2A_ENABLED:
            logger.info("A2A disabled, skipping investigation")
            return {"error": "A2A disabled", "fallback": True}

        async with httpx.AsyncClient(timeout=A2A_TIMEOUT) as client:
            try:
                response = await client.post(
                    A2A_URL + "/v1/investigate",
                    json={
                        "request_id": request_id,
                        "alert": {
                            "name": alert.get("alertname", alert.get("name", "unknown")),
                            "labels": {
                                "namespace": alert.get("namespace"),
                                "pod": alert.get("pod"),
                                "service": alert.get("service"),
                                "node": alert.get("node"),
                            },
                            "severity": alert.get("severity", "warning"),
                            "description": alert.get("description", alert.get("summary", "")),
                            "fingerprint": alert.get("fingerprint", alert.get("id", "")),
                        },
                        "context": {}
                    }
                )
                response.raise_for_status()
                return response.json()
            except httpx.TimeoutException:
                logger.warning("A2A investigate timed out after %ss", A2A_TIMEOUT)
                return {"error": "timeout", "fallback": True}
            except Exception as e:
                logger.error("A2A investigate failed: %s", e)
                return {"error": str(e), "fallback": True}

    async def call_a2a_plan_and_decide(alert: dict, investigation: dict, request_id: str) -> dict:
        """Call A2A /v1/plan endpoint for runbook matching and decision.

        Note: Uses /v1/plan (preferred) - /v1/plan_and_decide is deprecated alias.
        """
        if not A2A_ENABLED:
            logger.info("A2A disabled, skipping plan")
            return {"error": "A2A disabled", "fallback": True}

        async with httpx.AsyncClient(timeout=A2A_TIMEOUT) as client:
            try:
                response = await client.post(
                    A2A_URL + "/v1/plan",
                    json={
                        "request_id": request_id,
                        "alert": {
                            "name": alert.get("alertname", alert.get("name", "unknown")),
                            "labels": {
                                "namespace": alert.get("namespace"),
                                "pod": alert.get("pod"),
                                "service": alert.get("service"),
                                "node": alert.get("node"),
                            },
                            "severity": alert.get("severity", "warning"),
                            "description": alert.get("description", alert.get("summary", "")),
                            "fingerprint": alert.get("fingerprint", alert.get("id", "")),
                        },
                        "investigation": investigation,
                        "context": {}
                    }
                )
                response.raise_for_status()
                return response.json()
            except httpx.TimeoutException:
                logger.warning("A2A plan timed out after %ss", A2A_TIMEOUT)
                return {"error": "timeout", "fallback": True}
            except Exception as e:
                logger.error("A2A plan failed: %s", e)
                return {"error": str(e), "fallback": True}

    async def call_a2a_validate_and_document(
        alert: dict,
        investigation: dict,
        plan: dict,
        execution_result: dict,
        request_id: str
    ) -> dict:
        """Call A2A /v1/validate endpoint for resolution validation.

        Note: Uses /v1/validate (preferred) - /v1/validate_and_document is deprecated alias.
        """
        if not A2A_ENABLED:
            logger.info("A2A disabled, skipping validate")
            return {"error": "A2A disabled", "fallback": True}

        async with httpx.AsyncClient(timeout=A2A_TIMEOUT) as client:
            try:
                response = await client.post(
                    A2A_URL + "/v1/validate",
                    json={
                        "request_id": request_id,
                        "alert": {
                            "name": alert.get("alertname", alert.get("name", "unknown")),
                            "labels": {
                                "namespace": alert.get("namespace"),
                                "pod": alert.get("pod"),
                                "service": alert.get("service"),
                                "node": alert.get("node"),
                            },
                            "severity": alert.get("severity", "warning"),
                            "description": alert.get("description", alert.get("summary", "")),
                            "fingerprint": alert.get("fingerprint", alert.get("id", "")),
                        },
                        "investigation": investigation,
                        "plan": plan,
                        "execution_result": execution_result,
                        "context": {}
                    }
                )
                response.raise_for_status()
                return response.json()
            except httpx.TimeoutException:
                logger.warning("A2A validate timed out after %ss", A2A_TIMEOUT)
                return {"error": "timeout", "fallback": True}
            except Exception as e:
                logger.error("A2A validate failed: %s", e)
                return {"error": str(e), "fallback": True}

    # ============================================================================
    # BRAIN TRUST ESCALATION - Outline-based human review workflow
    # ============================================================================

    async def call_outline_mcp(tool_name: str, **kwargs) -> dict:
        """Call Outline MCP for wiki document operations."""
        async with httpx.AsyncClient(timeout=30.0) as client:
            try:
                response = await client.post(
                    OUTLINE_MCP_URL + "/mcp",
                    json={
                        "jsonrpc": "2.0",
                        "id": 1,
                        "method": "tools/call",
                        "params": {"name": tool_name, "arguments": kwargs}
                    }
                )
                if response.status_code == 200:
                    result = response.json()
                    content = result.get("result", {}).get("content", [])
                    if content:
                        text = content[0].get("text", "{}")
                        try:
                            return json.loads(text)
                        except:
                            return {"result": text}
                return {"error": f"HTTP {response.status_code}"}
            except Exception as e:
                logger.error(f"Outline MCP call failed: {e}")
                return {"error": str(e)}

    def format_brain_trust_document(alert: dict, context: dict, reason: str, suggestions: list = None) -> str:
        """Format an escalation document for Brain Trust review in Outline."""
        now = datetime.utcnow().isoformat()
        doc = dedent(f"""\
            # {alert.get('alertname', 'Unknown Alert')} - Escalated {now[:10]}

            ## Alert Details
            | Field | Value |
            |-------|-------|
            | Severity | {alert.get('severity', 'warning')} |
            | Source | {alert.get('source', ['alertmanager'])} |
            | Time | {alert.get('startsAt', now)} |
            | Namespace | {alert.get('namespace', 'default')} |

            ### Description
            {alert.get('description', 'No description provided')}

            ### Labels
            ```json
            {json.dumps(alert.get('labels', {}), indent=2)}
            ```

            ## Context Gathered (via MCPs)

            """)
        # Add gathered context
        if context.get("gathered_info"):
            for key, value in context["gathered_info"].items():
                doc += f"### {key}\n```\n{str(value)[:2000]}\n```\n\n"

        if context.get("diagnosis"):
            doc += dedent(f"""\
                ### Diagnosis Results
                - Result: {context['diagnosis'].get('diagnosis_result', 'unknown')}
                - Confidence: {context['diagnosis'].get('confidence', 0):.0%}

                """)

        doc += dedent(f"""\
            ## Why Escalated
            **Reason**: {reason}

            Ollama (qwen2.5:7b) analyzed this alert but determined it requires human review because:
            - {reason}

            """)

        if suggestions:
            doc += dedent("""\
                ## Suggested Approaches
                The following approaches were considered by Ollama:

                """)
            for i, suggestion in enumerate(suggestions, 1):
                doc += f"{i}. **{suggestion.get('name', 'Suggestion')}**\n"
                doc += f"   - {suggestion.get('description', 'No description')}\n"
                if suggestion.get('risk'):
                    doc += f"   - Risk: {suggestion['risk']}\n"
                doc += "\n"

        doc += dedent("""\
            ## Brain Trust Decision
            <!-- Human reviewer: Fill in your decision below -->

            ### Approved Solution
            _Describe the solution to implement_

            ### Implementation Notes
            _Any special considerations_

            ### Assigned To
            _Who will implement (e.g., "Claude Code session")_

            ---
            *This document was auto-generated by the LangGraph Alert Handler.*
            *Collection: Brain Trust Review*
            """)
        return doc

    async def escalate_to_brain_trust(alert: dict, context: dict, reason: str, suggestions: list = None) -> dict:
        """
        Escalate an alert to Brain Trust (humans) via Outline wiki.

        This is the new escalation path replacing cloud LLM escalation.
        Creates a document in the Brain Trust Review collection for human review.

        Args:
            alert: Alert details dict
            context: Gathered context including diagnosis, MCP data
            reason: Why Ollama couldn't handle this autonomously
            suggestions: Optional list of suggested approaches

        Returns:
            Dict with escalation status and document link
        """
        logger.info(f"Escalating to Brain Trust: {alert.get('alertname')} - {reason}")

        # Format the escalation document
        doc_content = format_brain_trust_document(alert, context, reason, suggestions)
        doc_title = f"{alert.get('alertname', 'Alert')} - Escalated {datetime.utcnow().strftime('%Y-%m-%d %H:%M')}"

        # Create document in Outline via MCP
        result = await call_outline_mcp(
            "create_document",
            title=doc_title,
            collection_id=BRAIN_TRUST_COLLECTION_ID,
            text=doc_content,
            publish=True
        )

        if "error" in result:
            logger.error(f"Failed to create Brain Trust document: {result['error']}")
            # Fallback: notify via Matrix with error details
            await call_matrix("message", {
                "room_id": os.environ.get("ALERT_ROOM_ID", "!alerts:agentic.local"),
                "message": f" **BRAIN TRUST ESCALATION FAILED**\n\nAlert: {alert.get('alertname')}\nReason: {reason}\nError: {result['error']}\n\nPlease review manually."
            })
            return {"status": "error", "error": result["error"]}

        # Notify via Matrix using the new /brain-trust endpoint
        doc_id = result.get("id", "unknown")
        outline_url = f"https://outline.kernow.io/doc/{doc_id}"

        # Build context summary for notification
        context_summary = None
        if context:
            diagnosis = context.get("diagnosis", {})
            if diagnosis:
                context_summary = f"Diagnosis: {diagnosis.get('summary', 'N/A')[:300]}"

        await call_matrix("brain-trust", {
            "alert_name": alert.get("alertname", "Unknown Alert"),
            "alert_id": alert.get("alert_id", alert.get("fingerprint", str(hash(str(alert)))[:12])),
            "severity": alert.get("severity", "info"),
            "reason": reason,
            "outline_url": outline_url,
            "outline_doc_id": doc_id,
            "context_summary": context_summary,
            "suggestions": suggestions
        })

        logger.info(f"Created Brain Trust document: {doc_id}")
        return {
            "status": "escalated",
            "document_id": doc_id,
            "collection": "Brain Trust Review",
            "reason": reason
        }

    async def propose_runbook_update(runbook_match: dict, tweaks: list, execution_result: dict) -> dict:
        """
        After executing a SIMILAR runbook with tweaks, propose an update.

        Creates a document in Brain Trust Review suggesting runbook improvements
        based on what worked.

        Args:
            runbook_match: The original matched runbook
            tweaks: List of tweaks/modifications made
            execution_result: The result of executing with tweaks

        Returns:
            Dict with proposal status
        """
        logger.info(f"Proposing runbook update for: {runbook_match.get('name')}")

        doc_content = dedent(f"""\
            # Runbook Update Proposal: {runbook_match.get('name')}

            ## Summary
            A similar alert was handled using runbook **{runbook_match.get('name')}** with modifications.
            The modifications were successful and should be considered for incorporation.

            ## Original Runbook
            - **ID**: {runbook_match.get('id')}
            - **Match Score**: {runbook_match.get('score', 0):.0%}
            - **Version**: {runbook_match.get('version', 1)}

            ## Modifications Made
            The following tweaks were applied to handle this specific case:

            """)
        for i, tweak in enumerate(tweaks, 1):
            doc_content += f"{i}. {tweak}\n"

        doc_content += dedent(f"""\

            ## Execution Result
            - **Status**: {'Success' if execution_result.get('success') else 'Partial'}
            - **Confidence**: {execution_result.get('confidence', 0):.0%}
            - **Diagnosis**: {execution_result.get('diagnosis_result', 'N/A')}

            ## Recommendation
            Consider updating the runbook to handle this case natively. Options:
            1. Add new trigger patterns to capture this alert type
            2. Add conditional logic to the fix steps
            3. Create a new runbook variant for this specific scenario

            ## Brain Trust Decision
            <!-- Human reviewer: Approve update, create variant, or dismiss -->

            ---
            *Auto-generated by LangGraph runbook learning system*
            """)

        result = await call_outline_mcp(
            "create_document",
            title=f"Runbook Update: {runbook_match.get('name')} - {datetime.utcnow().strftime('%Y-%m-%d')}",
            collection_id=BRAIN_TRUST_COLLECTION_ID,
            text=doc_content,
            publish=True
        )

        if "error" not in result:
            doc_id = result.get("id", "unknown")
            outline_url = f"https://outline.kernow.io/doc/{doc_id}"

            # Notify via Matrix using the new /runbook-proposal endpoint
            await call_matrix("runbook-proposal", {
                "alert_name": execution_result.get("alert_name", "Unknown Alert"),
                "runbook_title": runbook_match.get("name", "Unknown Runbook"),
                "tweaks_made": "\n".join(f"- {t}" for t in tweaks[:5]),
                "outline_url": outline_url,
                "execution_success": execution_result.get("success", True)
            })

        return result

    def classify_runbook_match(score: float) -> str:
        """Classify runbook match into EXACT, SIMILAR, or NO_MATCH."""
        if score >= RUNBOOK_EXACT_THRESHOLD:
            return "EXACT"
        elif score >= RUNBOOK_SIMILAR_THRESHOLD:
            return "SIMILAR"
        else:
            return "NO_MATCH"

    async def call_coroot_mcp(tool_name: str, **kwargs) -> dict:
        """Call Coroot MCP for metrics and anomaly data."""
        async with httpx.AsyncClient(timeout=30.0) as client:
            try:
                response = await client.post(
                    COROOT_MCP_URL + "/mcp",
                    json={
                        "jsonrpc": "2.0",
                        "id": 1,
                        "method": "tools/call",
                        "params": {"name": tool_name, "arguments": kwargs}
                    }
                )
                if response.status_code == 200:
                    result = response.json()
                    content = result.get("result", {}).get("content", [])
                    if content:
                        return json.loads(content[0].get("text", "{}"))
                return {}
            except Exception as e:
                logger.warning(f"Coroot MCP call failed: {e}")
                return {}

    async def discover_service(service_name: str) -> dict:
        """
        Discover service endpoint and credentials from Qdrant entities collection.
        Use this before calling infrastructure services to get their endpoints.
        """
        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                # Search entities collection in Qdrant
                response = await client.post(
                    QDRANT_URL + "/collections/entities/points/scroll",
                    json={
                        "limit": 100,
                        "with_payload": True,
                        "filter": {
                            "should": [
                                {"key": "hostname", "match": {"text": service_name}},
                                {"key": "type", "match": {"text": service_name}}
                            ]
                        } if service_name != "all" else None
                    }
                )
                if response.status_code == 200:
                    result = response.json()
                    points = result.get("result", {}).get("points", [])
                    return {"entities": [p.get("payload", {}) for p in points]}
                return {}
            except Exception as e:
                logger.warning(f"Service discovery failed for {service_name}: {e}")
                return {}

    async def get_credentials_path(service_name: str) -> str:
        """Get Infisical credentials path for a service from Qdrant device_types."""
        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                # Search device_types collection for credentials path
                response = await client.post(
                    QDRANT_URL + "/collections/device_types/points/scroll",
                    json={
                        "limit": 10,
                        "with_payload": True,
                        "filter": {
                            "must": [
                                {"key": "name", "match": {"text": service_name}}
                            ]
                        }
                    }
                )
                if response.status_code == 200:
                    result = response.json()
                    points = result.get("result", {}).get("points", [])
                    if points:
                        return points[0].get("payload", {}).get("default_credentials_path", "")
                return ""
            except Exception as e:
                logger.warning(f"Credentials path lookup failed for {service_name}: {e}")
                return ""

    async def get_embedding(text: str) -> list:
        """Get embedding from Gemini via LiteLLM."""
        async with httpx.AsyncClient(timeout=30.0) as client:
            try:
                response = await client.post(
                    LITELLM_URL + "/v1/embeddings",
                    json={"model": EMBEDDING_MODEL, "input": text}
                )
                if response.status_code == 200:
                    data = response.json()
                    if data.get("data") and len(data["data"]) > 0:
                        return data["data"][0].get("embedding", [])
                logger.warning("Embedding API returned %d", response.status_code)
                return []
            except Exception as e:
                logger.error("Embedding error: %s", e)
                return []

    async def build_comprehensive_context(alert: dict) -> dict:
        """
        Build comprehensive context for Gemini using its 1M token window.
        Combines static cached context with dynamic per-request context.
        """
        global _static_context_cache, _static_context_timestamp

        alert_text = f"{alert.get('alertname', '')} {alert.get('description', '')}"
        alert_embedding = await get_embedding(alert_text)

        # Check if static context needs refresh (hourly)
        now = datetime.utcnow()
        if (_static_context_timestamp is None or
            (now - _static_context_timestamp).total_seconds() > CONTEXT_CACHE_TTL):
            logger.info("Refreshing static context cache...")
            _static_context_cache = await _fetch_static_context()
            _static_context_timestamp = now

        # Fetch dynamic context (always fresh)
        dynamic_context = await _fetch_dynamic_context(alert, alert_embedding)

        return {
            **_static_context_cache,
            **dynamic_context,
            "alert": alert
        }

    async def _fetch_static_context() -> dict:
        """Fetch context that changes infrequently (cached hourly)."""
        context = {}

        # Get all runbooks from Qdrant
        async with httpx.AsyncClient(timeout=30.0) as client:
            try:
                response = await client.post(
                    QDRANT_URL + "/collections/runbooks/points/scroll",
                    json={"limit": 100, "with_payload": True}
                )
                if response.status_code == 200:
                    points = response.json().get("result", {}).get("points", [])
                    context["runbook_library"] = [p.get("payload", {}) for p in points]
            except Exception as e:
                logger.warning(f"Failed to fetch runbooks: {e}")
                context["runbook_library"] = []

        # Get documentation from knowledge MCP
        try:
            docs = await call_knowledge_mcp("documentation", "GET")
            context["documentation"] = docs if not docs.get("error") else []
        except:
            context["documentation"] = []

        # Get entity inventory from Qdrant
        try:
            inventory = await discover_service("all")
            context["entity_inventory"] = inventory
        except:
            context["entity_inventory"] = {}

        return context

    async def _fetch_dynamic_context(alert: dict, alert_embedding: list) -> dict:
        """Fetch fresh context for each request."""
        context = {}

        # Get cluster state from infrastructure MCP
        async with httpx.AsyncClient(timeout=15.0) as client:
            try:
                response = await client.get(INFRASTRUCTURE_MCP_URL + "/cluster/status")
                if response.status_code == 200:
                    context["cluster_state"] = response.json()
            except:
                context["cluster_state"] = {}

        # Get Coroot metrics and anomalies for affected service
        # Build Coroot app_id from alert labels: cluster_id:namespace:Kind:name
        coroot_app_id = None
        alert_url = alert.get("url", "")
        labels = alert.get("labels", {})
        if alert_url and "coroot" in str(alert_url):
            # Extract cluster_id from Coroot URL: https://coroot.kernow.io/p/{cluster_id}/...
            import re
            url_match = re.search(r'/p/([^/]+)/', str(alert_url))
            if url_match:
                cluster_id = url_match.group(1)
                ns = labels.get("namespace", alert.get("namespace", ""))
                kind = labels.get("kind", "Deployment")
                app_name = labels.get("application", alert.get("alertname", ""))
                if ns and app_name:
                    coroot_app_id = f"{cluster_id}:{ns}:{kind}:{app_name}"

        if coroot_app_id:
            context["coroot_metrics"] = await call_coroot_mcp("coroot_get_service_metrics", params={"app_id": coroot_app_id})
            context["coroot_anomalies"] = await call_coroot_mcp("coroot_get_recent_anomalies", params={"hours": 24})
            context["service_dependencies"] = await call_coroot_mcp("coroot_get_service_dependencies", params={"app_id": coroot_app_id})
        elif labels.get("namespace"):
            # For Prometheus alerts, try anomalies at least
            context["coroot_anomalies"] = await call_coroot_mcp("coroot_get_recent_anomalies", params={"hours": 24})

        # Get similar past decisions from Qdrant
        if alert_embedding:
            async with httpx.AsyncClient(timeout=10.0) as client:
                try:
                    response = await client.post(
                        QDRANT_URL + "/collections/decisions/points/search",
                        json={"vector": alert_embedding, "limit": 5, "with_payload": True}
                    )
                    if response.status_code == 200:
                        hits = response.json().get("result", [])
                        context["similar_past_decisions"] = [h.get("payload", {}) for h in hits]
                except:
                    context["similar_past_decisions"] = []

        # Get recent decisions (last 24h)
        context["recent_decisions_24h"] = await _get_recent_decisions(hours=24)

        return context

    async def _get_recent_decisions(hours: int = 24) -> list:
        """Get recent decisions from Qdrant."""
        cutoff = (datetime.utcnow() - timedelta(hours=hours)).isoformat()
        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                response = await client.post(
                    QDRANT_URL + "/collections/decisions/points/scroll",
                    json={
                        "limit": 50,
                        "with_payload": True,
                        "filter": {
                            "must": [
                                {"key": "timestamp", "range": {"gte": cutoff}}
                            ]
                        }
                    }
                )
                if response.status_code == 200:
                    points = response.json().get("result", {}).get("points", [])
                    return [p.get("payload", {}) for p in points]
            except:
                pass
        return []

    async def detect_capability_gap(gemini_response: str, alert: dict) -> Optional[dict]:
        """Detect if Gemini indicates a missing capability."""
        gap_signals = [
            "I don't have access to",
            "I cannot query",
            "No MCP available for",
            "I need a tool to",
            "Missing capability:",
            "Unable to retrieve",
        ]

        for signal in gap_signals:
            if signal.lower() in gemini_response.lower():
                # Extract description of what's needed
                gap = {
                    "description": f"Capability gap detected while processing alert: {alert.get('alertname')}",
                    "tools_needed": [],
                    "triggering_alerts": [alert.get("id", "")],
                    "gemini_response_excerpt": gemini_response[:500]
                }

                # Store to Qdrant for Claude Validator to process
                await store_capability_gap(gap)
                return gap

        return None

    async def store_capability_gap(gap: dict):
        """Store capability gap to Qdrant for later processing."""
        embedding = await get_embedding(gap.get("description", ""))
        if not embedding:
            return

        gap_id = f"cap-gap-{uuid_mod.uuid4().hex[:8]}"
        gap["id"] = gap_id
        gap["created_at"] = datetime.utcnow().isoformat()
        gap["status"] = "pending"

        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                await client.put(
                    QDRANT_URL + "/collections/capability_gaps/points",
                    json={
                        "points": [{
                            "id": str(uuid_mod.uuid5(uuid_mod.NAMESPACE_DNS, gap_id)),
                            "vector": embedding,
                            "payload": gap
                        }]
                    }
                )
                logger.info(f"Stored capability gap: {gap_id}")
            except Exception as e:
                logger.error(f"Failed to store capability gap: {e}")

    async def search_runbooks_qdrant(query: str, limit: int = 3):
        """Search Qdrant for relevant diagnostic runbooks."""
        embedding = await get_embedding(query)
        if not embedding:
            return []
        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                response = await client.post(
                    QDRANT_URL + "/collections/runbooks/points/search",
                    json={"vector": embedding, "limit": limit, "with_payload": True, "score_threshold": 0.5}
                )
                if response.status_code == 200:
                    results = []
                    for hit in response.json().get("result", []):
                        payload = hit.get("payload", {})
                        # Check if it's a v2 diagnostic runbook or legacy
                        if "diagnosis" in payload or "triggers" in payload:
                            # V2 diagnostic runbook - parse into Runbook model
                            try:
                                runbook = Runbook(**payload)
                                results.append({
                                    "id": hit.get("id"),
                                    "score": hit.get("score"),
                                    "runbook": runbook,
                                    "version": 2
                                })
                            except Exception as e:
                                logger.warning(f"Failed to parse runbook: {e}")
                        else:
                            # Legacy simple runbook
                            results.append({
                                "id": hit.get("id"),
                                "score": hit.get("score"),
                                "name": payload.get("name", "Unknown"),
                                "description": payload.get("description", ""),
                                "steps": payload.get("steps", []),
                                "fix_command": payload.get("fix_command", ""),
                                "version": 1
                            })
                    return results
                return []
            except Exception as e:
                logger.error("Runbook search error: %s", e)
                return []

    async def store_runbook_qdrant(runbook: Runbook) -> bool:
        """Store a diagnostic runbook in Qdrant."""
        search_text = f"{runbook.name} {runbook.description} {' '.join(runbook.triggers.keywords)}"
        embedding = await get_embedding(search_text)
        if not embedding:
            logger.error("Failed to generate embedding for runbook")
            return False
        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                payload = runbook.model_dump()
                # Generate a proper UUID for Qdrant (it doesn't accept arbitrary strings)
                point_id = str(uuid_mod.uuid5(uuid_mod.NAMESPACE_DNS, runbook.id))
                response = await client.put(
                    QDRANT_URL + "/collections/runbooks/points",
                    json={
                        "points": [{
                            "id": point_id,
                            "vector": embedding,
                            "payload": payload
                        }]
                    }
                )
                if response.status_code == 200:
                    logger.info(f"Stored runbook: {runbook.name} ({runbook.id}) -> point {point_id}")
                    return True
                else:
                    logger.error(f"Qdrant store failed: {response.status_code} - {response.text}")
                    return False
            except Exception as e:
                logger.error("Runbook store error: %s", e)
                return False

    async def update_runbook_stats(runbook_id: str, success: bool, escalated: bool = False):
        """Update runbook success/failure statistics."""
        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                # Get current runbook
                response = await client.get(
                    QDRANT_URL + f"/collections/runbooks/points/{runbook_id}"
                )
                if response.status_code != 200:
                    return False

                payload = response.json().get("result", {}).get("payload", {})
                metadata = payload.get("metadata", {})

                if success:
                    metadata["success_count"] = metadata.get("success_count", 0) + 1
                else:
                    metadata["failure_count"] = metadata.get("failure_count", 0) + 1
                if escalated:
                    metadata["escalation_count"] = metadata.get("escalation_count", 0) + 1

                # Check if eligible for auto-approve
                if (metadata.get("success_count", 0) >= AUTO_APPROVE_MIN_SUCCESSES and
                    metadata.get("failure_count", 0) == 0):
                    metadata["auto_approve_eligible"] = True
                    logger.info(f"Runbook {runbook_id} now eligible for auto-approve!")

                metadata["updated_at"] = datetime.utcnow().isoformat()
                payload["metadata"] = metadata

                # Update in Qdrant
                embedding = await get_embedding(f"{payload.get('name', '')} {payload.get('description', '')}")
                if embedding:
                    await client.put(
                        QDRANT_URL + "/collections/runbooks/points",
                        json={"points": [{"id": runbook_id, "vector": embedding, "payload": payload}]}
                    )
                return True
            except Exception as e:
                logger.error(f"Failed to update runbook stats: {e}")
                return False

    def parse_runbook_from_claude(response: str, alert: dict) -> Optional[Runbook]:
        """Parse a runbook definition from Claude's response."""
        try:
            # Find JSON block
            if "```json" in response:
                json_str = response.split("```json")[-1].split("```")[0]
            elif "```" in response:
                json_str = response.split("```")[-2] if response.count("```") >= 2 else response.split("```")[1]
                json_str = json_str.split("```")[0]
            else:
                return None

            data = json.loads(json_str.strip())

            # Build Runbook from Claude's response
            runbook = Runbook(
                name=data.get("runbook_name", f"Alert: {alert.get('alertname', 'Unknown')}"),
                description=data.get("runbook_description", alert.get("description", "")),
                triggers=TriggerConfig(
                    alert_patterns=data.get("triggers", {}).get("alert_patterns", [alert.get("alertname", "")]),
                    keywords=data.get("triggers", {}).get("keywords", [])
                ),
                diagnosis=DiagnosisConfig(
                    description="Verify the issue",
                    checks=[
                        DiagnosisCheck(
                            name=c.get("name", "Check"),
                            command=c.get("command", "echo 'no command'"),
                            look_for=[LookForPattern(pattern=p, confirms="issue") for p in c.get("look_for", [])]
                        )
                        for c in data.get("diagnosis_checks", [])
                    ],
                    confidence_rules=[
                        ConfidenceRule(condition="true", confidence=0.8, diagnosis="likely_match")
                    ]
                ),
                information_gathering=InfoGatherConfig(
                    description="Gather context",
                    commands=[
                        GatherCommand(name="Pod describe", command="kubectl describe pod {{pod_name}} -n {{namespace}}", store_as="pod_describe"),
                        GatherCommand(name="Pod logs", command="kubectl logs {{pod_name}} -n {{namespace}} --tail=50", store_as="pod_logs", on_failure="continue")
                    ]
                ),
                decision=DecisionRules(
                    escalate_if=["confidence < 0.7"],
                    handle_locally_if=["confidence >= 0.7"]
                ),
                fix=FixConfig(
                    description=data.get("runbook_description", "Apply fix"),
                    steps=[
                        FixStep(
                            name=s.get("name", "Step"),
                            action="command",
                            command=s.get("command", "echo 'no command'")
                        )
                        for s in data.get("fix_steps", [])
                    ],
                    validation=[
                        ValidationCheck(
                            name=v.get("name", "Validate"),
                            command=v.get("command", "echo 'ok'"),
                            expected=v.get("expected")
                        )
                        for v in data.get("validation", [])
                    ]
                ),
                metadata=RunbookMetadata(
                    created_by="claude",
                    tags=["auto-generated"]
                )
            )
            return runbook
        except Exception as e:
            logger.warning(f"Failed to parse runbook from Claude: {e}")
            return None

    # Priority levels (match claude-agent values)
    PRIORITY_USER = 1       # User-initiated requests (highest)
    PRIORITY_CRITICAL = 2   # Critical alerts
    PRIORITY_WARNING = 3    # Warning alerts
    PRIORITY_LOW = 4        # Background tasks (lowest)

    def severity_to_priority(severity: str) -> int:
        """Map alert severity to queue priority."""
        mapping = {
            "critical": PRIORITY_CRITICAL,
            "error": PRIORITY_CRITICAL,
            "warning": PRIORITY_WARNING,
            "info": PRIORITY_LOW,
        }
        return mapping.get(severity.lower(), PRIORITY_WARNING)

    async def queue_claude_task(prompt: str, priority: int = PRIORITY_LOW, context: dict = None,
                                allowed_tools: list = None, timeout: int = 600) -> str:
        """
        Queue a task for Claude Agent with priority.
        Returns task_id for polling status.
        """
        async with httpx.AsyncClient(timeout=30.0) as client:
            payload = {
                "prompt": prompt,
                "priority": priority,
                "context": context or {},
                "allowed_tools": allowed_tools or ["Read", "Glob", "Grep", "Bash", "WebSearch"],
                "working_directory": "/workspace",
                "timeout": timeout,
            }
            response = await client.post(
                CLAUDE_AGENT_URL + "/queue/submit",
                json=payload
            )
            response.raise_for_status()
            result = response.json()
            return result.get("task_id")

    async def poll_task_status(task_id: str, poll_interval: float = 2.0, max_wait: float = 600.0) -> dict:
        """
        Poll for task completion.
        Returns task result dict with status, result, error fields.
        """
        async with httpx.AsyncClient(timeout=30.0) as client:
            elapsed = 0.0
            while elapsed < max_wait:
                response = await client.get(f"{CLAUDE_AGENT_URL}/task/{task_id}")
                response.raise_for_status()
                task = response.json()
                if task.get("status") in ["completed", "failed"]:
                    return task
                await asyncio.sleep(poll_interval)
                elapsed += poll_interval
            return {"status": "timeout", "error": f"Task did not complete within {max_wait}s"}

    async def call_claude_agent(prompt: str, context: dict = None, allowed_tools: list = None,
                                priority: int = None, use_queue: bool = False, timeout: int = 300):
        """
        DEPRECATED: Cloud LLM escalation replaced with Brain Trust (Outline) escalation.

        This function now uses Ollama (qwen2.5:7b) for analysis and escalates
        complex issues to Brain Trust (humans via Outline wiki) instead of cloud LLMs.

        For backward compatibility, this still returns a response string.
        Callers should migrate to escalate_to_brain_trust() for new code.

        Args:
            prompt: The task prompt
            context: Optional context dict
            allowed_tools: Ignored (Ollama uses procedural tool calls via LangGraph)
            priority: Queue priority (ignored)
            use_queue: Ignored
            timeout: Timeout in seconds
        """
        logger.info("Processing with Ollama (qwen2.5:7b) - cloud LLMs disabled")

        # Use Ollama via LiteLLM for analysis
        analysis = await call_litellm([
            {"role": "system", "content": AGENT_SYSTEM_PROMPT},
            {"role": "user", "content": prompt}
        ])

        # Check if Ollama indicates it needs human help
        lower_analysis = analysis.lower() if analysis else ""
        needs_human = any(phrase in lower_analysis for phrase in [
            "i cannot", "i'm not sure", "need human", "escalate", "unclear",
            "insufficient information", "requires approval", "beyond my capability"
        ])

        if needs_human and context:
            # Extract alert info from context if available
            alert = context.get("alert", {"alertname": "Unknown", "description": prompt[:200]})
            await escalate_to_brain_trust(
                alert=alert,
                context=context,
                reason="Ollama analysis indicates human review needed",
                suggestions=[{"name": "Ollama Analysis", "description": analysis[:500]}]
            )
            return f"ESCALATED TO BRAIN TRUST: {analysis[:500]}..."

        return analysis

    def should_use_claude(assessment: dict) -> bool:
        """
        DEPRECATED: Determine if task should be routed to Claude.

        This function is kept for backward compatibility but always returns False.
        Complex tasks now escalate to Brain Trust (humans via Outline) instead of cloud LLMs.
        """
        return False  # Cloud LLMs disabled - use Ollama + Brain Trust escalation

    def build_assessment_prompt(alert, similar):
        return """Analyze this alert and provide assessment:
    Alert: """ + str(alert.get("alertname", "unknown")) + """
    Severity: """ + str(alert.get("severity", "warning")) + """
    Description: """ + str(alert.get("description", "")) + """
    Namespace: """ + str(alert.get("namespace", "default")) + """

    Similar runbooks found: """ + str(similar) + """

    Respond with JSON containing: domain, complexity, requires_approval, similar_runbook, similarity_score, recommended_topic"""

    def build_solutions_prompt(alert, assessment):
        return """Generate 2-3 solutions for this alert:
    Alert: """ + str(alert.get("alertname", "unknown")) + """
    Description: """ + str(alert.get("description", "")) + """
    Assessment: """ + str(assessment) + """

    Respond with JSON array containing objects with: name, description, impact, risk, commands"""

    async def assess_alert(state):
        """Assess alert using A2A Orchestrator for parallel specialist investigation.

        A2A handles all reasoning - LangGraph only orchestrates.
        Falls back to Brain Trust escalation if A2A unavailable.
        """
        alert = state["alert"]
        request_id = state.get("request_id", str(uuid_mod.uuid4()))
        alert_name = alert.get("alertname", alert.get("name", "unknown"))

        # Step 0: Check temporal patterns (recurring alerts)
        alert_fingerprint = alert.get("fingerprint", alert.get("id", ""))
        temporal_pattern = await check_temporal_patterns(alert_fingerprint, alert_name)

        # Step 1: Call A2A /v1/investigate for parallel specialist investigation
        logger.info(f"[A2A] Investigating alert: {alert_name}")
        investigation = await call_a2a_investigate(alert, request_id)

        # Check for A2A failure - escalate to Brain Trust
        if investigation.get("error") or investigation.get("fallback"):
            logger.warning(f"[A2A] Investigation failed, escalating to Brain Trust: {investigation.get('error')}")
            assessment = {
                "domain": "infrastructure",
                "complexity": "high",
                "requires_approval": True,
                "escalated": True,
                "match_type": "NO_MATCH",
                "escalation_reason": f"A2A investigation failed: {investigation.get('error', 'unknown')} - escalating to Brain Trust",
                "recommended_topic": "infrastructure",
                "confidence": 0.0,
                "a2a_error": investigation.get("error")
            }
            assessment["temporal_pattern"] = temporal_pattern
            return {
                "assessment": assessment,
                "runbook_match": {},
                "match_type": "NO_MATCH",
                "handled_locally": False,
                "temporal_pattern": temporal_pattern,
                "a2a_investigation": None
            }

        # Step 2: Process A2A investigation results
        verdict = investigation.get("verdict", "UNKNOWN")
        confidence = investigation.get("confidence", 0.5)
        synthesis = investigation.get("synthesis", "")
        findings = investigation.get("findings", [])

        logger.info(f"[A2A] Investigation complete: {verdict} ({confidence:.0%} confidence)")

        # Map A2A verdict to assessment
        if verdict == "ACTIONABLE":
            escalated = False
            complexity = "low" if confidence >= 0.8 else "medium"
        elif verdict == "FALSE_POSITIVE":
            escalated = False
            complexity = "low"
        else:  # UNKNOWN
            escalated = True
            complexity = "high"

        # Build assessment from A2A investigation
        assessment = {
            "domain": investigation.get("recommended_domain", "infrastructure"),
            "complexity": complexity,
            "requires_approval": True,  # Always require approval initially
            "confidence": confidence,
            "diagnosis_result": verdict.lower(),
            "escalated": escalated,
            "escalation_reason": investigation.get("escalation_reason") if escalated else None,
            "match_type": "A2A_INVESTIGATED",  # New match type for A2A
            "recommended_topic": investigation.get("recommended_domain", "infrastructure"),
            "a2a_verdict": verdict,
            "a2a_synthesis": synthesis,
            "a2a_findings": findings,
            "suggested_action": investigation.get("suggested_action")
        }

        # Add temporal pattern info to assessment
        assessment["temporal_pattern"] = temporal_pattern
        if temporal_pattern["is_recurring"]:
            assessment["escalation_priority"] = temporal_pattern["escalation_priority"]
            logger.warning(
                f"[ASSESS] Recurring alert: {alert_name} "
                f"({temporal_pattern['occurrence_count']} occurrences, priority: {temporal_pattern['escalation_priority']})"
            )

        # Handle escalation for inconclusive/conflicting findings
        if verdict in ["INCONCLUSIVE", "CONFLICTING"] or (verdict == "UNKNOWN" and confidence < 0.5):
            assessment["escalated"] = True
            assessment["escalation_reason"] = f"A2A investigation: {verdict} - needs human review"

        return {
            "assessment": assessment,
            "runbook_match": {},  # Runbook matching happens in plan_and_decide
            "match_type": "A2A_INVESTIGATED",
            "handled_locally": not assessment.get("escalated", False) and verdict == "ACTIONABLE",
            "temporal_pattern": temporal_pattern,
            "a2a_investigation": investigation
        }

    async def generate_solutions(state):
        """Generate solutions using A2A Orchestrator for planning and decision.

        A2A handles runbook matching, plan generation, and decision (EXECUTE/ESCALATE/WAIT).
        Falls back to Brain Trust escalation if A2A unavailable.
        """
        alert = state["alert"]
        assessment = state["assessment"]
        request_id = state.get("request_id", str(uuid_mod.uuid4()))
        a2a_investigation = state.get("a2a_investigation")

        # If already escalated in assess_alert, continue escalation
        if assessment.get("escalated"):
            logger.info(f"Assessment already escalated: {assessment.get('escalation_reason')}")
            escalation_result = await escalate_to_brain_trust(
                alert=alert,
                context={
                    "assessment": assessment,
                    "a2a_investigation": a2a_investigation,
                },
                reason=assessment.get("escalation_reason", "Assessment determined escalation needed"),
                suggestions=[{
                    "name": "A2A Investigation Summary",
                    "description": assessment.get("a2a_synthesis", "Investigation complete"),
                    "risk": "unknown"
                }] if assessment.get("a2a_synthesis") else []
            )
            return {
                "solutions": [{
                    "name": f"Brain Trust Review: {alert.get('alertname', 'Unknown')}",
                    "description": assessment.get("escalation_reason", "Escalated for human review"),
                    "impact": "pending",
                    "risk": "pending",
                    "source": "brain_trust_escalation",
                    "escalation_document": escalation_result.get("document_id"),
                    "requires_human_action": True
                }],
                "handled_locally": False,
                "escalated_to_brain_trust": True
            }

        # Call A2A /v1/plan_and_decide for runbook matching and plan generation
        logger.info(f"[A2A] Planning for alert: {alert.get('alertname', 'Unknown')}")
        plan_result = await call_a2a_plan_and_decide(alert, a2a_investigation or {}, request_id)

        # Check for A2A failure - escalate to Brain Trust
        if plan_result.get("error") or plan_result.get("fallback"):
            logger.warning(f"[A2A] Plan generation failed, escalating: {plan_result.get('error')}")
            escalation_result = await escalate_to_brain_trust(
                alert=alert,
                context={
                    "assessment": assessment,
                    "a2a_investigation": a2a_investigation,
                    "plan_error": plan_result.get("error")
                },
                reason=f"A2A planning failed: {plan_result.get('error', 'unknown')}",
                suggestions=[]
            )
            return {
                "solutions": [{
                    "name": f"Brain Trust Review: {alert.get('alertname', 'Unknown')}",
                    "description": f"A2A planning failed. Escalated for human review.",
                    "impact": "pending",
                    "risk": "pending",
                    "source": "brain_trust_escalation",
                    "escalation_document": escalation_result.get("document_id"),
                    "requires_human_action": True
                }],
                "handled_locally": False,
                "escalated_to_brain_trust": True
            }

        # Process A2A plan_and_decide result
        decision = plan_result.get("decision", "ESCALATE")
        match_type = plan_result.get("match_type", "NO_PLAN")
        plan_steps = plan_result.get("plan", [])
        confidence = plan_result.get("confidence", 0)

        logger.info(f"[A2A] Plan decision: {decision} (match: {match_type}, confidence: {confidence:.0%})")

        # Handle ESCALATE decision
        if decision == "ESCALATE" or match_type == "NO_PLAN":
            logger.info(f"[A2A] Escalating to Brain Trust: {plan_result.get('escalation_reason')}")
            escalation_result = await escalate_to_brain_trust(
                alert=alert,
                context={
                    "assessment": assessment,
                    "a2a_investigation": a2a_investigation,
                    "a2a_plan": plan_result
                },
                reason=plan_result.get("escalation_reason", plan_result.get("decision_rationale", "A2A decided escalation")),
                suggestions=[{
                    "name": "A2A Suggested Action",
                    "description": assessment.get("suggested_action", "Review required"),
                    "risk": plan_result.get("risk_level", "unknown")
                }] if assessment.get("suggested_action") else []
            )
            return {
                "solutions": [{
                    "name": f"Brain Trust Review: {alert.get('alertname', 'Unknown')}",
                    "description": plan_result.get("decision_rationale", "Escalated for human review"),
                    "impact": "pending",
                    "risk": "pending",
                    "source": "brain_trust_escalation",
                    "escalation_document": escalation_result.get("document_id"),
                    "requires_human_action": True
                }],
                "handled_locally": False,
                "escalated_to_brain_trust": True
            }

        # Handle WAIT decision
        if decision == "WAIT":
            logger.info("[A2A] Decision: WAIT - will re-evaluate later")
            return {
                "solutions": [{
                    "name": "Wait and Re-evaluate",
                    "description": plan_result.get("decision_rationale", "A2A suggests waiting before action"),
                    "impact": "low",
                    "risk": "low",
                    "source": "a2a_wait",
                    "wait_reason": plan_result.get("decision_rationale")
                }],
                "handled_locally": False,
                "wait_and_retry": True
            }

        # Handle EXECUTE decision - convert A2A plan to solutions
        runbook_name = plan_result.get("runbook_name")
        runbook_id = plan_result.get("runbook_id")

        solutions = [{
            "name": f"Execute Plan: {runbook_name or alert.get('alertname', 'A2A Plan')}",
            "description": f"A2A {match_type} plan (confidence: {confidence:.0%}). {plan_result.get('decision_rationale', '')}",
            "impact": "low" if confidence >= 0.8 else "medium",
            "risk": plan_result.get("risk_level", "medium"),
            "fix_steps": [
                {
                    "order": step.get("order", i + 1),
                    "action": step.get("action", ""),
                    "command": step.get("command"),
                    "rollback": step.get("rollback"),
                    "risk": step.get("risk", "low")
                }
                for i, step in enumerate(plan_steps)
            ],
            "source": "a2a_plan",
            "match_type": match_type,
            "runbook_id": runbook_id,
            "runbook_name": runbook_name,
            "a2a_confidence": confidence,
            "requires_approval": plan_result.get("requires_approval", True)
        }]

        # Flag SIMILAR/GENERATED matches for runbook updates
        if match_type in ["SIMILAR", "GENERATED"]:
            solutions[0]["propose_update"] = True
            solutions[0]["tweaks_applied"] = plan_result.get("tweaks_applied", [])
            if plan_result.get("runbook_score"):
                solutions[0]["similarity_score"] = plan_result["runbook_score"]

        return {
            "solutions": solutions,
            "handled_locally": True,
            "match_type": match_type,
            "a2a_plan": plan_result
        }

    def _parse_solutions_from_response(result: str, alert: dict) -> list:
        """Parse solutions from Claude's response."""
        try:
            # Look for JSON solutions block
            if "```json" in result:
                parts = result.split("```json")
                for part in parts[1:]:
                    json_str = part.split("```")[0]
                    try:
                        data = json.loads(json_str.strip())
                        if isinstance(data, list):
                            return data
                        elif isinstance(data, dict) and "fix_steps" in data:
                            # This is a runbook, convert to solution
                            return [{
                                "name": data.get("runbook_name", f"Fix: {alert.get('alertname')}"),
                                "description": data.get("runbook_description", "Apply fix from Claude"),
                                "impact": "medium",
                                "risk": "medium",
                                "commands": [s.get("command", "") for s in data.get("fix_steps", [])],
                                "source": "claude"
                            }]
                    except:
                        continue
        except:
            pass

        # Default solution if parsing fails
        return [{
            "name": f"Claude Analysis: {alert.get('alertname', 'Unknown')}",
            "description": result[:500] if result else "Review Claude's analysis",
            "impact": "medium",
            "risk": "medium",
            "commands": [],
            "source": "claude",
            "full_response": result
        }]

    async def request_approval(state):
        """Request approval via Matrix bot."""
        alert = state["alert"]
        solutions = state["solutions"]
        assessment = state["assessment"]

        # Store in Redis for persistence across restarts
        await store_pending_approval(alert["id"], {
            "alert": alert,
            "solutions": solutions,
            "assessment": assessment,
            "created_at": datetime.utcnow().isoformat()
        })

        # Determine room based on severity
        severity = alert.get("severity", "warning")
        room = "#critical" if severity == "critical" else "#infrastructure"

        # Format solutions for Matrix message
        solutions_text = ""
        for i, sol in enumerate(solutions, 1):
            solutions_text += f"\n{i}. **{sol.get('name', 'Solution')}**\n"
            solutions_text += f"   {sol.get('description', '')}\n"
            solutions_text += f"   Risk: {sol.get('risk', 'unknown')}\n"

        result = await call_matrix("approval", {
            "alert_id": alert["id"],
            "room": room,
            "alert": alert,
            "solutions": solutions,
            "message": (
                f"**Alert: {alert.get('alertname', 'Unknown')}**\n"
                f"Severity: {severity}\n"
                f"Namespace: {alert.get('namespace', 'default')}\n\n"
                f"{alert.get('description', 'No description')}\n\n"
                f"**Recommended Solutions:**\n"
                f"{solutions_text}\n\n"
                f"React  to approve solution 1, or reply with solution number.\n"
                f"React  to reject/ignore."
            ),
            "context": {
                "similar_runbook": assessment.get("similar_runbook"),
                "similarity": assessment.get("similarity_score", 0),
                "confidence": assessment.get("confidence", 0)
            }
        })

        # Also notify Afferent mobile PWA (non-blocking)
        try:
            await notify_afferent(alert, solutions, assessment)
        except Exception as e:
            logger.warning(f"Afferent notification failed (non-blocking): {e}")

        return {"approval_status": "pending", "thread_id": result.get("thread_id", "")}

    async def execute_solution(state):
        """Execute the selected solution using MCP tools via REST bridge.

        Supports three execution paths:
        1. Diagnostic runbook with executor (existing)
        2. Tool-based steps from A2A v2 schema (new preferred path)
        3. Legacy commands converted to tool calls (backwards compatible)
        """
        solution = state["selected_solution"]
        if not solution:
            return {"execution_result": {"success": False, "error": "No solution selected"}}

        # Case 1: Diagnostic runbook with executor (unchanged)
        if solution.get("source") == "diagnostic_runbook" and solution.get("executor"):
            executor = solution["executor"]
            logger.info(f"Executing diagnostic runbook fix via executor")

            fix_result = await executor.execute_approved_fix()

            runbook_id = solution.get("runbook_id")
            if runbook_id:
                await update_runbook_stats(runbook_id, success=fix_result.get("success", False))

            return {
                "execution_result": {
                    "success": fix_result.get("success", False),
                    "action": fix_result.get("action", "unknown"),
                    "fix_result": fix_result.get("fix_result", {}),
                    "validation": fix_result.get("validation", {}),
                    "rollback": fix_result.get("rollback"),
                    "summary": f"Runbook execution: {fix_result.get('action', 'unknown')}"
                }
            }

        # Case 2: Tool-based execution (from A2A plan or converted commands)
        steps = solution.get("plan", solution.get("fix_steps", []))
        results = []
        rollback_stack = []
        all_captured = {}

        for step in steps:
            # Determine tool and arguments
            tool_name = step.get("tool")
            arguments = step.get("arguments", {})

            # If no tool specified, try to convert legacy command
            if not tool_name and step.get("command"):
                tool_name, arguments = command_to_tool(step.get("command"))
                if not tool_name:
                    logger.warning(f"Cannot convert command to tool: {step.get('command', '')[:50]}...")
                    results.append({
                        "success": False,
                        "error": f"Unsupported command: {step.get('command', '')[:50]}",
                        "step": step.get("order", len(results) + 1),
                    })
                    continue

            # Validate tool is in catalog
            spec = TOOL_CATALOG.get(tool_name)
            if not spec:
                logger.warning(f"Tool not in catalog: {tool_name}")
                results.append({
                    "success": False,
                    "error": f"Unknown tool: {tool_name}",
                    "step": step.get("order", len(results) + 1),
                })
                continue

            # Check required arguments
            missing = [arg for arg in spec.get("required", []) if arg not in arguments]
            if missing:
                logger.warning(f"Missing arguments for {tool_name}: {missing}")
                results.append({
                    "success": False,
                    "error": f"Missing required arguments: {missing}",
                    "step": step.get("order", len(results) + 1),
                    "tool": tool_name,
                })
                continue

            # Capture pre-execution state for rollback (if configured)
            pre_capture = step.get("pre_capture", [])
            if pre_capture:
                captured = await capture_pre_state(pre_capture, arguments)
                all_captured.update(captured)
                logger.info(f"Pre-execution state captured: {list(captured.keys())}")

            # Get MCP URL for this tool
            mcp_domain = spec.get("mcp", "infrastructure")
            mcp_url = MCP_DOMAIN_URLS.get(mcp_domain, INFRASTRUCTURE_MCP_URL)

            # Execute the tool via REST bridge
            logger.info(f"Executing tool: {tool_name}({arguments}) via {mcp_domain}-mcp")
            result = await call_mcp_rest(mcp_url, tool_name, arguments)
            result["step"] = step.get("order", len(results) + 1)
            result["action"] = step.get("action", tool_name)
            results.append(result)

            # Track rollback info if step failed or for potential rollback
            if step.get("rollback_tool"):
                rollback_args = step.get("rollback_args", {})
                # Substitute captured values
                rollback_args = substitute_captured(rollback_args, all_captured)
                rollback_stack.append({
                    "tool": step["rollback_tool"],
                    "arguments": rollback_args,
                    "original_step": step.get("order", len(results)),
                })

            # Stop execution on failure (unless step allows continue)
            if not result.get("success") and step.get("on_failure", "abort") == "abort":
                logger.error(f"Step {step.get('order', len(results))} failed, aborting execution")
                break

        success = all(r.get("success", False) for r in results)

        # Update runbook stats if applicable
        runbook_id = solution.get("runbook_id")
        if runbook_id:
            await update_runbook_stats(runbook_id, success=success)

        return {
            "execution_result": {
                "success": success,
                "results": results,
                "rollback_stack": rollback_stack if not success else [],
                "captured_state": all_captured,
                "summary": f"Executed {len(results)} tool calls, {'all succeeded' if success else 'some failed'}"
            }
        }

    async def record_outcome(state):
        # 1. Record decision (existing behaviour - keep)
        await call_knowledge_mcp("record", "POST", {
            "collection": "decisions",
            "data": {
                "alert_id": state["alert"]["id"],
                "alert": state["alert"],
                "assessment": state["assessment"],
                "solution": state["selected_solution"],
                "result": state["execution_result"],
                "approval_status": state["approval_status"],
                "timestamp": datetime.utcnow().isoformat()
            }
        })

        # 2. Log execution event with rich context (Project 03 - Gap 1 fix)
        event_result = await call_mcp_tool(KNOWLEDGE_MCP_URL, "log_event", {
            "event_type": "runbook.execution",
            "description": f"Executed solution for {state['alert'].get('alertname', state['alert'].get('name', 'unknown alert'))}",
            "source_agent": "langgraph",
            "metadata": {
                "alert_id": state["alert"]["id"],
                "alert_name": state["alert"].get("alertname", state["alert"].get("name")),
                "runbook_id": state.get("runbook_id"),
                "service_name": state["alert"].get("labels", {}).get("service"),
                "namespace": state["alert"].get("namespace", state["alert"].get("labels", {}).get("namespace")),
                "alert_fingerprint": state["alert"].get("fingerprint", state["alert"].get("id")),
                "endpoint": state["alert"].get("labels", {}).get("endpoint"),
                "match_type": state["assessment"].get("match_type"),
                "confidence": state["assessment"].get("confidence"),
                "success": state["execution_result"].get("success", False),
            },
            "resolution": "completed" if state["execution_result"].get("success") else "failed",
        })

        # 3. Trigger validation with 5-min cooling period (Gemini R2)
        event_id = event_result.get("event_id")
        if event_id:
            initial_state = {
                "event_id": event_id,
                "triggered_by": "langgraph-auto",
                "event": {},
                "runbook": None,
                "reported_success": state["execution_result"].get("success", False),
                "ground_truth": {},
                "verdict": "pending",
                "confidence": 0.0,
                "actual_success": None,
                "signal_count": 0,
                "validation_recorded": False,
            }
            asyncio.create_task(_delayed_validation(initial_state, delay_seconds=300))

        # 4. Create GitHub issue for successful SIMILAR match improvements (Project 04 Phase 7)
        match_type = state["assessment"].get("match_type")
        execution_success = state["execution_result"].get("success", False)

        if match_type == "SIMILAR" and execution_success:
            runbook_match = state.get("runbook_match", {})
            tweaks_applied = state["assessment"].get("tweaks_applied", [])

            if tweaks_applied:
                asyncio.create_task(create_runbook_improvement_issue(
                    runbook_name=runbook_match.get("name", "Unknown"),
                    alert_name=state["alert"].get("alertname", state["alert"].get("name", "Unknown")),
                    tweaks_applied=tweaks_applied,
                    original_steps=runbook_match.get("steps", []),
                    execution_result=state["execution_result"]
                ))
                logger.info(f"[GITHUB] Triggered improvement issue for SIMILAR match: {runbook_match.get('name')}")

        return {}

    def should_request_approval(state):
        """Determine if human approval is required based on APPROVAL_POLICY.

        Policies:
        - VERBOSE: Always request approval (default, safest)
        - STANDARD: Auto-execute EXACT matches with high confidence + proven track record
        - AGGRESSIVE: Auto-execute EXACT and high-confidence SIMILAR matches
        """
        assessment = state.get("assessment", {})
        match_type = state.get("match_type", assessment.get("match_type", "NO_MATCH"))
        confidence = assessment.get("confidence", 0)
        runbook_match = state.get("runbook_match", {})

        # Always require approval for escalated alerts or NO_MATCH
        if assessment.get("escalated", False) or match_type == "NO_MATCH":
            logger.info(f"[APPROVAL] Required: escalated={assessment.get('escalated')}, match_type={match_type}")
            return "request_approval"

        # VERBOSE policy: always ask (except runbooks marked auto_approve_eligible with history)
        if APPROVAL_POLICY == "VERBOSE":
            if not assessment.get("requires_approval", True):
                # Runbook explicitly allows auto-approval (via metadata.auto_approve_eligible)
                logger.info(f"[APPROVAL] Skipped: VERBOSE but runbook allows auto-approve")
                return "execute_solution"
            return "request_approval"

        # STANDARD policy: auto-execute EXACT matches with high confidence
        if APPROVAL_POLICY == "STANDARD":
            if match_type == "EXACT" and confidence >= AUTO_EXECUTE_MIN_CONFIDENCE:
                logger.info(f"[APPROVAL] Skipped: STANDARD policy, EXACT match, confidence={confidence:.2f}")
                return "execute_solution"
            return "request_approval"

        # AGGRESSIVE policy: auto-execute EXACT and high-confidence SIMILAR
        if APPROVAL_POLICY == "AGGRESSIVE":
            if match_type == "EXACT":
                logger.info(f"[APPROVAL] Skipped: AGGRESSIVE policy, EXACT match")
                return "execute_solution"
            if match_type == "SIMILAR" and confidence >= AUTO_EXECUTE_MIN_CONFIDENCE:
                logger.info(f"[APPROVAL] Skipped: AGGRESSIVE policy, SIMILAR match, confidence={confidence:.2f}")
                return "execute_solution"
            return "request_approval"

        # Fallback: require approval
        if assessment.get("requires_approval", True):
            return "request_approval"
        return "execute_solution"

    # ============================================================================
    # GITHUB ISSUE CREATION - Project 04 Phase 7: Runbook Improvement Suggestions
    # ============================================================================

    async def create_runbook_improvement_issue(
        runbook_name: str,
        alert_name: str,
        tweaks_applied: list,
        original_steps: list,
        execution_result: dict
    ) -> Optional[str]:
        """Create a GitHub issue suggesting runbook improvements based on SIMILAR match execution.

        Called when a SIMILAR match was executed with tweaks that worked.
        Returns the issue URL if created successfully.
        """
        if not tweaks_applied:
            return None

        tweaks_summary = "\n".join([f"- {tweak}" for tweak in tweaks_applied])
        original_summary = "\n".join([f"{i+1}. {step}" for i, step in enumerate(original_steps[:5])])

        issue_body = (
            "## Runbook Improvement Suggestion\n\n"
            f"- Alert: `{alert_name}`\n"
            f"- Runbook: `{runbook_name}`\n"
            f"- Match Type: SIMILAR (executed with modifications)\n\n"
            "### Original Steps\n"
            f"{original_summary}\n\n"
            "### Tweaks Applied During Execution\n"
            f"{tweaks_summary}\n\n"
            "### Execution Result\n"
            f"- Success: {execution_result.get('success', 'unknown')}\n"
            f"- Confidence: {execution_result.get('confidence', 0):.2f}\n"
            f"- Output: {execution_result.get('output', 'N/A')[:500]}\n\n"
            "### Suggested Action\n"
            "Consider updating the runbook to incorporate these tweaks as the standard procedure.\n\n"
            "---\n"
            "_Generated by LangGraph Orchestrator (SIMILAR match auto-improvement)_"
        )

        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    EXTERNAL_MCP_URL + "/mcp",
                    json={
                        "jsonrpc": "2.0",
                        "id": 1,
                        "method": "tools/call",
                        "params": {
                            "name": "github_create_issue",
                            "arguments": {
                                "owner": GITHUB_OWNER,
                                "repo": GITHUB_REPO,
                                "title": f"[Runbook Update] {runbook_name}: tweaks from SIMILAR match",
                                "body": issue_body,
                                "labels": ["runbook-improvement", "auto-generated"]
                            }
                        }
                    }
                )
                if response.status_code == 200:
                    result = response.json()
                    content = result.get("result", {}).get("content", [])
                    if content:
                        issue_data = json.loads(content[0].get("text", "{}"))
                        issue_url = issue_data.get("html_url")
                        logger.info(f"[GITHUB] Created runbook improvement issue: {issue_url}")
                        return issue_url
        except Exception as e:
            logger.warning(f"[GITHUB] Failed to create improvement issue: {e}")
        return None

    # ============================================================================
    # TEMPORAL PATTERN DETECTION - Project 04 Phase 7: Recurring Alert Detection
    # ============================================================================

    async def check_temporal_patterns(alert_fingerprint: str, alert_name: str) -> dict:
        """Check for recurring alerts with the same fingerprint in the temporal window.

        Returns pattern info: is_recurring, occurrence_count, first_seen, escalation_priority
        """
        pattern_info = {
            "is_recurring": False,
            "occurrence_count": 1,
            "first_seen": datetime.utcnow().isoformat(),
            "escalation_priority": "normal"
        }

        try:
            # Query recent events with same alert fingerprint
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    KNOWLEDGE_MCP_URL + "/mcp",
                    json={
                        "jsonrpc": "2.0",
                        "id": 1,
                        "method": "tools/call",
                        "params": {
                            "name": "list_recent_events",
                            "arguments": {
                                "hours": TEMPORAL_WINDOW_HOURS,
                                "event_type": "alert",
                                "limit": 50
                            }
                        }
                    }
                )
                if response.status_code == 200:
                    result = response.json()
                    content = result.get("result", {}).get("content", [])
                    if content:
                        events = json.loads(content[0].get("text", "[]"))

                        # Count matching fingerprints
                        matching_events = [
                            e for e in events
                            if e.get("metadata", {}).get("alert_fingerprint") == alert_fingerprint
                            or e.get("metadata", {}).get("alert_name") == alert_name
                        ]

                        if len(matching_events) >= RECURRING_ALERT_THRESHOLD:
                            pattern_info["is_recurring"] = True
                            pattern_info["occurrence_count"] = len(matching_events) + 1  # +1 for current
                            pattern_info["escalation_priority"] = "high" if len(matching_events) >= 5 else "medium"

                            # Find first occurrence
                            if matching_events:
                                timestamps = [e.get("timestamp") for e in matching_events if e.get("timestamp")]
                                if timestamps:
                                    pattern_info["first_seen"] = min(timestamps)

                            logger.warning(
                                f"[TEMPORAL] Recurring alert detected: {alert_name} "
                                f"({pattern_info['occurrence_count']} times in {TEMPORAL_WINDOW_HOURS}h)"
                            )

        except Exception as e:
            logger.warning(f"[TEMPORAL] Failed to check patterns: {e}")

        return pattern_info

    # ============================================================================
    # VALIDATION SUBGRAPH - Project 03: Evaluator System
    # 3-node workflow with A2A reasoning:
    #   fetch_context  call_a2a_validation  record_validation
    # A2A handles: evidence gathering, verdict computation, incident documentation
    # LangGraph handles: orchestration, context fetching, result recording
    # ============================================================================

    # Consolidated MCP URLs for observability tools
    OBSERVABILITY_MCP_URL = os.environ.get("OBSERVABILITY_MCP_URL", "http://observability-mcp:8000")

    class ValidationState(TypedDict):
        event_id: str
        triggered_by: str  # langgraph-auto, human, pattern-detector
        event: dict
        runbook: Optional[dict]
        reported_success: bool
        # Legacy fields (kept for compatibility)
        ground_truth: dict
        verdict: str  # pending, confirmed, false_positive, false_negative, uncertain
        confidence: float
        actual_success: Optional[bool]
        signal_count: int
        validation_recorded: bool
        # A2A fields
        alert: dict  # Reconstructed alert for A2A
        investigation: dict  # Reconstructed or retrieved investigation
        plan: dict  # Reconstructed or retrieved plan
        execution_result: dict  # Execution result for validation
        a2a_response: dict  # Full A2A validate_and_document response
        incident_document: dict  # Generated incident documentation

    async def call_mcp_tool(mcp_url: str, tool_name: str, arguments: dict = None) -> dict:
        """Generic MCP tool caller using JSON-RPC protocol."""
        async with httpx.AsyncClient(timeout=30.0) as client:
            try:
                response = await client.post(
                    mcp_url + "/mcp",
                    json={
                        "jsonrpc": "2.0",
                        "id": 1,
                        "method": "tools/call",
                        "params": {"name": tool_name, "arguments": arguments or {}}
                    }
                )
                if response.status_code == 200:
                    result = response.json()
                    content = result.get("result", {}).get("content", [])
                    if content:
                        return json.loads(content[0].get("text", "{}"))
                return {}
            except Exception as e:
                logger.warning(f"MCP tool call failed ({tool_name}): {e}")
                return {}

    async def fetch_validation_context(state: ValidationState) -> dict:
        """Node 1: Fetch event and runbook from knowledge-mcp, reconstruct A2A context."""
        event_id = state["event_id"]
        logger.info(f"[VALIDATE] Fetching context for event {event_id}")

        # Get event details
        event = await call_mcp_tool(KNOWLEDGE_MCP_URL, "get_event", {"event_id": event_id})
        if not event or event.get("error"):
            logger.error(f"[VALIDATE] Event {event_id} not found: {event}")
            return {
                "event": {},
                "runbook": None,
                "reported_success": False,
                "verdict": "uncertain",
                "confidence": 0.0,
                "alert": {},
                "investigation": {},
                "plan": {},
                "execution_result": {},
            }

        metadata = event.get("metadata", {})

        # Get runbook if referenced
        runbook = None
        runbook_id = metadata.get("runbook_id")
        if runbook_id:
            runbook = await call_mcp_tool(KNOWLEDGE_MCP_URL, "get_runbook", {"runbook_id": runbook_id})

        reported_success = metadata.get("success", event.get("resolution") == "completed")

        # Reconstruct alert from event metadata for A2A
        alert = {
            "name": metadata.get("alert_name", metadata.get("alertname", event.get("event_type", "unknown"))),
            "labels": {
                "namespace": metadata.get("namespace"),
                "pod": metadata.get("pod"),
                "service": metadata.get("service_name", metadata.get("service")),
                "node": metadata.get("node"),
            },
            "severity": metadata.get("severity", "warning"),
            "description": event.get("description", ""),
            "fingerprint": metadata.get("alert_fingerprint", metadata.get("alert_id", event_id)),
        }

        # Reconstruct investigation context from event metadata
        investigation = {
            "request_id": event_id,
            "grade": metadata.get("investigation_grade", "PARTIAL"),
            "confidence": metadata.get("investigation_confidence", 0.5),
            "findings": metadata.get("findings", []),
            "synthesis": metadata.get("synthesis", ""),
            "recommended_domain": metadata.get("domain", "general"),
        }

        # Reconstruct plan context from event metadata
        plan = {
            "request_id": event_id,
            "match_type": metadata.get("match_type", "GENERATED"),
            "runbook_id": runbook_id,
            "runbook_name": runbook.get("name") if runbook else None,
            "plan": metadata.get("plan_steps", []),
            "decision": metadata.get("decision", "EXECUTE"),
            "confidence": metadata.get("plan_confidence", 0.5),
        }

        # Get execution result from event
        execution_result = {
            "success": reported_success,
            "steps_executed": metadata.get("steps_executed", []),
            "output": metadata.get("execution_output", ""),
            "errors": metadata.get("execution_errors", []),
        }

        return {
            "event": event,
            "runbook": runbook,
            "reported_success": bool(reported_success),
            "alert": alert,
            "investigation": investigation,
            "plan": plan,
            "execution_result": execution_result,
        }

    async def call_a2a_validation(state: ValidationState) -> dict:
        """Node 2: Call A2A /v1/validate_and_document for reasoning.

        A2A handles all validation reasoning:
        - Gathering ground truth signals from observability tools
        - Computing verdict based on evidence
        - Generating incident documentation
        """
        event_id = state["event_id"]
        alert = state.get("alert", {})
        investigation = state.get("investigation", {})
        plan = state.get("plan", {})
        execution_result = state.get("execution_result", {})

        if not alert:
            logger.warning(f"[VALIDATE] No alert context for event {event_id}")
            return {
                "a2a_response": {},
                "verdict": "uncertain",
                "confidence": 0.3,
                "ground_truth": {},
                "signal_count": 0,
                "incident_document": {},
            }

        # Call A2A for validation reasoning
        logger.info(f"[VALIDATE] Calling A2A validate_and_document for event {event_id}")
        a2a_response = await call_a2a_validate_and_document(
            alert=alert,
            investigation=investigation,
            plan=plan,
            execution_result=execution_result,
            request_id=event_id
        )

        if a2a_response.get("fallback") or a2a_response.get("error"):
            # A2A unavailable - escalate to Brain Trust
            logger.warning(f"[VALIDATE] A2A failed for {event_id}: {a2a_response.get('error')}, escalating to Brain Trust")
            await escalate_to_brain_trust(
                alert=alert,
                context={
                    "event_id": event_id,
                    "reason": "validation_a2a_failed",
                    "error": a2a_response.get("error", "A2A unavailable"),
                    "execution_result": execution_result,
                },
                domain="validation"
            )
            return {
                "a2a_response": a2a_response,
                "verdict": "uncertain",
                "confidence": 0.0,
                "ground_truth": {},
                "signal_count": 0,
                "incident_document": {},
            }

        # Map A2A verdict to validation verdict
        a2a_verdict = a2a_response.get("verdict", "PARTIAL")
        verdict_map = {
            "RESOLVED": "confirmed",
            "PARTIAL": "uncertain",
            "STILL_FAILING": "false_positive",
            "FALSE_POSITIVE": "false_negative",  # Alert was false, execution was right to ignore
        }
        verdict = verdict_map.get(a2a_verdict, "uncertain")

        # Extract validation evidence as ground_truth for compatibility
        validation_evidence = a2a_response.get("validation_evidence", [])
        ground_truth = {
            "a2a_evidence": validation_evidence,
            "a2a_verdict": a2a_verdict,
        }

        # Get incident document
        incident_document = a2a_response.get("document", {})

        logger.info(f"[VALIDATE] A2A verdict: {a2a_verdict} -> {verdict}, confidence: {a2a_response.get('confidence', 0.5)}")
        return {
            "a2a_response": a2a_response,
            "verdict": verdict,
            "confidence": a2a_response.get("confidence", 0.5),
            "actual_success": a2a_verdict == "RESOLVED",
            "ground_truth": ground_truth,
            "signal_count": len(validation_evidence),
            "incident_document": incident_document,
        }

    # NOTE: compute_verdict removed - reasoning now handled by A2A /v1/validate_and_document

    async def record_validation(state: ValidationState) -> dict:
        """Node 3: Record validation result and incident documentation back to knowledge-mcp."""
        event_id = state["event_id"]
        verdict = state.get("verdict", "uncertain")
        confidence = state.get("confidence", 0.0)
        a2a_response = state.get("a2a_response", {})
        incident_document = state.get("incident_document", {})
        event = state.get("event", {})
        runbook_id = event.get("metadata", {}).get("runbook_id")

        validation_payload = {
            "validated": True,
            "validated_at": datetime.utcnow().isoformat(),
            "validated_by": state.get("triggered_by", "langgraph-auto"),
            "verdict": verdict,
            "confidence": confidence,
            "actual_success": state.get("actual_success"),
            "signal_count": state.get("signal_count", 0),
            "ground_truth": state.get("ground_truth", {}),
            # A2A-specific validation data
            "a2a_verdict": a2a_response.get("verdict"),
            "validation_evidence": a2a_response.get("validation_evidence", []),
            "runbook_action": a2a_response.get("runbook_action"),
        }

        # Update event with validation
        result = await call_mcp_tool(KNOWLEDGE_MCP_URL, "update_event", {
            "event_id": event_id,
            "validation": validation_payload,
            "resolution": "validated" if verdict == "confirmed" else "needs_review",
        })

        # If false positive, block runbook from autonomous execution (Gemini R3)
        if verdict == "false_positive":
            event = state.get("event", {})
            runbook_id = event.get("metadata", {}).get("runbook_id")
            if runbook_id:
                logger.warning(f"[VALIDATE] FALSE POSITIVE detected for runbook {runbook_id}, blocking autonomous execution")
                # Log autonomy.blocked event
                await call_mcp_tool(KNOWLEDGE_MCP_URL, "log_event", {
                    "event_type": "autonomy.blocked",
                    "description": f"Runbook {runbook_id} blocked from autonomous execution due to false positive in event {event_id}",
                    "source_agent": "validation-workflow",
                    "metadata": {
                        "runbook_id": runbook_id,
                        "event_id": event_id,
                        "verdict": verdict,
                        "confidence": confidence,
                    },
                })
                # Downgrade runbook to manual
                await call_mcp_tool(KNOWLEDGE_MCP_URL, "update_runbook", {
                    "runbook_id": runbook_id,
                    "automation_level": "manual",
                })

        # Log validation completed event
        await call_mcp_tool(KNOWLEDGE_MCP_URL, "log_event", {
            "event_type": "validation.completed",
            "description": f"Validation of event {event_id}: {verdict} (confidence {confidence:.2f})",
            "source_agent": "validation-workflow",
            "metadata": {
                "event_id": event_id,
                "verdict": verdict,
                "confidence": confidence,
                "triggered_by": state.get("triggered_by", "langgraph-auto"),
            },
        })

        # Create incident documentation in Outline if A2A generated it
        incident_document = state.get("incident_document", {})
        if incident_document and incident_document.get("title"):
            try:
                # Create incident document in Outline
                timeline_items = chr(10).join('- ' + t for t in incident_document.get('timeline', []))
                lessons_items = chr(10).join('- ' + l for l in incident_document.get('lessons_learned', []))
                doc_content = (
                    f"# {incident_document.get('title', 'Incident Report')}\n\n"
                    f"## Summary\n{incident_document.get('summary', 'No summary available.')}\n\n"
                    f"## Timeline\n{timeline_items}\n\n"
                    f"## Root Cause\n{incident_document.get('root_cause', 'Under investigation.')}\n\n"
                    f"## Resolution\n{incident_document.get('resolution', 'Pending resolution.')}\n\n"
                    f"## Lessons Learned\n{lessons_items}\n\n"
                    f"---\n*Generated by A2A Validation Workflow*\n"
                    f"*Event ID - {event_id}*\n"
                    f"*Validated at - {datetime.utcnow().isoformat()}*\n"
                )
                # Store in Outline incidents collection
                await call_mcp_tool(KNOWLEDGE_MCP_URL, "create_document", {
                    "collection_id": "incidents",  # Outline collection for incidents
                    "title": incident_document.get("title"),
                    "content": doc_content,
                    "publish": True,
                })
                logger.info(f"[VALIDATE] Created incident document: {incident_document.get('title')}")
            except Exception as e:
                logger.warning(f"[VALIDATE] Failed to create incident document: {e}")

        # Handle runbook action recommendation from A2A
        runbook_action = a2a_response.get("runbook_action")
        if runbook_action and runbook_id:
            if runbook_action == "CREATE":
                # Propose new runbook from incident
                logger.info(f"[VALIDATE] A2A recommends creating new runbook from incident {event_id}")
                await call_mcp_tool(KNOWLEDGE_MCP_URL, "log_event", {
                    "event_type": "runbook.proposal",
                    "description": f"New runbook proposed based on incident {event_id}",
                    "source_agent": "validation-workflow",
                    "metadata": {
                        "event_id": event_id,
                        "proposed_runbook": incident_document.get("runbook_proposal"),
                    },
                })
            elif runbook_action == "UPDATE":
                # Recommend runbook update
                logger.info(f"[VALIDATE] A2A recommends updating runbook {runbook_id}")

        logger.info(f"[VALIDATE] Recorded validation for {event_id}: {verdict}")
        return {"validation_recorded": True}

    def create_validation_workflow():
        """Create the 3-node validation SubGraph with A2A reasoning.

        Flow: fetch_context -> call_a2a_validation -> record_validation
        A2A handles all reasoning (gather evidence, compute verdict, generate docs).
        LangGraph handles orchestration (fetch context, record results).
        """
        workflow = StateGraph(ValidationState)
        workflow.add_node("fetch_context", fetch_validation_context)
        workflow.add_node("call_a2a_validation", call_a2a_validation)  # Replaces gather_truth + compute_verdict
        workflow.add_node("record_validation", record_validation)
        workflow.set_entry_point("fetch_context")
        workflow.add_edge("fetch_context", "call_a2a_validation")
        workflow.add_edge("call_a2a_validation", "record_validation")
        workflow.add_edge("record_validation", END)
        return workflow.compile()

    validation_graph = create_validation_workflow()

    async def _delayed_validation(initial_state: dict, delay_seconds: int = 300):
        """Run validation after a cooling period (Gemini R2)."""
        logger.info(f"[VALIDATE] Scheduled validation for event {initial_state['event_id']} in {delay_seconds}s")
        await asyncio.sleep(delay_seconds)
        try:
            await validation_graph.ainvoke(initial_state)
            logger.info(f"[VALIDATE] Delayed validation completed for {initial_state['event_id']}")
        except Exception as e:
            logger.error(f"[VALIDATE] Delayed validation failed: {e}")

    # =========================================================================
    # Project 05: Skill Router Node
    # Routes requests to appropriate skills before assessment
    # =========================================================================

    async def skill_router(state: AgentState) -> dict:
        """
        Route request to appropriate skill based on intent.

        Skill routing flow:
        1. Check for explicit slash command (e.g., /troubleshoot)
        2. If no command, use semantic search against skills collection
        3. Handle collisions using relative threshold + command priority
        4. Load skill context (MCPs, system_prompt, runbook_patterns)

        Returns updated state with skill_id, skill_context, and loaded_mcps.
        """
        if not SKILL_ROUTER_ENABLED:
            logger.debug("[SKILL_ROUTER] Disabled, passing through")
            return {"skill_id": "", "skill_context": {}, "loaded_mcps": []}

        alert = state.get("alert", {})
        description = alert.get("description", "")
        alertname = alert.get("alertname", "")
        query = f"{alertname}: {description}"

        logger.info(f"[SKILL_ROUTER] Routing: {query[:100]}...")

        try:
            # Step 1: Check for explicit slash command in description
            command_match = re.match(r"^/(\w+)", description.strip())
            if command_match:
                command = f"/{command_match.group(1)}"
                logger.info(f"[SKILL_ROUTER] Explicit command detected: {command}")

                # Get skill by command
                skill = await call_mcp_tool(KNOWLEDGE_MCP_URL, "get_skill_by_command", {
                    "command": command
                })

                if skill and not skill.get("error"):
                    logger.info(f"[SKILL_ROUTER] Matched skill: {skill.get('id')} via command")
                    return {
                        "skill_id": skill.get("id", ""),
                        "skill_context": skill,
                        "loaded_mcps": skill.get("mcps_primary", [])[:MAX_MCPS_PER_REQUEST]
                    }

            # Step 2: Semantic search for matching skills
            matches = await call_mcp_tool(KNOWLEDGE_MCP_URL, "search_skills", {
                "query": query,
                "limit": 3,
                "min_score": 0.5
            })

            if not matches or (isinstance(matches, list) and matches and matches[0].get("error")):
                logger.warning("[SKILL_ROUTER] No skills found, using default")
                return {"skill_id": "", "skill_context": {}, "loaded_mcps": ["knowledge-mcp"]}

            # Step 3: Handle collisions using relative threshold
            if len(matches) > 1:
                top_score = matches[0].get("score", 0)
                relative_threshold = top_score * SKILL_COLLISION_THRESHOLD

                # Check if second match is within relative threshold
                second_score = matches[1].get("score", 0)
                if top_score - second_score <= relative_threshold:
                    # Collision detected - check if either is from a command match
                    top_commands = matches[0].get("commands", [])
                    for cmd in top_commands:
                        if cmd in query.lower():
                            logger.info(f"[SKILL_ROUTER] Collision resolved via command match: {matches[0]['id']}")
                            break
                    else:
                        # True collision - log and use top match
                        logger.warning(
                            f"[SKILL_ROUTER] Skill collision: {matches[0]['id']} ({top_score:.2f}) vs "
                            f"{matches[1]['id']} ({second_score:.2f}), using top match"
                        )

            # Use top match
            top_skill = matches[0]
            skill_id = top_skill.get("id", "")

            # Get full skill definition
            full_skill = await call_mcp_tool(KNOWLEDGE_MCP_URL, "get_skill", {
                "skill_id": skill_id
            })

            if full_skill and not full_skill.get("error"):
                logger.info(f"[SKILL_ROUTER] Loaded skill: {skill_id} (score: {top_skill.get('score', 0):.2f})")
                return {
                    "skill_id": skill_id,
                    "skill_context": full_skill,
                    "loaded_mcps": full_skill.get("mcps_primary", [])[:MAX_MCPS_PER_REQUEST]
                }

            # Fallback
            logger.warning(f"[SKILL_ROUTER] Could not load skill {skill_id}, using default")
            return {"skill_id": "", "skill_context": {}, "loaded_mcps": ["knowledge-mcp"]}

        except Exception as e:
            logger.error(f"[SKILL_ROUTER] Error: {e}")
            return {"skill_id": "", "skill_context": {}, "loaded_mcps": ["knowledge-mcp"]}

    def create_workflow():
        workflow = StateGraph(AgentState)

        # Add skill_router if enabled (Project 05)
        if SKILL_ROUTER_ENABLED:
            workflow.add_node("skill_router", skill_router)

        workflow.add_node("assess_alert", assess_alert)
        workflow.add_node("generate_solutions", generate_solutions)
        workflow.add_node("request_approval", request_approval)
        workflow.add_node("execute_solution", execute_solution)
        workflow.add_node("record_outcome", record_outcome)

        # Set entry point based on skill_router status
        if SKILL_ROUTER_ENABLED:
            workflow.set_entry_point("skill_router")
            workflow.add_edge("skill_router", "assess_alert")
        else:
            workflow.set_entry_point("assess_alert")

        workflow.add_edge("assess_alert", "generate_solutions")
        workflow.add_conditional_edges(
            "generate_solutions",
            should_request_approval,
            {"request_approval": "request_approval", "execute_solution": "execute_solution"}
        )
        workflow.add_edge("request_approval", "record_outcome")
        workflow.add_edge("execute_solution", "record_outcome")
        workflow.add_edge("record_outcome", END)
        return workflow.compile()

    graph = create_workflow()

    async def _process_alert_background(alert_dict: dict, initial_state: dict):
        """Background task to process alert through the graph."""
        try:
            logger.info("Processing alert %s in background", alert_dict["id"])
            result = await graph.ainvoke(initial_state)
            logger.info("Alert %s processed: status=%s", alert_dict["id"], result.get("approval_status", "unknown"))
        except Exception as e:
            logger.error("Background graph execution failed for %s: %s", alert_dict["id"], e)

    @app.post("/alert", status_code=202)
    async def process_alert(alert: AlertInput):
        """Receive alert and process asynchronously. Returns immediately with 202 Accepted."""
        alert_dict = alert.model_dump()
        alert_dict["id"] = alert_dict.get("id") or "alert-" + datetime.utcnow().strftime("%Y%m%d%H%M%S")
        initial_state = {
            "messages": [],
            "alert": alert_dict,
            "assessment": {},
            "solutions": [],
            "selected_solution": {},
            "approval_status": "pending",
            "execution_result": {},
            "runbook_id": "",
            "topic_id": 0,
            "thread_id": alert_dict["id"],
            "runbook_match": {},
            "handled_locally": False,
            # Project 05: Skill Router fields
            "skill_id": "",
            "skill_context": {},
            "loaded_mcps": [],
            "execution_checkpoint": {}
        }
        # Spawn background task - returns immediately
        asyncio.create_task(_process_alert_background(alert_dict, initial_state))
        logger.info("Alert %s accepted for async processing", alert_dict["id"])
        return {
            "alert_id": alert_dict["id"],
            "status": "accepted",
            "message": "Alert queued for processing"
        }

    @app.post("/keep-alert-debug", status_code=200)
    async def keep_alert_debug(request: Request):
        """Debug endpoint to see what Keep sends."""
        body = await request.body()
        query_params = dict(request.query_params)
        content_type = request.headers.get('content-type', 'none')
        logger.info("Keep debug - content-type: %s", content_type)
        logger.info("Keep debug - query params: %s", query_params)
        logger.info("Keep debug - raw body: %s", body.decode('utf-8', errors='replace')[:1000])
        return {"content_type": content_type, "query_params": query_params, "body": body.decode('utf-8', errors='replace')[:500]}

    @app.post("/keep-alert", status_code=202)
    async def process_keep_alert(request: Request):
        """Receive alert from Keep and process. Transforms Keep format to internal format."""
        # Debug: Log raw request body
        body = await request.body()
        logger.info("Keep alert raw body: %s", body.decode('utf-8', errors='replace')[:2000])

        # Parse as KeepAlert
        alert = KeepAlert.parse_raw(body)
        logger.info("Parsed KeepAlert: name=%s, alertname=%s, fingerprint=%s, labels=%s",
                    alert.name, alert.alertname, alert.fingerprint, alert.labels)

        # Transform Keep alert to internal format (preserve rich context)
        alert_id = alert.fingerprint or alert.id or f"keep-{datetime.utcnow().strftime('%Y%m%d%H%M%S')}"
        namespace = alert.namespace or "default"
        if not namespace or namespace == "default":
            if alert.labels:
                namespace = alert.labels.get("namespace", "default")

        alert_dict = {
            "id": alert_id,
            "alertname": alert.name or alert.alertname or "unknown",
            "severity": alert.severity or "warning",
            "namespace": namespace,
            "description": alert.description or alert.message or "",
            "labels": alert.labels or {},
            "annotations": alert.annotations or {},
            "source": alert.source,
            "keep_status": alert.status,
            # Rich context from Keep
            "url": alert.url,  # Coroot incident link
            "generatorURL": alert.generatorURL,  # Prometheus query link
            "cluster": alert.cluster,
            "pod": alert.pod or (alert.labels or {}).get("pod"),
            "deployment": alert.deployment or (alert.labels or {}).get("deployment"),
            "daemonset": alert.daemonset or (alert.labels or {}).get("daemonset"),
            "service": alert.service or (alert.labels or {}).get("service"),
            "container": alert.container or (alert.labels or {}).get("container"),
            "firingStartTime": alert.firingStartTime,
            "firingCounter": alert.firingCounter,
            "lastReceived": alert.lastReceived,
            "startedAt": alert.startedAt,
            "firstTimestamp": alert.firstTimestamp,
            "alert_hash": alert.alert_hash,
            "payload": alert.payload,
        }

        initial_state = {
            "messages": [],
            "alert": alert_dict,
            "assessment": {},
            "solutions": [],
            "selected_solution": {},
            "approval_status": "pending",
            "execution_result": {},
            "runbook_id": "",
            "topic_id": 0,
            "thread_id": alert_dict["id"],
            "runbook_match": {},
            "handled_locally": False,
            # Project 05: Skill Router fields
            "skill_id": "",
            "skill_context": {},
            "loaded_mcps": [],
            "execution_checkpoint": {}
        }
        # Spawn background task - returns immediately
        asyncio.create_task(_process_alert_background(alert_dict, initial_state))
        logger.info("Keep alert %s (%s) accepted for async processing", alert_dict["alertname"], alert_dict["id"])
        return {
            "alert_id": alert_dict["id"],
            "status": "accepted",
            "message": "Keep alert queued for processing"
        }

    @app.post("/approve")
    async def approve_action(request: ApprovalRequest):
        # Get from Redis-backed storage
        pending = await get_pending_approval(request.alert_id)
        if not pending:
            raise HTTPException(status_code=404, detail="Approval not found or expired")
        solutions = pending["solutions"]
        if request.solution_index < 1 or request.solution_index > len(solutions):
            raise HTTPException(status_code=400, detail="Invalid solution index")
        selected = solutions[request.solution_index - 1]

        # Build complete state for graph execution
        # Note: This still calls functions directly (graph bypass) but with proper state
        # Future: Use LangGraph thread resumption for true graph integration
        state = {
            "messages": [],
            "alert": pending["alert"],
            "assessment": pending["assessment"],
            "solutions": solutions,
            "selected_solution": selected,
            "approval_status": "approved",
            "execution_result": {},
            "runbook_id": selected.get("runbook_id", ""),
            "handled_locally": True,
            "topic_id": 0,
            "thread_id": request.alert_id,
            "skill_id": "",
            "skill_context": {},
            "loaded_mcps": [],
            "execution_checkpoint": {}
        }
        result = await execute_solution(state)
        state.update(result)
        await record_outcome(state)
        # Remove from Redis-backed storage
        await delete_pending_approval(request.alert_id)
        return {
            "success": result["execution_result"].get("success", False),
            "summary": result["execution_result"].get("summary", ""),
            "runbook_id": selected.get("runbook_id", f"runbook-{request.alert_id[:8]}")
        }

    @app.post("/ignore")
    async def ignore_action(request: IgnoreRequest):
        pending = await get_pending_approval(request.alert_id)
        if pending:
            await call_knowledge_mcp("record", "POST", {
                "collection": "decisions",
                "data": {
                    "alert_id": request.alert_id,
                    "alert": pending["alert"],
                    "action": "ignored",
                    "ignored_by": request.ignored_by,
                    "timestamp": datetime.utcnow().isoformat()
                }
            })
            await delete_pending_approval(request.alert_id)
        return {"status": "ignored"}

    @app.get("/pending/{alert_id}")
    async def get_pending(alert_id: str):
        pending = await get_pending_approval(alert_id)
        return pending

    @app.get("/status")
    async def get_status():
        pending_count = await get_pending_count()
        return {"pending_count": pending_count, "auto_executed": 0, "active_runbooks": 0, "learning_queue": 0}

    # ============================================================================
    # VALIDATION ENDPOINTS - Project 03: Evaluator System
    # ============================================================================

    class ValidateRequest(BaseModel):
        event_id: str
        triggered_by: str = "human"

    class ValidateCandidatesRequest(BaseModel):
        max_candidates: int = 10

    @app.post("/validate", status_code=202)
    async def validate_event(request: ValidateRequest):
        """Trigger validation workflow for a specific event.

        For human-triggered: runs immediately (no cooling period).
        For langgraph-auto: uses 5-minute cooling period.
        """
        initial_state = {
            "event_id": request.event_id,
            "triggered_by": request.triggered_by,
            "event": {},
            "runbook": None,
            "reported_success": False,
            "ground_truth": {},
            "verdict": "pending",
            "confidence": 0.0,
            "actual_success": None,
            "signal_count": 0,
            "validation_recorded": False,
        }

        if request.triggered_by == "langgraph-auto":
            # Delayed validation with 5-minute cooling period (Gemini R2)
            asyncio.create_task(_delayed_validation(initial_state, delay_seconds=300))
            return {"status": "scheduled", "event_id": request.event_id, "delay_seconds": 300}
        else:
            # Immediate validation for human-triggered
            asyncio.create_task(validation_graph.ainvoke(initial_state))
            return {"status": "processing", "event_id": request.event_id}

    @app.post("/validate-candidates", status_code=202)
    async def validate_candidates(request: ValidateCandidatesRequest):
        """Trigger validation for autonomy upgrade candidates.

        Called by pattern-detector to validate candidates before notification.
        """
        # Get autonomy candidates from knowledge-mcp
        candidates = await call_mcp_tool(KNOWLEDGE_MCP_URL, "list_autonomy_candidates", {
            "min_executions": 5,
            "min_success_rate": 0.7,
        })

        if not candidates or isinstance(candidates, dict) and candidates.get("error"):
            return {"status": "no_candidates", "candidates": 0, "validations_queued": 0}

        # Limit candidates
        candidates = candidates[:request.max_candidates] if isinstance(candidates, list) else []

        queued = 0
        for candidate in candidates:
            # Get recent execution events for this runbook
            runbook_id = candidate.get("id")
            if not runbook_id:
                continue

            # Find recent execution events for this runbook
            recent_events = await call_mcp_tool(KNOWLEDGE_MCP_URL, "list_recent_events", {
                "event_type": "runbook.execution",
                "limit": 5,
            })

            if isinstance(recent_events, list):
                for event in recent_events:
                    event_meta = event.get("metadata", {})
                    if event_meta.get("runbook_id") == runbook_id:
                        # Check if already validated
                        if not event.get("validation"):
                            event_id = event.get("id") or event.get("event_id")
                            if event_id:
                                initial_state = {
                                    "event_id": event_id,
                                    "triggered_by": "pattern-detector",
                                    "event": {},
                                    "runbook": None,
                                    "reported_success": event_meta.get("success", False),
                                    "ground_truth": {},
                                    "verdict": "pending",
                                    "confidence": 0.0,
                                    "actual_success": None,
                                    "signal_count": 0,
                                    "validation_recorded": False,
                                }
                                asyncio.create_task(validation_graph.ainvoke(initial_state))
                                queued += 1

        logger.info(f"[VALIDATE] Queued {queued} validations for {len(candidates)} candidates")
        return {
            "status": "processing",
            "candidates": len(candidates),
            "validations_queued": queued,
        }

    class QueryRequest(BaseModel):
        prompt: str
        use_claude: bool = False
        context: Optional[dict] = None
        messages: Optional[List[dict]] = None  # Conversation history [{"role": "user/assistant", "content": "..."}]
        conversation_id: Optional[str] = None  # For tracking conversations

    async def build_query_context(prompt: str) -> dict:
        """Build context from MCPs based on query keywords."""
        context = {}
        errors = []
        prompt_lower = prompt.lower()

        async with httpx.AsyncClient(timeout=15.0, verify=False) as client:
            # UniFi context for network queries
            if any(kw in prompt_lower for kw in ["unifi", "network", "device", "update", "firmware", "ap", "switch", "gateway", "wifi"]):
                try:
                    # Use REST API endpoints (not MCP protocol)
                    resp = await client.get("http://unifi-mcp:8000/api/devices")
                    if resp.status_code == 200:
                        data = resp.json()
                        if data.get("status") == "ok":
                            context["unifi_devices"] = data.get("data", [])
                            logger.info(f"Got {len(context.get('unifi_devices', []))} UniFi devices")
                        else:
                            errors.append(f"UniFi API error: {data.get('error', 'unknown')}")
                    resp = await client.get("http://unifi-mcp:8000/api/health")
                    if resp.status_code == 200:
                        data = resp.json()
                        if data.get("status") == "ok":
                            context["unifi_health"] = data.get("data", {})
                        else:
                            errors.append(f"UniFi health error: {data.get('error', 'unknown')}")
                except Exception as e:
                    logger.warning(f"Failed to get UniFi context: {e}")
                    errors.append(f"UniFi connection failed: {str(e)}")

            # Infrastructure context for cluster queries
            if any(kw in prompt_lower for kw in ["pod", "kubernetes", "k8s", "deployment", "cluster", "container", "issue", "problem"]):
                try:
                    resp = await client.get(INFRASTRUCTURE_MCP_URL + "/api/cluster")
                    if resp.status_code == 200:
                        data = resp.json()
                        if data.get("status") == "ok":
                            context["cluster_state"] = data.get("data", {})
                        else:
                            errors.append(f"Infrastructure API error: {data.get('error', 'unknown')}")
                    else:
                        errors.append(f"Infrastructure MCP returned {resp.status_code} - cluster data not available")
                except Exception as e:
                    logger.warning(f"Failed to get cluster context: {e}")
                    errors.append(f"Failed to connect to infrastructure monitoring: {str(e)}")

            # Entity discovery context from Qdrant
            if any(kw in prompt_lower for kw in ["service", "server", "ip", "host", "device"]):
                try:
                    resp = await client.post(
                        QDRANT_URL + "/collections/entities/points/scroll",
                        json={"limit": 50, "with_payload": True}
                    )
                    if resp.status_code == 200:
                        data = resp.json()
                        points = data.get("result", {}).get("points", [])
                        context["entities"] = [p.get("payload", {}) for p in points]
                except:
                    pass

            # TrueNAS context for storage queries
            if any(kw in prompt_lower for kw in ["storage", "nas", "truenas", "zfs", "pool", "dataset", "disk", "share", "smb", "nfs"]):
                try:
                    resp = await client.get("http://truenas-mcp:8000/api/pools")
                    if resp.status_code == 200:
                        data = resp.json()
                        if data.get("status") == "ok":
                            context["truenas_pools"] = data.get("data", [])
                    resp = await client.get("http://truenas-mcp:8000/api/datasets")
                    if resp.status_code == 200:
                        data = resp.json()
                        if data.get("status") == "ok":
                            context["truenas_datasets"] = data.get("data", [])
                    resp = await client.get("http://truenas-mcp:8000/api/disks")
                    if resp.status_code == 200:
                        data = resp.json()
                        if data.get("status") == "ok":
                            context["truenas_disks"] = data.get("data", [])
                except Exception as e:
                    logger.warning(f"Failed to get TrueNAS context: {e}")
                    errors.append(f"TrueNAS connection failed: {str(e)}")

            # Home Assistant context for smart home queries
            if any(kw in prompt_lower for kw in ["home", "light", "sensor", "climate", "temperature", "humidity", "automation", "smart", "zigbee", "z-wave"]):
                try:
                    resp = await client.get("http://home-assistant-mcp:8000/api/states")
                    if resp.status_code == 200:
                        data = resp.json()
                        if data.get("status") == "ok":
                            context["home_assistant_states"] = data.get("data", [])
                except Exception as e:
                    logger.warning(f"Failed to get Home Assistant context: {e}")
                    errors.append(f"Home Assistant connection failed: {str(e)}")

            # Proxmox context for VM queries
            if any(kw in prompt_lower for kw in ["vm", "virtual", "proxmox", "container", "lxc", "qemu", "hypervisor"]):
                try:
                    resp = await client.get("http://proxmox-mcp:8000/api/vms")
                    if resp.status_code == 200:
                        data = resp.json()
                        if data.get("status") == "ok":
                            context["proxmox_vms"] = data.get("data", [])
                    resp = await client.get("http://proxmox-mcp:8000/api/nodes")
                    if resp.status_code == 200:
                        data = resp.json()
                        if data.get("status") == "ok":
                            context["proxmox_nodes"] = data.get("data", [])
                except Exception as e:
                    logger.warning(f"Failed to get Proxmox context: {e}")
                    errors.append(f"Proxmox connection failed: {str(e)}")

            # OPNsense context for firewall queries
            if any(kw in prompt_lower for kw in ["firewall", "opnsense", "gateway", "route", "dhcp", "dns", "rule", "nat", "vpn"]):
                try:
                    resp = await client.get("http://opnsense-mcp:8000/api/gateway_status")
                    if resp.status_code == 200:
                        data = resp.json()
                        if data.get("status") == "ok":
                            context["opnsense_gateways"] = data.get("data", [])
                    resp = await client.get("http://opnsense-mcp:8000/api/dhcp_leases")
                    if resp.status_code == 200:
                        data = resp.json()
                        if data.get("status") == "ok":
                            context["opnsense_dhcp_leases"] = data.get("data", [])
                except Exception as e:
                    logger.warning(f"Failed to get OPNsense context: {e}")
                    errors.append(f"OPNsense connection failed: {str(e)}")

            # Coroot context for monitoring queries (via observability-mcp)
            if any(kw in prompt_lower for kw in ["metric", "alert", "anomaly", "monitoring", "performance", "latency", "error rate", "health"]):
                try:
                    resp = await client.post(
                        COROOT_MCP_URL + "/mcp",
                        json={"jsonrpc": "2.0", "id": 1, "method": "tools/call",
                              "params": {"name": "coroot_get_recent_anomalies", "arguments": {"params": {"hours": 24}}}}
                    )
                    if resp.status_code == 200:
                        result = resp.json()
                        content = result.get("result", {}).get("content", [])
                        if content:
                            context["coroot_anomalies"] = content[0].get("text", "")
                    resp = await client.post(
                        COROOT_MCP_URL + "/mcp",
                        json={"jsonrpc": "2.0", "id": 2, "method": "tools/call",
                              "params": {"name": "coroot_get_alerts", "arguments": {}}}
                    )
                    if resp.status_code == 200:
                        result = resp.json()
                        content = result.get("result", {}).get("content", [])
                        if content:
                            context["coroot_alerts"] = content[0].get("text", "")
                except Exception as e:
                    logger.warning(f"Failed to get Coroot context: {e}")
                    errors.append(f"Coroot connection failed: {str(e)}")

            # AdGuard context for DNS/ad blocking queries
            if any(kw in prompt_lower for kw in ["adguard", "dns", "blocking", "ad", "filter", "query log"]):
                try:
                    resp = await client.get("http://adguard-mcp:8000/api/stats")
                    if resp.status_code == 200:
                        data = resp.json()
                        if data.get("status") == "ok":
                            context["adguard_stats"] = data.get("data", {})
                except Exception as e:
                    logger.warning(f"Failed to get AdGuard context: {e}")
                    errors.append(f"AdGuard connection failed: {str(e)}")

            # Runbooks from Qdrant for troubleshooting context
            if any(kw in prompt_lower for kw in ["troubleshoot", "fix", "error", "problem", "issue", "runbook", "how to"]):
                try:
                    resp = await client.post(
                        QDRANT_URL + "/collections/runbooks/points/scroll",
                        json={"limit": 10, "with_payload": True}
                    )
                    if resp.status_code == 200:
                        data = resp.json()
                        points = data.get("result", {}).get("points", [])
                        context["available_runbooks"] = [
                            {"name": p.get("payload", {}).get("name"), "description": p.get("payload", {}).get("description")}
                            for p in points
                        ]
                except:
                    pass

            # Web search for research queries (explicitly requested searches)
            if any(kw in prompt_lower for kw in ["search for", "look up", "find online", "research", "what is the latest", "news about"]):
                try:
                    # Extract search query from prompt
                    search_query = prompt  # Use full prompt as search query
                    resp = await client.post(
                        "http://web-search-mcp:8000/mcp",
                        json={"tool": "web_search", "arguments": {"query": search_query, "num_results": 5}}
                    )
                    if resp.status_code == 200:
                        data = resp.json()
                        if data.get("success"):
                            context["web_search_results"] = data.get("result", [])
                except Exception as e:
                    logger.warning(f"Failed to get web search results: {e}")
                    errors.append(f"Web search failed: {str(e)}")

            # Arr Suite context for media queries
            if any(kw in prompt_lower for kw in ["sonarr", "radarr", "media", "movie", "tv show", "download", "arr"]):
                try:
                    resp = await client.get("http://arr-suite-mcp:8000/api/status")
                    if resp.status_code == 200:
                        data = resp.json()
                        if data.get("status") == "ok":
                            context["arr_suite_status"] = data.get("data", {})
                except Exception as e:
                    logger.warning(f"Failed to get Arr Suite context: {e}")
                    errors.append(f"Arr Suite connection failed: {str(e)}")

        # Include errors so Gemini knows when data wasn't available
        if errors:
            context["_data_fetch_errors"] = errors

        return context

    @app.post("/query")
    async def query_llm(request: QueryRequest):
        """
        Direct query endpoint for LLM access.
        Builds context from MCPs before querying Gemini.
        """
        # Build context from MCPs based on query
        mcp_context = await build_query_context(request.prompt)

        # Merge with any provided context
        full_context = {**(request.context or {}), **mcp_context}

        if request.use_claude:
            result = await call_claude_agent(
                request.prompt,
                context=full_context,
                allowed_tools=["Read", "Glob", "Grep", "WebSearch", "WebFetch"],
                priority=PRIORITY_USER
            )
        else:
            # Build context-enriched prompt for Gemini
            context_str = ""
            errors_str = ""
            has_real_data = False

            if full_context:
                # Extract errors if any
                fetch_errors = full_context.pop("_data_fetch_errors", [])
                if fetch_errors:
                    errors_str = "\n\n## DATA FETCH ERRORS - TELL USER ABOUT THESE:\n" + "\n".join(f"- {e}" for e in fetch_errors) + "\n"
                if full_context:
                    has_real_data = True
                    context_str = f"\n\n## ACTUAL DATA FROM HOMELAB:\n{json.dumps(full_context, indent=2, default=str)}\n\n"

            # Build messages list with conversation history
            llm_messages = []

            # Always include comprehensive system prompt with anti-hallucination rules
            system_content = AGENT_SYSTEM_PROMPT + "\n\n## IMPORTANT RULES FOR THIS QUERY:\n"
            if has_real_data:
                system_content += "- Answer based ONLY on the actual data provided below\n"
                system_content += "- Do not invent any device names, IPs, versions, or statuses\n"
            else:
                system_content += "- CRITICAL: Could not retrieve homelab data\n"
                system_content += "- Tell the user what systems failed to respond\n"
                system_content += "- DO NOT make up, guess, or hallucinate any information\n"
            llm_messages.append({"role": "system", "content": system_content})

            # Build user message with context
            if has_real_data:
                enriched_prompt = f"{errors_str}{context_str}USER QUERY: {request.prompt}"
            else:
                enriched_prompt = (
                    f"CRITICAL: I could not retrieve any data from the homelab systems.\n"
                    f"{errors_str}\n"
                    f"USER QUERY: {request.prompt}\n\n"
                    f"Explain what systems failed to respond and suggest troubleshooting steps."
                )

            # Add conversation history if provided
            if request.messages:
                for msg in request.messages[:-1]:  # Exclude last message (current prompt)
                    llm_messages.append(msg)

            # Add current message with context
            current_content = enriched_prompt if not request.messages else f"{context_str}{errors_str}USER: {request.prompt}"
            llm_messages.append({"role": "user", "content": current_content})

            result = await call_litellm(llm_messages)

        return {"response": result, "conversation_id": request.conversation_id}

    #  Skill Router Endpoint (Project 05) 

    class SkillRequest(BaseModel):
        """Request to execute a skill via slash command."""
        skill_id: str
        command: str
        query: str
        source: str = "api"
        room_id: Optional[str] = None

    @app.post("/skill")
    async def execute_skill(request: SkillRequest):
        """Execute a skill by ID or command routing.

        This endpoint is called by Matrix bot for slash commands like:
        /troubleshoot pod crash -> infrastructure-skill
        /home turn on lights -> home-skill
        """
        logger.info(f"Skill request: {request.skill_id} - {request.command} - {request.query}")

        try:
            # Look up skill from Qdrant
            skill = await get_skill_by_id(request.skill_id)
            if not skill:
                raise HTTPException(status_code=404, detail=f"Skill not found: {request.skill_id}")

            skill_name = skill.get("name", request.skill_id)
            skill_context = {
                "skill_id": request.skill_id,
                "skill_name": skill_name,
                "domain": skill.get("domain", ""),
                "mcps_primary": skill.get("mcps_primary", []),
                "mcps_secondary": skill.get("mcps_secondary", []),
                "runbook_patterns": skill.get("runbook_patterns", []),
                "priority_keywords": skill.get("priority_keywords", []),
            }

            # Build prompt with skill context
            full_prompt = (
                f"You are handling a {skill_name} request.\n"
                f"Domain: {skill.get('domain', 'general')}\n"
                f"Command: {request.command}\n"
                f"User Query: {request.query}\n\n"
                f"Based on the skill context, provide a helpful response. "
                f"If this requires infrastructure actions, describe what would be done. "
                f"For informational queries, provide the relevant information."
            )

            # Call LLM with skill context
            messages = [
                {"role": "system", "content": AGENT_SYSTEM_PROMPT},
                {"role": "user", "content": full_prompt}
            ]

            result = await call_litellm(messages)

            # Record skill execution (for learning)
            try:
                await record_skill_execution(request.skill_id, success=True)
            except Exception as e:
                logger.warning(f"Failed to record skill execution: {e}")

            return {
                "response": result,
                "skill_id": request.skill_id,
                "skill_name": skill_name,
                "command": request.command,
                "success": True,
            }

        except HTTPException:
            raise
        except Exception as e:
            logger.error(f"Skill execution error: {e}")
            # Record failure
            try:
                await record_skill_execution(request.skill_id, success=False)
            except Exception:
                pass
            raise HTTPException(status_code=500, detail=str(e))

    async def get_skill_by_id(skill_id: str) -> Optional[dict]:
        """Fetch skill from Qdrant by ID."""
        try:
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{QDRANT_URL}/collections/skills/points/scroll",
                    json={
                        "filter": {
                            "must": [{"key": "id", "match": {"value": skill_id}}]
                        },
                        "limit": 1,
                        "with_payload": True,
                    },
                    timeout=10.0,
                )
                if response.status_code == 200:
                    points = response.json().get("result", {}).get("points", [])
                    if points:
                        return points[0].get("payload", {})
        except Exception as e:
            logger.error(f"Failed to fetch skill {skill_id}: {e}")
        return None

    async def record_skill_execution(skill_id: str, success: bool):
        """Record skill execution for learning metrics."""
        try:
            async with httpx.AsyncClient() as client:
                # Get current skill
                skill = await get_skill_by_id(skill_id)
                if not skill:
                    return

                # Update counts
                success_count = skill.get("success_count", 0) + (1 if success else 0)
                failure_count = skill.get("failure_count", 0) + (0 if success else 1)
                total = skill.get("total_executions", 0) + 1

                # Update via Qdrant set_payload
                # Note: This is a simplified update - production would use proper point ID
                logger.info(f"Skill {skill_id}: success={success_count}, failure={failure_count}, total={total}")
        except Exception as e:
            logger.warning(f"Failed to record skill execution: {e}")

    @app.post("/claude/run")
    async def run_claude_task(prompt: str, context: Optional[dict] = None):
        """Direct endpoint to run a Claude Agent task with user priority."""
        result = await call_claude_agent(
            prompt,
            context=context,
            priority=PRIORITY_USER  # User requests get highest priority
        )
        return {"response": result}

    @app.post("/runbook/store")
    async def store_runbook_api(runbook_data: dict):
        """Store a diagnostic runbook via API."""
        try:
            runbook = Runbook(**runbook_data)
            success = await store_runbook_qdrant(runbook)
            if success:
                return {"status": "stored", "runbook_id": runbook.id, "name": runbook.name}
            else:
                raise HTTPException(status_code=500, detail="Failed to store runbook")
        except Exception as e:
            logger.error(f"Failed to store runbook: {e}")
            raise HTTPException(status_code=400, detail=str(e))

    @app.get("/runbook/{runbook_id}")
    async def get_runbook(runbook_id: str):
        """Get a runbook by ID."""
        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                response = await client.get(
                    QDRANT_URL + f"/collections/runbooks/points/{runbook_id}"
                )
                if response.status_code == 200:
                    payload = response.json().get("result", {}).get("payload", {})
                    return payload
                return None
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))

    @app.get("/runbooks")
    async def list_runbooks(limit: int = 20):
        """List all runbooks."""
        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                response = await client.post(
                    QDRANT_URL + "/collections/runbooks/points/scroll",
                    json={"limit": limit, "with_payload": True}
                )
                if response.status_code == 200:
                    points = response.json().get("result", {}).get("points", [])
                    return {
                        "runbooks": [
                            {
                                "id": p.get("id"),
                                "name": p.get("payload", {}).get("name", "Unknown"),
                                "description": p.get("payload", {}).get("description", ""),
                                "version": 2 if "diagnosis" in p.get("payload", {}) else 1,
                                "success_count": p.get("payload", {}).get("metadata", {}).get("success_count", 0),
                                "auto_approve": p.get("payload", {}).get("metadata", {}).get("auto_approve_eligible", False)
                            }
                            for p in points
                        ],
                        "count": len(points)
                    }
                return {"runbooks": [], "count": 0}
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))

    @app.delete("/runbook/{runbook_id}")
    async def delete_runbook(runbook_id: str):
        """Delete a runbook by ID."""
        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                response = await client.post(
                    QDRANT_URL + "/collections/runbooks/points/delete",
                    json={"points": [runbook_id]}
                )
                return {"status": "deleted" if response.status_code == 200 else "failed"}
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))

    class SimpleRunbookRequest(BaseModel):
        """Simple runbook creation - Claude provides all details, no LLM needed."""
        name: str = Field(..., description="Runbook name")
        description: str = Field(..., description="What this runbook handles")
        keywords: List[str] = Field(..., description="Keywords for matching alerts")
        diagnosis_command: str = Field(default="echo 'Check manually'", description="Command to verify issue exists")
        fix_commands: List[str] = Field(default_factory=list, description="Commands to fix the issue")
        validation_command: str = Field(default="echo 'OK'", description="Command to verify fix worked")
        escalate_if: List[str] = Field(default_factory=lambda: ["confidence < 0.7"], description="Conditions to escalate")

    @app.post("/runbook/create-simple")
    async def create_simple_runbook(request: SimpleRunbookRequest):
        """
        Create a runbook from Claude-provided details. No LLM call needed.

        Claude (the architect) creates runbooks after fixing issues.
        Gemini (the operator) uses these runbooks to handle future occurrences.
        """
        logger.info(f"Creating runbook: {request.name}")

        try:
            runbook = Runbook(
                name=request.name,
                description=request.description,
                triggers=TriggerConfig(
                    alert_patterns=[],
                    keywords=request.keywords
                ),
                diagnosis=DiagnosisConfig(
                    checks=[DiagnosisCheck(name="Verify issue", command=request.diagnosis_command)],
                    confidence_rules=[]
                ),
                decision=DecisionRules(
                    escalate_if=request.escalate_if,
                    handle_locally_if=["confidence >= 0.8"]
                ),
                fix=FixConfig(
                    steps=[
                        FixStep(name=f"Step {i+1}", action="command", command=cmd)
                        for i, cmd in enumerate(request.fix_commands)
                    ] if request.fix_commands else [
                        FixStep(name="Manual fix", action="command", command="echo 'Apply fix manually'")
                    ],
                    validation=[ValidationCheck(name="Verify fix", command=request.validation_command)]
                ),
                metadata=RunbookMetadata(
                    created_by="claude-code",
                    created_from="fix-session",
                    auto_approve_eligible=False
                )
            )

            success = await store_runbook_qdrant(runbook)

            if success:
                logger.info(f"Created runbook: {runbook.name} ({runbook.id})")
                return {
                    "status": "created",
                    "runbook_id": runbook.id,
                    "name": runbook.name,
                    "description": runbook.description,
                    "keywords": request.keywords
                }
            else:
                raise HTTPException(status_code=500, detail="Failed to store runbook")

        except Exception as e:
            logger.error(f"Failed to create runbook: {e}")
            raise HTTPException(status_code=500, detail=str(e))

    @app.get("/health")
    async def health():
        return {"status": "healthy"}

    def main():
        port = int(os.environ.get("PORT", "8000"))
        logger.info("Starting LangGraph orchestrator on port %d", port)
        uvicorn.run(app, host="0.0.0.0", port=port)

    if __name__ == "__main__":
        main()

  requirements.txt: |
    langgraph>=0.2.0
    langchain-core>=0.3.0
    langchain-openai>=0.2.0
    asyncpg>=0.30.0
    psycopg>=3.2.0
    psycopg-pool>=3.2.0
    redis>=5.2.0
    fastapi>=0.115.0
    uvicorn>=0.34.0
    httpx>=0.28.0
    pydantic>=2.11.0
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: langgraph
  namespace: ai-platform
  labels:
    app: langgraph
    component: orchestrator
spec:
  replicas: 1
  selector:
    matchLabels:
      app: langgraph
  template:
    metadata:
      labels:
        app: langgraph
        component: orchestrator
    spec:
      initContainers:
        - name: install-deps
          image: python:3.11-slim
          command: ['sh', '-c', 'pip install --target=/app/deps -r /code/requirements.txt']
          volumeMounts:
            - name: code
              mountPath: /code
            - name: deps
              mountPath: /app/deps
      containers:
        - name: langgraph
          image: python:3.11-slim
          command: ['sh', '-c', 'cd /app && PYTHONPATH=/app/deps python /code/main.py']
          ports:
            - containerPort: 8000
              name: http
          env:
            - name: PORT
              value: "8000"
            # Python path for pip installed deps
            - name: PYTHONPATH
              value: "/app/deps"
            # Core services
            - name: POSTGRES_URL
              value: "postgresql://postgres:postgres@postgres:5432/langgraph"
            - name: REDIS_URL
              value: "redis://redis:6379"
            - name: LITELLM_URL
              value: "http://litellm:4000"
            - name: QDRANT_URL
              value: "http://qdrant:6333"
            # Claude services (validator runs separately as daily cronjob)
            - name: CLAUDE_AGENT_URL
              value: "http://claude-agent:8000"
            # Matrix bot (replaces Telegram)
            - name: MATRIX_BOT_URL
              value: "http://matrix-bot:8000"
            # Afferent mobile PWA for alert approvals
            - name: AFFERENT_WEBHOOK_URL
              value: "http://10.10.0.22:3456/webhook/new-approval"
            # Model configuration - Gemini only
            - name: GEMINI_MODEL
              value: "gemini/gemini-2.0-pro"
            - name: GEMINI_FLASH_MODEL
              value: "gemini/gemini-2.0-flash"
            - name: EMBEDDING_MODEL
              value: "embeddings"
            # Tiered Runbook Matching Thresholds
            # >= EXACT: Execute as-is
            # >= SIMILAR (but < EXACT): Execute with tweaks, propose runbook update
            # < SIMILAR: Escalate to Brain Trust (humans via Outline)
            - name: RUNBOOK_EXACT_THRESHOLD
              value: "0.95"
            - name: RUNBOOK_SIMILAR_THRESHOLD
              value: "0.80"
            # Legacy threshold (backward compatibility, equals SIMILAR)
            - name: RUNBOOK_MATCH_THRESHOLD
              value: "0.80"
            - name: CONTEXT_CACHE_TTL
              value: "3600"
            # Brain Trust Escalation (Outline wiki)
            - name: BRAIN_TRUST_COLLECTION_ID
              value: "9bd8c738-4505-494a-a867-74349a887dc6"
            - name: OUTLINE_MCP_URL
              value: "http://outline-mcp:8000"
            # Phase 7: Approval Policy (VERBOSE/STANDARD/AGGRESSIVE)
            - name: APPROVAL_POLICY
              value: "STANDARD"
            - name: AUTO_EXECUTE_MIN_CONFIDENCE
              value: "0.9"
            # Phase 7: Temporal Pattern Detection
            - name: TEMPORAL_WINDOW_HOURS
              value: "24"
            - name: RECURRING_ALERT_THRESHOLD
              value: "3"
            # Phase 7: GitHub Integration for Runbook Improvements
            - name: GITHUB_OWNER
              value: "charlieshreck"
            - name: GITHUB_REPO
              value: "agentic_lab"
            - name: EXTERNAL_MCP_URL
              value: "http://external-mcp:8000"
            # A2A API token for MCP REST bridge authentication
            - name: A2A_API_TOKEN
              valueFrom:
                secretKeyRef:
                  name: a2a-api-token
                  key: TOKEN
            # Observability MCP for alerting tools
            - name: OBSERVABILITY_MCP_URL
              value: "http://observability-mcp:8000"
            # Project 05: Skill Router Configuration
            - name: SKILL_ROUTER_ENABLED
              value: "true"
            - name: SKILL_COLLISION_THRESHOLD
              value: "0.2"
            - name: MAX_MCPS_PER_REQUEST
              value: "8"
          # Import ALL MCP server URLs from shared ConfigMap
          # Source: /home/.mcp.json via sync-mcp-config.sh
          envFrom:
            - configMapRef:
                name: mcp-servers-config
          volumeMounts:
            - name: code
              mountPath: /code
            - name: deps
              mountPath: /app/deps
          resources:
            requests:
              memory: "256Mi"
              cpu: "200m"
            limits:
              memory: "512Mi"
              cpu: "1000m"
          readinessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 60
            periodSeconds: 10
            timeoutSeconds: 5
          livenessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 90
            periodSeconds: 30
            timeoutSeconds: 5
      volumes:
        - name: code
          configMap:
            name: langgraph-code
        - name: deps
          emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: langgraph
  namespace: ai-platform
  labels:
    app: langgraph
    component: orchestrator
spec:
  selector:
    app: langgraph
  ports:
    - port: 8000
      targetPort: 8000
      name: http
