apiVersion: v1
kind: ConfigMap
metadata:
  name: langgraph-code
  namespace: ai-platform
  labels:
    app: langgraph
data:
  main.py: |
    #!/usr/bin/env python3
    """LangGraph Orchestrator for Agentic AI Platform with Diagnostic Runbooks."""
    import os
    import logging
    import json
    import re
    import uuid as uuid_mod
    from typing import TypedDict, Annotated, Sequence, Optional, List, Dict, Any
    from datetime import datetime, timedelta
    import httpx
    import asyncio
    from textwrap import dedent

    from langgraph.graph import StateGraph, END
    from langchain_core.messages import BaseMessage

    from fastapi import FastAPI, HTTPException, Request
    from pydantic import BaseModel, Field
    import uvicorn

    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

    # Environment configuration
    # Core service URLs
    POSTGRES_URL = os.environ.get("POSTGRES_URL", "postgresql://postgres:postgres@postgres:5432/langgraph")
    REDIS_URL = os.environ.get("REDIS_URL", "redis://redis:6379")
    LITELLM_URL = os.environ.get("LITELLM_URL", "http://litellm:4000")
    QDRANT_URL = os.environ.get("QDRANT_URL", "http://qdrant:6333")

    # Claude services - agent for escalation (validator runs separately as daily cronjob)
    CLAUDE_AGENT_URL = os.environ.get("CLAUDE_AGENT_URL", "http://claude-agent:8000")

    # MCP servers - loaded dynamically from mcp-servers-config ConfigMap
    # These env vars are injected via envFrom in deployment
    def load_mcp_urls():
        """Load all MCP URLs from environment variables."""
        urls = {}
        mcp_names = [
            "knowledge", "infrastructure", "coroot", "proxmox", "opnsense",
            "adguard", "cloudflare", "unifi", "truenas", "home_assistant",
            "arr_suite", "homepage", "web_search", "browser_automation",
            "plex", "vikunja", "neo4j", "tasmota", "monitoring", "keep", "infisical",
            "github", "wikipedia", "reddit", "outline"
        ]
        for name in mcp_names:
            env_key = f"{name.upper()}_MCP_URL"
            url = os.environ.get(env_key)
            if url:
                urls[name.replace("_", "-")] = url
        return urls

    MCP_URLS = load_mcp_urls()
    logger.info(f"Loaded {len(MCP_URLS)} MCP server URLs from environment")

    # Convenience accessors for frequently used MCPs
    KNOWLEDGE_MCP_URL = os.environ.get("KNOWLEDGE_MCP_URL", "http://knowledge-mcp:8000")
    INFRASTRUCTURE_MCP_URL = os.environ.get("INFRASTRUCTURE_MCP_URL", "http://infrastructure-mcp:8000")
    COROOT_MCP_URL = os.environ.get("COROOT_MCP_URL", "http://coroot-mcp:8000")
    OPNSENSE_MCP_URL = os.environ.get("OPNSENSE_MCP_URL", "http://opnsense-mcp:8000")
    UNIFI_MCP_URL = os.environ.get("UNIFI_MCP_URL", "http://unifi-mcp:8000")
    TRUENAS_MCP_URL = os.environ.get("TRUENAS_MCP_URL", "http://truenas-mcp:8000")
    PROXMOX_MCP_URL = os.environ.get("PROXMOX_MCP_URL", "http://proxmox-mcp:8000")
    HOME_ASSISTANT_MCP_URL = os.environ.get("HOME_ASSISTANT_MCP_URL", "http://home-assistant-mcp:8000")
    TASMOTA_MCP_URL = os.environ.get("TASMOTA_MCP_URL", "http://tasmota-mcp:8000")
    MONITORING_MCP_URL = os.environ.get("MONITORING_MCP_URL", "http://monitoring-mcp:8000")

    # Matrix bot for notifications (replaces Telegram)
    MATRIX_BOT_URL = os.environ.get("MATRIX_BOT_URL", "http://matrix-bot:8000")

    # Model configuration - Gemini only (via LiteLLM)
    # Using 2.0-flash for everything - best free tier limits (1500 RPD, 10 RPM)
    GEMINI_MODEL = os.environ.get("GEMINI_MODEL", "gemini/gemini-2.0-flash")
    GEMINI_FLASH_MODEL = os.environ.get("GEMINI_FLASH_MODEL", "gemini/gemini-2.0-flash")
    EMBEDDING_MODEL = os.environ.get("EMBEDDING_MODEL", "embeddings")

    # Thresholds and configuration - TIERED RUNBOOK MATCHING
    # >= 0.95: EXACT match - execute runbook as-is
    # >= 0.80: SIMILAR match - execute with tweaks, propose runbook update PR
    # <  0.80: NO_MATCH - escalate to Brain Trust (humans via Outline)
    RUNBOOK_EXACT_THRESHOLD = float(os.environ.get("RUNBOOK_EXACT_THRESHOLD", "0.95"))
    RUNBOOK_SIMILAR_THRESHOLD = float(os.environ.get("RUNBOOK_SIMILAR_THRESHOLD", "0.80"))
    # Legacy threshold for backward compatibility
    RUNBOOK_MATCH_THRESHOLD = RUNBOOK_SIMILAR_THRESHOLD
    MIN_CONFIDENCE_FOR_LOCAL_FIX = float(os.environ.get("MIN_CONFIDENCE_FOR_LOCAL_FIX", "0.7"))
    AUTO_APPROVE_MIN_SUCCESSES = int(os.environ.get("AUTO_APPROVE_MIN_SUCCESSES", "5"))
    CONTEXT_CACHE_TTL = int(os.environ.get("CONTEXT_CACHE_TTL", "3600"))

    # Brain Trust Escalation - Outline collection for human review
    BRAIN_TRUST_COLLECTION_ID = os.environ.get("BRAIN_TRUST_COLLECTION_ID", "9bd8c738-4505-494a-a867-74349a887dc6")
    OUTLINE_MCP_URL = os.environ.get("OUTLINE_MCP_URL", "http://outline-mcp:8000")

    # Static context cache (refreshed hourly for cost savings)
    _static_context_cache = {}
    _static_context_timestamp = None

    # ============================================================================
    # SYSTEM PROMPT - Injected into all LLM calls
    # ============================================================================

    AGENT_SYSTEM_PROMPT = """
    You are the AI assistant for Charlie's Agentic Homelab Platform.

    Your Capabilities:

    You have access to real-time data from 25 MCP servers (data is fetched automatically based on your query):

    Infrastructure & Network:
    - infrastructure-mcp - Kubernetes cluster state, pods, deployments, services
    - unifi-mcp - WiFi clients, access points, switches, network health
    - opnsense-mcp - Firewall rules, DHCP leases, gateway status, DNS
    - proxmox-mcp - VMs, LXCs, hypervisor nodes, resource usage
    - truenas-mcp - Storage pools, datasets, ZFS health, shares
    - cloudflare-mcp - DNS records, tunnels, zone management

    Monitoring & Observability:
    - coroot-mcp - Service metrics, anomaly detection, dependencies
    - monitoring-mcp - VictoriaMetrics, AlertManager, VictoriaLogs, Grafana, Gatus
    - adguard-mcp - DNS stats, query logs, blocking rates
    - keep-mcp - Alert aggregation, incidents, deduplication

    Smart Home & IoT:
    - home-assistant-mcp - Lights, sensors, climate, automations
    - tasmota-mcp - 26 Tasmota smart devices (power control, WiFi config)

    Media & Entertainment:
    - arr-suite-mcp - Sonarr, Radarr, Prowlarr, Overseerr, Transmission
    - plex-mcp - Plex server status, libraries, active streams, GPU usage

    Knowledge & Search:
    - knowledge-mcp - Qdrant vector DB, runbooks, docs, entities, Neo4j graph
    - neo4j-mcp - Knowledge graph queries, dependencies, impact analysis
    - web-search-mcp - Internet search via SearXNG
    - browser-automation-mcp - Playwright browser automation
    - wikipedia-mcp - Wikipedia articles, summaries, search

    Information & Research:
    - github-mcp - GitHub repos, issues, PRs, code search
    - reddit-mcp - Reddit browsing, subreddit search, discussions

    Documentation:
    - outline-mcp - Outline wiki document and collection management

    Utilities:
    - vikunja-mcp - Task management, kanban boards
    - homepage-mcp - Dashboard widgets and service status
    - infisical-mcp - Secrets management (read-only)

    Qdrant Collections:
    - runbooks - Operational procedures for common issues
    - documentation - Architecture docs and guides
    - entities - Every device on the network (IP, MAC, hostname, type)
    - decisions - Historical decisions and outcomes
    - agent_events - Forever Learning System event log

    Guidelines:
    - Answer based on ACTUAL DATA provided - never invent device names, IPs, or statuses
    - If data retrieval fails, tell the user what failed and suggest troubleshooting
    - Be concise but thorough - include relevant metrics and specifics
    - When suggesting fixes, provide confidence level and reasoning
    - For complex issues, recommend escalating to Claude Code session

    Network Overview:
    - 10.10.0.0/24 - Production network (Proxmox, K8s prod cluster)
    - 10.20.0.0/24 - Agentic platform (this AI system)
    - 10.30.0.0/24 - Monitoring cluster (Grafana, Prometheus, Coroot)
    """

    # ============================================================================
    # RUNBOOK SCHEMA - Pydantic Models for Diagnostic Workflows
    # ============================================================================

    class ExtractField(BaseModel):
        """Extract a value from command output."""
        field: str = Field(..., description="JSONPath or regex pattern to extract")
        expected: Optional[Any] = Field(None, description="Expected value for confidence")
        store_as: str = Field(..., description="Variable name to store result")

    class LookForPattern(BaseModel):
        """Pattern to look for in command output."""
        pattern: str = Field(..., description="Regex pattern to search for")
        confirms: Optional[str] = Field(None, description="What finding this confirms")
        indicates: Optional[str] = Field(None, description="What finding indicates")

    class DiagnosisCheck(BaseModel):
        """A diagnostic check to verify the error."""
        name: str
        command: str = Field(..., description="Command with {{variable}} placeholders")
        extract: Optional[List[ExtractField]] = None
        look_for: Optional[List[LookForPattern]] = None
        store_as: Optional[str] = Field(None, description="Store raw output as variable")
        timeout: int = 30

    class ConfidenceRule(BaseModel):
        """Rule for calculating diagnostic confidence."""
        condition: str = Field(..., description="Python expression using stored variables")
        confidence: float = Field(..., ge=0, le=1)
        diagnosis: str
        note: Optional[str] = None

    class GatherCommand(BaseModel):
        """Command to gather additional context."""
        name: str
        command: str
        store_as: str
        on_failure: str = "continue"
        timeout: int = 30

    class Precondition(BaseModel):
        """Precondition before applying fix."""
        check: str = Field(..., description="Python expression that must be true")
        reason: str

    class FixStep(BaseModel):
        """A step in the fix process."""
        name: str
        action: str = Field(..., description="command, compute, or wait")
        command: Optional[str] = None
        formula: Optional[str] = None
        store_as: Optional[str] = None
        rollback: Optional[str] = None
        timeout: int = 60
        on_failure: str = "abort"
        max_value: Optional[str] = None

    class ValidationCheck(BaseModel):
        """Validation after fix is applied."""
        name: str
        command: str
        expected: Optional[str] = None
        should_not_contain: Optional[str] = None
        wait: int = 0
        retries: int = 1

    class DecisionRules(BaseModel):
        """Rules for deciding fix locally vs escalate."""
        escalate_if: List[str] = Field(default_factory=list)
        handle_locally_if: List[str] = Field(default_factory=list)

    class TriggerConfig(BaseModel):
        """What triggers this runbook."""
        alert_patterns: List[str] = Field(default_factory=list)
        keywords: List[str] = Field(default_factory=list)
        severity: List[str] = Field(default_factory=lambda: ["warning", "critical"])

    class DiagnosisConfig(BaseModel):
        """Diagnosis phase configuration."""
        description: str = ""
        checks: List[DiagnosisCheck] = Field(default_factory=list)
        confidence_rules: List[ConfidenceRule] = Field(default_factory=list)

    class InfoGatherConfig(BaseModel):
        """Information gathering phase."""
        description: str = ""
        commands: List[GatherCommand] = Field(default_factory=list)

    class FixConfig(BaseModel):
        """Fix phase configuration."""
        description: str = ""
        preconditions: List[Precondition] = Field(default_factory=list)
        steps: List[FixStep] = Field(default_factory=list)
        validation: List[ValidationCheck] = Field(default_factory=list)

    class RunbookMetadata(BaseModel):
        """Runbook metadata for learning."""
        created_by: str = "system"
        created_at: str = Field(default_factory=lambda: datetime.utcnow().isoformat())
        updated_at: Optional[str] = None
        success_count: int = 0
        failure_count: int = 0
        false_positive_count: int = 0
        escalation_count: int = 0
        avg_resolution_time: Optional[float] = None
        auto_approve_eligible: bool = False
        tags: List[str] = Field(default_factory=list)
        related_runbooks: List[str] = Field(default_factory=list)

    class Runbook(BaseModel):
        """Complete diagnostic runbook."""
        id: str = Field(default_factory=lambda: f"rb-{uuid_mod.uuid4().hex[:8]}")
        name: str
        version: int = 1
        description: str = ""
        triggers: TriggerConfig = Field(default_factory=TriggerConfig)
        diagnosis: DiagnosisConfig = Field(default_factory=DiagnosisConfig)
        information_gathering: InfoGatherConfig = Field(default_factory=InfoGatherConfig)
        decision: DecisionRules = Field(default_factory=DecisionRules)
        fix: FixConfig = Field(default_factory=FixConfig)
        escalation_template: str = ""
        metadata: RunbookMetadata = Field(default_factory=RunbookMetadata)

    # ============================================================================
    # RUNBOOK EXECUTOR - Executes diagnostic workflows
    # ============================================================================

    class RunbookExecutor:
        """Executes runbook diagnostic workflows."""

        def __init__(self, runbook: Runbook, alert: dict):
            self.runbook = runbook
            self.alert = alert
            self.variables: Dict[str, Any] = {}
            self.diagnosis_result: str = "unknown"
            self.confidence: float = 0.0
            self.escalation_reason: Optional[str] = None
            self.execution_log: List[str] = []
            self.rollback_stack: List[str] = []

            # Initialize variables from alert
            self.variables["alertname"] = alert.get("alertname", "")
            self.variables["severity"] = alert.get("severity", "warning")
            self.variables["description"] = alert.get("description", "")
            self.variables["namespace"] = alert.get("namespace", "default")
            self.variables["pod_name"] = self._extract_pod_name(alert)
            self.variables["node_name"] = (alert.get("labels") or {}).get("node", "")

        def _extract_pod_name(self, alert: dict) -> str:
            """Extract pod name from alert."""
            labels = alert.get("labels") or {}
            if "pod" in labels:
                return labels["pod"]
            desc = alert.get("description", "")
            match = re.search(r'pod[:\s]+([a-z0-9-]+)', desc, re.I)
            return match.group(1) if match else ""

        def _substitute_variables(self, text: str) -> str:
            """Replace {{variable}} placeholders with values."""
            def replacer(match):
                var_name = match.group(1)
                return str(self.variables.get(var_name, f"{{unknown:{var_name}}}"))
            return re.sub(r'\{\{(\w+)\}\}', replacer, text)

        async def _execute_command(self, command: str, timeout: int = 30) -> tuple:
            """Execute a command via infrastructure MCP and return (success, output)."""
            cmd = self._substitute_variables(command)
            self.execution_log.append(f"EXEC: {cmd}")

            async with httpx.AsyncClient(timeout=float(timeout + 10)) as client:
                try:
                    response = await client.post(
                        INFRASTRUCTURE_MCP_URL + "/execute",
                        json={"command": cmd, "timeout": timeout}
                    )
                    if response.status_code == 200:
                        result = response.json()
                        output = result.get("output", result.get("stdout", ""))
                        success = result.get("success", result.get("exit_code", 1) == 0)
                        self.execution_log.append(f"  -> {'OK' if success else 'FAIL'}: {output[:200]}")
                        return success, output
                    # Command execution not available - escalate
                    self.execution_log.append(f"  -> INFRA MCP unavailable ({response.status_code})")
                    return False, f"Command execution unavailable (HTTP {response.status_code})"
                except Exception as e:
                    self.execution_log.append(f"  -> ERROR: {e}")
                    return False, str(e)

        def _extract_jsonpath(self, data: str, path: str) -> Any:
            """Extract value using simplified JSONPath."""
            try:
                obj = json.loads(data)
                parts = path.replace("[", ".").replace("]", "").split(".")
                for part in parts:
                    if not part:
                        continue
                    if part.isdigit():
                        obj = obj[int(part)]
                    else:
                        obj = obj.get(part, None)
                    if obj is None:
                        return None
                return obj
            except:
                return None

        def _check_pattern(self, text: str, pattern: str) -> bool:
            """Check if pattern exists in text."""
            try:
                return bool(re.search(pattern, text, re.IGNORECASE | re.MULTILINE))
            except:
                return pattern.lower() in text.lower()

        def _evaluate_condition(self, condition: str) -> bool:
            """Evaluate a condition using stored variables."""
            try:
                # Create safe evaluation context - convert None to empty string for string checks
                safe_vars = {k: (v if v is not None else "") for k, v in self.variables.items()}
                safe_vars["true"] = True
                safe_vars["false"] = False
                # Handle common patterns
                cond = condition
                # Handle 'literal' in variable pattern (e.g., 'OOMKill' in recent_events)
                # Substitute the actual variable value at substitution time
                cond = re.sub(r"'([^']+)'\s+in\s+(\w+)",
                    lambda m: f"'{m.group(1)}' in '{str(self.variables.get(m.group(2), '') or '').replace(chr(39), chr(92)+chr(39))}'", cond)
                cond = re.sub(r"(\w+)\s+in\s+\[([^\]]+)\]",
                    lambda m: f"'{self.variables.get(m.group(1), '')}' in [{m.group(2)}]", cond)
                cond = re.sub(r"(\w+)\s*==\s*'([^']+)'",
                    lambda m: f"'{self.variables.get(m.group(1), '')}' == '{m.group(2)}'", cond)
                cond = re.sub(r"(\w+)\s*!=\s*'([^']+)'",
                    lambda m: f"'{self.variables.get(m.group(1), '')}' != '{m.group(2)}'", cond)
                cond = re.sub(r"(\w+)\s*([<>]=?)\s*(\d+)",
                    lambda m: f"{self.variables.get(m.group(1), 0) or 0} {m.group(2)} {m.group(3)}", cond)
                cond = cond.replace(" AND ", " and ").replace(" OR ", " or ")
                return eval(cond, {"__builtins__": {}}, safe_vars)
            except Exception as e:
                logger.warning(f"Condition eval failed: {condition} - {e}")
                return False

        async def run_diagnosis(self) -> dict:
            """Run diagnostic checks to verify the error."""
            logger.info(f"Running diagnosis for {self.runbook.name}")
            results = {"checks": [], "confirmations": set(), "indications": set()}

            for check in self.runbook.diagnosis.checks:
                logger.info(f"  Check: {check.name}")
                success, output = await self._execute_command(check.command, check.timeout)

                check_result = {"name": check.name, "success": success, "output": output[:500]}

                if check.store_as:
                    self.variables[check.store_as] = output

                if check.extract:
                    for ext in check.extract:
                        value = self._extract_jsonpath(output, ext.field)
                        self.variables[ext.store_as] = value
                        check_result[ext.store_as] = value
                        if ext.expected is not None:
                            matches = str(value) == str(ext.expected)
                            check_result[f"{ext.store_as}_matches"] = matches

                if check.look_for:
                    for lf in check.look_for:
                        found = self._check_pattern(output, lf.pattern)
                        if found:
                            if lf.confirms:
                                results["confirmations"].add(lf.confirms)
                            if lf.indicates:
                                results["indications"].add(lf.indicates)

                results["checks"].append(check_result)

            # Convert sets to lists for JSON serialization
            results["confirmations"] = list(results["confirmations"])
            results["indications"] = list(results["indications"])
            return results

        def calculate_confidence(self) -> tuple:
            """Calculate confidence based on diagnosis results."""
            for rule in self.runbook.diagnosis.confidence_rules:
                if self._evaluate_condition(rule.condition):
                    self.confidence = rule.confidence
                    self.diagnosis_result = rule.diagnosis
                    logger.info(f"Confidence: {self.confidence} ({self.diagnosis_result})")
                    if rule.note:
                        self.execution_log.append(f"NOTE: {rule.note}")
                    return self.confidence, self.diagnosis_result

            # Default if no rules match
            self.confidence = 0.3
            self.diagnosis_result = "unclear"
            return self.confidence, self.diagnosis_result

        async def gather_information(self) -> dict:
            """Gather additional context information."""
            logger.info("Gathering additional information...")
            gathered = {}

            for cmd in self.runbook.information_gathering.commands:
                logger.info(f"  Gather: {cmd.name}")
                success, output = await self._execute_command(cmd.command, cmd.timeout)

                if success or cmd.on_failure == "continue":
                    self.variables[cmd.store_as] = output
                    gathered[cmd.store_as] = output[:1000]
                else:
                    gathered[cmd.store_as] = f"FAILED: {output}"

            return gathered

        def should_escalate(self) -> tuple:
            """Determine if we should escalate to Claude."""
            # Check escalation rules
            for rule in self.runbook.decision.escalate_if:
                if self._evaluate_condition(rule):
                    self.escalation_reason = rule
                    logger.info(f"Escalating due to: {rule}")
                    return True, rule

            # Check if confidence is too low
            if self.confidence < MIN_CONFIDENCE_FOR_LOCAL_FIX:
                self.escalation_reason = f"confidence ({self.confidence}) < threshold ({MIN_CONFIDENCE_FOR_LOCAL_FIX})"
                return True, self.escalation_reason

            # Check handle locally rules
            for rule in self.runbook.decision.handle_locally_if:
                if self._evaluate_condition(rule):
                    logger.info(f"Handling locally: {rule}")
                    return False, None

            # Default: escalate if uncertain
            return True, "no matching local handling rule"

        def check_preconditions(self) -> tuple:
            """Check if preconditions for fix are met."""
            for pre in self.runbook.fix.preconditions:
                if not self._evaluate_condition(pre.check):
                    return False, pre.reason
            return True, None

        async def run_fix(self) -> dict:
            """Execute the fix steps."""
            logger.info("Executing fix steps...")
            results = {"steps": [], "success": True}

            for step in self.runbook.fix.steps:
                logger.info(f"  Step: {step.name}")
                step_result = {"name": step.name, "action": step.action}

                if step.action == "command":
                    success, output = await self._execute_command(step.command, step.timeout)
                    step_result["success"] = success
                    step_result["output"] = output[:500]

                    if step.rollback:
                        self.rollback_stack.append(step.rollback)

                    if not success and step.on_failure == "abort":
                        results["success"] = False
                        results["error"] = f"Step '{step.name}' failed: {output}"
                        results["steps"].append(step_result)
                        break

                elif step.action == "compute":
                    try:
                        value = eval(self._substitute_variables(step.formula),
                                   {"__builtins__": {}}, self.variables)
                        if step.max_value:
                            max_val = self._parse_resource(step.max_value)
                            if self._parse_resource(str(value)) > max_val:
                                value = step.max_value
                        if step.store_as:
                            self.variables[step.store_as] = value
                        step_result["success"] = True
                        step_result["value"] = value
                    except Exception as e:
                        step_result["success"] = False
                        step_result["error"] = str(e)

                elif step.action == "wait":
                    await asyncio.sleep(step.timeout)
                    step_result["success"] = True

                results["steps"].append(step_result)

            return results

        def _parse_resource(self, value: str) -> int:
            """Parse resource string like '4Gi' to bytes."""
            value = str(value).strip()
            units = {"Ki": 1024, "Mi": 1024**2, "Gi": 1024**3, "Ti": 1024**4}
            for suffix, mult in units.items():
                if value.endswith(suffix):
                    return int(float(value[:-2]) * mult)
            return int(value)

        async def run_validation(self) -> dict:
            """Validate the fix worked."""
            logger.info("Running validation...")
            results = {"checks": [], "success": True}

            for val in self.runbook.fix.validation:
                logger.info(f"  Validate: {val.name}")

                if val.wait:
                    await asyncio.sleep(val.wait)

                for attempt in range(val.retries):
                    success, output = await self._execute_command(val.command, 30)
                    passed = success

                    if val.expected:
                        expected = self._substitute_variables(val.expected)
                        passed = passed and (expected in output or output.strip() == expected)

                    if val.should_not_contain:
                        forbidden = self._substitute_variables(val.should_not_contain)
                        passed = passed and (forbidden not in output)

                    if passed:
                        break

                    if attempt < val.retries - 1:
                        await asyncio.sleep(5)

                results["checks"].append({
                    "name": val.name,
                    "success": passed,
                    "output": output[:200]
                })

                if not passed:
                    results["success"] = False

            return results

        async def run_rollback(self) -> dict:
            """Execute rollback commands in reverse order."""
            logger.info("Running rollback...")
            results = []

            for cmd in reversed(self.rollback_stack):
                success, output = await self._execute_command(cmd, 60)
                results.append({"command": cmd, "success": success})

            return {"rollback_results": results}

        def format_escalation(self) -> str:
            """Format escalation message for Claude."""
            template = self.runbook.escalation_template or self._default_escalation_template()
            return self._substitute_variables(template)

        def _default_escalation_template(self) -> str:
            """Default escalation template if none provided."""
            return """## Alert Escalation: {{alertname}}

    ### Original Alert
    - Name: {{alertname}}
    - Severity: {{severity}}
    - Description: {{description}}
    - Namespace: {{namespace}}

    ### Diagnosis Attempted
    - Runbook: """ + self.runbook.name + """
    - Confidence: """ + str(self.confidence) + """
    - Diagnosis: """ + self.diagnosis_result + """
    - Escalation Reason: """ + str(self.escalation_reason) + """

    ### Gathered Information
    """ + "\n".join([f"**{k}:**\n```\n{str(v)[:500]}\n```" for k, v in self.variables.items() if k not in ["alertname", "severity", "description", "namespace"]]) + """

    ### Execution Log
    ```
    """ + "\n".join(self.execution_log[-20:]) + """
    ```

    ### Request
    Please analyze this issue and provide:
    1. Root cause analysis
    2. Recommended fix with commands
    3. A runbook definition for future occurrences

    Return the runbook as JSON at the end with this structure:
    ```json
    {
      "runbook_name": "...",
      "runbook_description": "...",
      "triggers": {"alert_patterns": [...], "keywords": [...]},
      "diagnosis_checks": [{"name": "...", "command": "...", "look_for": [...]}],
      "fix_steps": [{"name": "...", "command": "..."}],
      "validation": [{"name": "...", "command": "...", "expected": "..."}]
    }
    ```"""

        async def execute(self) -> dict:
            """Main execution entry point."""
            start_time = datetime.utcnow()
            result = {
                "runbook_id": self.runbook.id,
                "runbook_name": self.runbook.name,
                "alert": self.alert,
                "started_at": start_time.isoformat()
            }

            try:
                # Phase 1: Diagnosis
                diag_results = await self.run_diagnosis()
                result["diagnosis"] = diag_results
                self.calculate_confidence()
                result["confidence"] = self.confidence
                result["diagnosis_result"] = self.diagnosis_result

                # Phase 2: Gather information (always)
                gathered = await self.gather_information()
                result["gathered_info"] = gathered

                # Phase 3: Decision
                should_esc, reason = self.should_escalate()
                result["escalated"] = should_esc

                if should_esc:
                    result["escalation_reason"] = reason
                    result["escalation_message"] = self.format_escalation()
                    result["action"] = "escalate"
                    return result

                # Phase 4: Check preconditions
                pre_ok, pre_reason = self.check_preconditions()
                if not pre_ok:
                    result["action"] = "escalate"
                    result["escalation_reason"] = f"Precondition failed: {pre_reason}"
                    result["escalation_message"] = self.format_escalation()
                    return result

                # Phase 5: Execute fix (requires approval first)
                result["action"] = "fix_ready"
                result["fix_steps"] = [s.model_dump() for s in self.runbook.fix.steps]
                result["requires_approval"] = not self.runbook.metadata.auto_approve_eligible

                return result

            except Exception as e:
                logger.error(f"Runbook execution error: {e}")
                result["action"] = "error"
                result["error"] = str(e)
                return result

        async def execute_approved_fix(self) -> dict:
            """Execute the fix after approval."""
            result = {"runbook_id": self.runbook.id}

            try:
                fix_result = await self.run_fix()
                result["fix_result"] = fix_result

                if fix_result["success"]:
                    val_result = await self.run_validation()
                    result["validation"] = val_result

                    if val_result["success"]:
                        result["action"] = "completed"
                        result["success"] = True
                    else:
                        # Validation failed, rollback
                        rollback_result = await self.run_rollback()
                        result["rollback"] = rollback_result
                        result["action"] = "rolled_back"
                        result["success"] = False
                else:
                    # Fix failed, rollback
                    rollback_result = await self.run_rollback()
                    result["rollback"] = rollback_result
                    result["action"] = "rolled_back"
                    result["success"] = False

                return result

            except Exception as e:
                result["action"] = "error"
                result["error"] = str(e)
                result["success"] = False
                return result

    class AgentState(TypedDict):
        messages: Annotated[Sequence[BaseMessage], lambda x, y: x + y]
        alert: dict
        assessment: dict
        solutions: list
        selected_solution: dict
        approval_status: str
        execution_result: dict
        runbook_id: str
        topic_id: int
        thread_id: str
        runbook_match: dict  # Matched runbook from Qdrant if found
        handled_locally: bool  # Whether issue was handled with existing runbook

    pending_approvals = {}
    app = FastAPI(title="LangGraph Orchestrator")

    class AlertInput(BaseModel):
        id: str
        alertname: str
        severity: str = "warning"
        namespace: str = "default"
        description: str = ""
        labels: Optional[dict] = None
        annotations: Optional[dict] = None

    class KeepAlert(BaseModel):
        """Keep alert format - maps to AlertInput internally."""
        fingerprint: Optional[str] = None
        id: Optional[str] = None
        name: Optional[str] = None
        alertname: Optional[str] = None  # Alternative field name
        status: str = "firing"
        severity: str = "warning"
        lastReceived: Optional[str] = None
        description: Optional[str] = None
        message: Optional[str] = None
        labels: Optional[dict] = None
        source: Optional[list] = None
        # Accept any additional fields Keep might send
        class Config:
            extra = "allow"

    class ApprovalRequest(BaseModel):
        alert_id: str
        solution_index: int
        approved_by: str

    class IgnoreRequest(BaseModel):
        alert_id: str
        ignored_by: str

    async def call_gemini(prompt: str, context: dict = None, model: str = None) -> str:
        """
        Call Gemini via LiteLLM for operational tasks.
        Uses Gemini Pro by default (1M token context window).
        """
        model = model or GEMINI_MODEL
        messages = []

        # Always include comprehensive system prompt
        system_content = AGENT_SYSTEM_PROMPT
        if context:
            system_content += "\n\n## CURRENT CONTEXT:\n" + json.dumps(context, indent=2, default=str)
        messages.append({"role": "system", "content": system_content})

        messages.append({"role": "user", "content": prompt})

        async with httpx.AsyncClient(timeout=120.0) as client:
            try:
                response = await client.post(
                    LITELLM_URL + "/v1/chat/completions",
                    json={"model": model, "messages": messages}
                )
                response.raise_for_status()
                return response.json()["choices"][0]["message"]["content"]
            except Exception as e:
                logger.error("Gemini call failed: %s", e)
                return "Error: " + str(e)

    # Alias for backward compatibility
    async def call_litellm(messages, model=None):
        model = model or GEMINI_MODEL
        async with httpx.AsyncClient(timeout=120.0) as client:
            try:
                response = await client.post(
                    LITELLM_URL + "/v1/chat/completions",
                    json={"model": model, "messages": messages}
                )
                response.raise_for_status()
                return response.json()["choices"][0]["message"]["content"]
            except Exception as e:
                logger.error("LiteLLM call failed: %s", e)
                return "Error: " + str(e)

    async def call_knowledge_mcp(endpoint, method="GET", data=None):
        async with httpx.AsyncClient(timeout=30.0) as client:
            try:
                url = KNOWLEDGE_MCP_URL + "/" + endpoint
                if method == "GET":
                    response = await client.get(url)
                else:
                    response = await client.post(url, json=data)
                response.raise_for_status()
                return response.json()
            except Exception as e:
                logger.error("Knowledge MCP call failed: %s", e)
                return {"error": str(e)}

    async def call_matrix(endpoint: str, data: dict) -> dict:
        """Send notifications and requests to Matrix bot."""
        async with httpx.AsyncClient(timeout=30.0) as client:
            try:
                response = await client.post(MATRIX_BOT_URL + "/" + endpoint, json=data)
                response.raise_for_status()
                return response.json()
            except Exception as e:
                logger.error("Matrix bot call failed: %s", e)
                return {"error": str(e)}

    # ============================================================================
    # BRAIN TRUST ESCALATION - Outline-based human review workflow
    # ============================================================================

    async def call_outline_mcp(tool_name: str, **kwargs) -> dict:
        """Call Outline MCP for wiki document operations."""
        async with httpx.AsyncClient(timeout=30.0) as client:
            try:
                response = await client.post(
                    OUTLINE_MCP_URL + "/mcp",
                    json={
                        "jsonrpc": "2.0",
                        "id": 1,
                        "method": "tools/call",
                        "params": {"name": tool_name, "arguments": kwargs}
                    }
                )
                if response.status_code == 200:
                    result = response.json()
                    content = result.get("result", {}).get("content", [])
                    if content:
                        text = content[0].get("text", "{}")
                        try:
                            return json.loads(text)
                        except:
                            return {"result": text}
                return {"error": f"HTTP {response.status_code}"}
            except Exception as e:
                logger.error(f"Outline MCP call failed: {e}")
                return {"error": str(e)}

    def format_brain_trust_document(alert: dict, context: dict, reason: str, suggestions: list = None) -> str:
        """Format an escalation document for Brain Trust review in Outline."""
        now = datetime.utcnow().isoformat()
        doc = dedent(f"""\
            # {alert.get('alertname', 'Unknown Alert')} - Escalated {now[:10]}

            ## Alert Details
            | Field | Value |
            |-------|-------|
            | Severity | {alert.get('severity', 'warning')} |
            | Source | {alert.get('source', ['alertmanager'])} |
            | Time | {alert.get('startsAt', now)} |
            | Namespace | {alert.get('namespace', 'default')} |

            ### Description
            {alert.get('description', 'No description provided')}

            ### Labels
            ```json
            {json.dumps(alert.get('labels', {}), indent=2)}
            ```

            ## Context Gathered (via MCPs)

            """)
        # Add gathered context
        if context.get("gathered_info"):
            for key, value in context["gathered_info"].items():
                doc += f"### {key}\n```\n{str(value)[:2000]}\n```\n\n"

        if context.get("diagnosis"):
            doc += dedent(f"""\
                ### Diagnosis Results
                - Result: {context['diagnosis'].get('diagnosis_result', 'unknown')}
                - Confidence: {context['diagnosis'].get('confidence', 0):.0%}

                """)

        doc += dedent(f"""\
            ## Why Escalated
            **Reason**: {reason}

            Ollama (qwen2.5:7b) analyzed this alert but determined it requires human review because:
            - {reason}

            """)

        if suggestions:
            doc += dedent("""\
                ## Suggested Approaches
                The following approaches were considered by Ollama:

                """)
            for i, suggestion in enumerate(suggestions, 1):
                doc += f"{i}. **{suggestion.get('name', 'Suggestion')}**\n"
                doc += f"   - {suggestion.get('description', 'No description')}\n"
                if suggestion.get('risk'):
                    doc += f"   - Risk: {suggestion['risk']}\n"
                doc += "\n"

        doc += dedent("""\
            ## Brain Trust Decision
            <!-- Human reviewer: Fill in your decision below -->

            ### Approved Solution
            _Describe the solution to implement_

            ### Implementation Notes
            _Any special considerations_

            ### Assigned To
            _Who will implement (e.g., "Claude Code session")_

            ---
            *This document was auto-generated by the LangGraph Alert Handler.*
            *Collection: Brain Trust Review*
            """)
        return doc

    async def escalate_to_brain_trust(alert: dict, context: dict, reason: str, suggestions: list = None) -> dict:
        """
        Escalate an alert to Brain Trust (humans) via Outline wiki.

        This is the new escalation path replacing cloud LLM escalation.
        Creates a document in the Brain Trust Review collection for human review.

        Args:
            alert: Alert details dict
            context: Gathered context including diagnosis, MCP data
            reason: Why Ollama couldn't handle this autonomously
            suggestions: Optional list of suggested approaches

        Returns:
            Dict with escalation status and document link
        """
        logger.info(f"Escalating to Brain Trust: {alert.get('alertname')} - {reason}")

        # Format the escalation document
        doc_content = format_brain_trust_document(alert, context, reason, suggestions)
        doc_title = f"{alert.get('alertname', 'Alert')} - Escalated {datetime.utcnow().strftime('%Y-%m-%d %H:%M')}"

        # Create document in Outline via MCP
        result = await call_outline_mcp(
            "create_document",
            title=doc_title,
            collection_id=BRAIN_TRUST_COLLECTION_ID,
            text=doc_content,
            publish=True
        )

        if "error" in result:
            logger.error(f"Failed to create Brain Trust document: {result['error']}")
            # Fallback: notify via Matrix with error details
            await call_matrix("message", {
                "room_id": os.environ.get("ALERT_ROOM_ID", "!alerts:agentic.local"),
                "message": f"⚠️ **BRAIN TRUST ESCALATION FAILED**\n\nAlert: {alert.get('alertname')}\nReason: {reason}\nError: {result['error']}\n\nPlease review manually."
            })
            return {"status": "error", "error": result["error"]}

        # Notify via Matrix using the new /brain-trust endpoint
        doc_id = result.get("id", "unknown")
        outline_url = f"https://outline.kernow.io/doc/{doc_id}"

        # Build context summary for notification
        context_summary = None
        if context:
            diagnosis = context.get("diagnosis", {})
            if diagnosis:
                context_summary = f"Diagnosis: {diagnosis.get('summary', 'N/A')[:300]}"

        await call_matrix("brain-trust", {
            "alert_name": alert.get("alertname", "Unknown Alert"),
            "alert_id": alert.get("alert_id", alert.get("fingerprint", str(hash(str(alert)))[:12])),
            "severity": alert.get("severity", "info"),
            "reason": reason,
            "outline_url": outline_url,
            "outline_doc_id": doc_id,
            "context_summary": context_summary,
            "suggestions": suggestions
        })

        logger.info(f"Created Brain Trust document: {doc_id}")
        return {
            "status": "escalated",
            "document_id": doc_id,
            "collection": "Brain Trust Review",
            "reason": reason
        }

    async def propose_runbook_update(runbook_match: dict, tweaks: list, execution_result: dict) -> dict:
        """
        After executing a SIMILAR runbook with tweaks, propose an update.

        Creates a document in Brain Trust Review suggesting runbook improvements
        based on what worked.

        Args:
            runbook_match: The original matched runbook
            tweaks: List of tweaks/modifications made
            execution_result: The result of executing with tweaks

        Returns:
            Dict with proposal status
        """
        logger.info(f"Proposing runbook update for: {runbook_match.get('name')}")

        doc_content = dedent(f"""\
            # Runbook Update Proposal: {runbook_match.get('name')}

            ## Summary
            A similar alert was handled using runbook **{runbook_match.get('name')}** with modifications.
            The modifications were successful and should be considered for incorporation.

            ## Original Runbook
            - **ID**: {runbook_match.get('id')}
            - **Match Score**: {runbook_match.get('score', 0):.0%}
            - **Version**: {runbook_match.get('version', 1)}

            ## Modifications Made
            The following tweaks were applied to handle this specific case:

            """)
        for i, tweak in enumerate(tweaks, 1):
            doc_content += f"{i}. {tweak}\n"

        doc_content += dedent(f"""\

            ## Execution Result
            - **Status**: {'Success' if execution_result.get('success') else 'Partial'}
            - **Confidence**: {execution_result.get('confidence', 0):.0%}
            - **Diagnosis**: {execution_result.get('diagnosis_result', 'N/A')}

            ## Recommendation
            Consider updating the runbook to handle this case natively. Options:
            1. Add new trigger patterns to capture this alert type
            2. Add conditional logic to the fix steps
            3. Create a new runbook variant for this specific scenario

            ## Brain Trust Decision
            <!-- Human reviewer: Approve update, create variant, or dismiss -->

            ---
            *Auto-generated by LangGraph runbook learning system*
            """)

        result = await call_outline_mcp(
            "create_document",
            title=f"Runbook Update: {runbook_match.get('name')} - {datetime.utcnow().strftime('%Y-%m-%d')}",
            collection_id=BRAIN_TRUST_COLLECTION_ID,
            text=doc_content,
            publish=True
        )

        if "error" not in result:
            doc_id = result.get("id", "unknown")
            outline_url = f"https://outline.kernow.io/doc/{doc_id}"

            # Notify via Matrix using the new /runbook-proposal endpoint
            await call_matrix("runbook-proposal", {
                "alert_name": execution_result.get("alert_name", "Unknown Alert"),
                "runbook_title": runbook_match.get("name", "Unknown Runbook"),
                "tweaks_made": "\n".join(f"- {t}" for t in tweaks[:5]),
                "outline_url": outline_url,
                "execution_success": execution_result.get("success", True)
            })

        return result

    def classify_runbook_match(score: float) -> str:
        """Classify runbook match into EXACT, SIMILAR, or NO_MATCH."""
        if score >= RUNBOOK_EXACT_THRESHOLD:
            return "EXACT"
        elif score >= RUNBOOK_SIMILAR_THRESHOLD:
            return "SIMILAR"
        else:
            return "NO_MATCH"

    async def call_coroot_mcp(tool_name: str, **kwargs) -> dict:
        """Call Coroot MCP for metrics and anomaly data."""
        async with httpx.AsyncClient(timeout=30.0) as client:
            try:
                response = await client.post(
                    COROOT_MCP_URL + "/mcp",
                    json={
                        "jsonrpc": "2.0",
                        "id": 1,
                        "method": "tools/call",
                        "params": {"name": tool_name, "arguments": kwargs}
                    }
                )
                if response.status_code == 200:
                    result = response.json()
                    content = result.get("result", {}).get("content", [])
                    if content:
                        return json.loads(content[0].get("text", "{}"))
                return {}
            except Exception as e:
                logger.warning(f"Coroot MCP call failed: {e}")
                return {}

    async def discover_service(service_name: str) -> dict:
        """
        Discover service endpoint and credentials from Qdrant entities collection.
        Use this before calling infrastructure services to get their endpoints.
        """
        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                # Search entities collection in Qdrant
                response = await client.post(
                    QDRANT_URL + "/collections/entities/points/scroll",
                    json={
                        "limit": 100,
                        "with_payload": True,
                        "filter": {
                            "should": [
                                {"key": "hostname", "match": {"text": service_name}},
                                {"key": "type", "match": {"text": service_name}}
                            ]
                        } if service_name != "all" else None
                    }
                )
                if response.status_code == 200:
                    result = response.json()
                    points = result.get("result", {}).get("points", [])
                    return {"entities": [p.get("payload", {}) for p in points]}
                return {}
            except Exception as e:
                logger.warning(f"Service discovery failed for {service_name}: {e}")
                return {}

    async def get_credentials_path(service_name: str) -> str:
        """Get Infisical credentials path for a service from Qdrant device_types."""
        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                # Search device_types collection for credentials path
                response = await client.post(
                    QDRANT_URL + "/collections/device_types/points/scroll",
                    json={
                        "limit": 10,
                        "with_payload": True,
                        "filter": {
                            "must": [
                                {"key": "name", "match": {"text": service_name}}
                            ]
                        }
                    }
                )
                if response.status_code == 200:
                    result = response.json()
                    points = result.get("result", {}).get("points", [])
                    if points:
                        return points[0].get("payload", {}).get("default_credentials_path", "")
                return ""
            except Exception as e:
                logger.warning(f"Credentials path lookup failed for {service_name}: {e}")
                return ""

    async def get_embedding(text: str) -> list:
        """Get embedding from Gemini via LiteLLM."""
        async with httpx.AsyncClient(timeout=30.0) as client:
            try:
                response = await client.post(
                    LITELLM_URL + "/v1/embeddings",
                    json={"model": EMBEDDING_MODEL, "input": text}
                )
                if response.status_code == 200:
                    data = response.json()
                    if data.get("data") and len(data["data"]) > 0:
                        return data["data"][0].get("embedding", [])
                logger.warning("Embedding API returned %d", response.status_code)
                return []
            except Exception as e:
                logger.error("Embedding error: %s", e)
                return []

    async def build_comprehensive_context(alert: dict) -> dict:
        """
        Build comprehensive context for Gemini using its 1M token window.
        Combines static cached context with dynamic per-request context.
        """
        global _static_context_cache, _static_context_timestamp

        alert_text = f"{alert.get('alertname', '')} {alert.get('description', '')}"
        alert_embedding = await get_embedding(alert_text)

        # Check if static context needs refresh (hourly)
        now = datetime.utcnow()
        if (_static_context_timestamp is None or
            (now - _static_context_timestamp).total_seconds() > CONTEXT_CACHE_TTL):
            logger.info("Refreshing static context cache...")
            _static_context_cache = await _fetch_static_context()
            _static_context_timestamp = now

        # Fetch dynamic context (always fresh)
        dynamic_context = await _fetch_dynamic_context(alert, alert_embedding)

        return {
            **_static_context_cache,
            **dynamic_context,
            "alert": alert
        }

    async def _fetch_static_context() -> dict:
        """Fetch context that changes infrequently (cached hourly)."""
        context = {}

        # Get all runbooks from Qdrant
        async with httpx.AsyncClient(timeout=30.0) as client:
            try:
                response = await client.post(
                    QDRANT_URL + "/collections/runbooks/points/scroll",
                    json={"limit": 100, "with_payload": True}
                )
                if response.status_code == 200:
                    points = response.json().get("result", {}).get("points", [])
                    context["runbook_library"] = [p.get("payload", {}) for p in points]
            except Exception as e:
                logger.warning(f"Failed to fetch runbooks: {e}")
                context["runbook_library"] = []

        # Get documentation from knowledge MCP
        try:
            docs = await call_knowledge_mcp("documentation", "GET")
            context["documentation"] = docs if not docs.get("error") else []
        except:
            context["documentation"] = []

        # Get entity inventory from Qdrant
        try:
            inventory = await discover_service("all")
            context["entity_inventory"] = inventory
        except:
            context["entity_inventory"] = {}

        return context

    async def _fetch_dynamic_context(alert: dict, alert_embedding: list) -> dict:
        """Fetch fresh context for each request."""
        context = {}

        # Get cluster state from infrastructure MCP
        async with httpx.AsyncClient(timeout=15.0) as client:
            try:
                response = await client.get(INFRASTRUCTURE_MCP_URL + "/cluster/status")
                if response.status_code == 200:
                    context["cluster_state"] = response.json()
            except:
                context["cluster_state"] = {}

        # Get Coroot metrics and anomalies for affected service
        service = alert.get("labels", {}).get("service") or alert.get("namespace", "")
        if service:
            context["coroot_metrics"] = await call_coroot_mcp("get_service_metrics", service=service)
            context["coroot_anomalies"] = await call_coroot_mcp("get_recent_anomalies", hours=24)
            context["service_dependencies"] = await call_coroot_mcp("get_service_dependencies", service=service)

        # Get similar past decisions from Qdrant
        if alert_embedding:
            async with httpx.AsyncClient(timeout=10.0) as client:
                try:
                    response = await client.post(
                        QDRANT_URL + "/collections/decisions/points/search",
                        json={"vector": alert_embedding, "limit": 5, "with_payload": True}
                    )
                    if response.status_code == 200:
                        hits = response.json().get("result", [])
                        context["similar_past_decisions"] = [h.get("payload", {}) for h in hits]
                except:
                    context["similar_past_decisions"] = []

        # Get recent decisions (last 24h)
        context["recent_decisions_24h"] = await _get_recent_decisions(hours=24)

        return context

    async def _get_recent_decisions(hours: int = 24) -> list:
        """Get recent decisions from Qdrant."""
        cutoff = (datetime.utcnow() - timedelta(hours=hours)).isoformat()
        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                response = await client.post(
                    QDRANT_URL + "/collections/decisions/points/scroll",
                    json={
                        "limit": 50,
                        "with_payload": True,
                        "filter": {
                            "must": [
                                {"key": "timestamp", "range": {"gte": cutoff}}
                            ]
                        }
                    }
                )
                if response.status_code == 200:
                    points = response.json().get("result", {}).get("points", [])
                    return [p.get("payload", {}) for p in points]
            except:
                pass
        return []

    async def detect_capability_gap(gemini_response: str, alert: dict) -> Optional[dict]:
        """Detect if Gemini indicates a missing capability."""
        gap_signals = [
            "I don't have access to",
            "I cannot query",
            "No MCP available for",
            "I need a tool to",
            "Missing capability:",
            "Unable to retrieve",
        ]

        for signal in gap_signals:
            if signal.lower() in gemini_response.lower():
                # Extract description of what's needed
                gap = {
                    "description": f"Capability gap detected while processing alert: {alert.get('alertname')}",
                    "tools_needed": [],
                    "triggering_alerts": [alert.get("id", "")],
                    "gemini_response_excerpt": gemini_response[:500]
                }

                # Store to Qdrant for Claude Validator to process
                await store_capability_gap(gap)
                return gap

        return None

    async def store_capability_gap(gap: dict):
        """Store capability gap to Qdrant for later processing."""
        embedding = await get_embedding(gap.get("description", ""))
        if not embedding:
            return

        gap_id = f"cap-gap-{uuid_mod.uuid4().hex[:8]}"
        gap["id"] = gap_id
        gap["created_at"] = datetime.utcnow().isoformat()
        gap["status"] = "pending"

        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                await client.put(
                    QDRANT_URL + "/collections/capability_gaps/points",
                    json={
                        "points": [{
                            "id": str(uuid_mod.uuid5(uuid_mod.NAMESPACE_DNS, gap_id)),
                            "vector": embedding,
                            "payload": gap
                        }]
                    }
                )
                logger.info(f"Stored capability gap: {gap_id}")
            except Exception as e:
                logger.error(f"Failed to store capability gap: {e}")

    async def search_runbooks_qdrant(query: str, limit: int = 3):
        """Search Qdrant for relevant diagnostic runbooks."""
        embedding = await get_embedding(query)
        if not embedding:
            return []
        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                response = await client.post(
                    QDRANT_URL + "/collections/runbooks/points/search",
                    json={"vector": embedding, "limit": limit, "with_payload": True, "score_threshold": 0.5}
                )
                if response.status_code == 200:
                    results = []
                    for hit in response.json().get("result", []):
                        payload = hit.get("payload", {})
                        # Check if it's a v2 diagnostic runbook or legacy
                        if "diagnosis" in payload or "triggers" in payload:
                            # V2 diagnostic runbook - parse into Runbook model
                            try:
                                runbook = Runbook(**payload)
                                results.append({
                                    "id": hit.get("id"),
                                    "score": hit.get("score"),
                                    "runbook": runbook,
                                    "version": 2
                                })
                            except Exception as e:
                                logger.warning(f"Failed to parse runbook: {e}")
                        else:
                            # Legacy simple runbook
                            results.append({
                                "id": hit.get("id"),
                                "score": hit.get("score"),
                                "name": payload.get("name", "Unknown"),
                                "description": payload.get("description", ""),
                                "steps": payload.get("steps", []),
                                "fix_command": payload.get("fix_command", ""),
                                "version": 1
                            })
                    return results
                return []
            except Exception as e:
                logger.error("Runbook search error: %s", e)
                return []

    async def store_runbook_qdrant(runbook: Runbook) -> bool:
        """Store a diagnostic runbook in Qdrant."""
        search_text = f"{runbook.name} {runbook.description} {' '.join(runbook.triggers.keywords)}"
        embedding = await get_embedding(search_text)
        if not embedding:
            logger.error("Failed to generate embedding for runbook")
            return False
        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                payload = runbook.model_dump()
                # Generate a proper UUID for Qdrant (it doesn't accept arbitrary strings)
                point_id = str(uuid_mod.uuid5(uuid_mod.NAMESPACE_DNS, runbook.id))
                response = await client.put(
                    QDRANT_URL + "/collections/runbooks/points",
                    json={
                        "points": [{
                            "id": point_id,
                            "vector": embedding,
                            "payload": payload
                        }]
                    }
                )
                if response.status_code == 200:
                    logger.info(f"Stored runbook: {runbook.name} ({runbook.id}) -> point {point_id}")
                    return True
                else:
                    logger.error(f"Qdrant store failed: {response.status_code} - {response.text}")
                    return False
            except Exception as e:
                logger.error("Runbook store error: %s", e)
                return False

    async def update_runbook_stats(runbook_id: str, success: bool, escalated: bool = False):
        """Update runbook success/failure statistics."""
        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                # Get current runbook
                response = await client.get(
                    QDRANT_URL + f"/collections/runbooks/points/{runbook_id}"
                )
                if response.status_code != 200:
                    return False

                payload = response.json().get("result", {}).get("payload", {})
                metadata = payload.get("metadata", {})

                if success:
                    metadata["success_count"] = metadata.get("success_count", 0) + 1
                else:
                    metadata["failure_count"] = metadata.get("failure_count", 0) + 1
                if escalated:
                    metadata["escalation_count"] = metadata.get("escalation_count", 0) + 1

                # Check if eligible for auto-approve
                if (metadata.get("success_count", 0) >= AUTO_APPROVE_MIN_SUCCESSES and
                    metadata.get("failure_count", 0) == 0):
                    metadata["auto_approve_eligible"] = True
                    logger.info(f"Runbook {runbook_id} now eligible for auto-approve!")

                metadata["updated_at"] = datetime.utcnow().isoformat()
                payload["metadata"] = metadata

                # Update in Qdrant
                embedding = await get_embedding(f"{payload.get('name', '')} {payload.get('description', '')}")
                if embedding:
                    await client.put(
                        QDRANT_URL + "/collections/runbooks/points",
                        json={"points": [{"id": runbook_id, "vector": embedding, "payload": payload}]}
                    )
                return True
            except Exception as e:
                logger.error(f"Failed to update runbook stats: {e}")
                return False

    def parse_runbook_from_claude(response: str, alert: dict) -> Optional[Runbook]:
        """Parse a runbook definition from Claude's response."""
        try:
            # Find JSON block
            if "```json" in response:
                json_str = response.split("```json")[-1].split("```")[0]
            elif "```" in response:
                json_str = response.split("```")[-2] if response.count("```") >= 2 else response.split("```")[1]
                json_str = json_str.split("```")[0]
            else:
                return None

            data = json.loads(json_str.strip())

            # Build Runbook from Claude's response
            runbook = Runbook(
                name=data.get("runbook_name", f"Alert: {alert.get('alertname', 'Unknown')}"),
                description=data.get("runbook_description", alert.get("description", "")),
                triggers=TriggerConfig(
                    alert_patterns=data.get("triggers", {}).get("alert_patterns", [alert.get("alertname", "")]),
                    keywords=data.get("triggers", {}).get("keywords", [])
                ),
                diagnosis=DiagnosisConfig(
                    description="Verify the issue",
                    checks=[
                        DiagnosisCheck(
                            name=c.get("name", "Check"),
                            command=c.get("command", "echo 'no command'"),
                            look_for=[LookForPattern(pattern=p, confirms="issue") for p in c.get("look_for", [])]
                        )
                        for c in data.get("diagnosis_checks", [])
                    ],
                    confidence_rules=[
                        ConfidenceRule(condition="true", confidence=0.8, diagnosis="likely_match")
                    ]
                ),
                information_gathering=InfoGatherConfig(
                    description="Gather context",
                    commands=[
                        GatherCommand(name="Pod describe", command="kubectl describe pod {{pod_name}} -n {{namespace}}", store_as="pod_describe"),
                        GatherCommand(name="Pod logs", command="kubectl logs {{pod_name}} -n {{namespace}} --tail=50", store_as="pod_logs", on_failure="continue")
                    ]
                ),
                decision=DecisionRules(
                    escalate_if=["confidence < 0.7"],
                    handle_locally_if=["confidence >= 0.7"]
                ),
                fix=FixConfig(
                    description=data.get("runbook_description", "Apply fix"),
                    steps=[
                        FixStep(
                            name=s.get("name", "Step"),
                            action="command",
                            command=s.get("command", "echo 'no command'")
                        )
                        for s in data.get("fix_steps", [])
                    ],
                    validation=[
                        ValidationCheck(
                            name=v.get("name", "Validate"),
                            command=v.get("command", "echo 'ok'"),
                            expected=v.get("expected")
                        )
                        for v in data.get("validation", [])
                    ]
                ),
                metadata=RunbookMetadata(
                    created_by="claude",
                    tags=["auto-generated"]
                )
            )
            return runbook
        except Exception as e:
            logger.warning(f"Failed to parse runbook from Claude: {e}")
            return None

    # Priority levels (match claude-agent values)
    PRIORITY_USER = 1       # User-initiated requests (highest)
    PRIORITY_CRITICAL = 2   # Critical alerts
    PRIORITY_WARNING = 3    # Warning alerts
    PRIORITY_LOW = 4        # Background tasks (lowest)

    def severity_to_priority(severity: str) -> int:
        """Map alert severity to queue priority."""
        mapping = {
            "critical": PRIORITY_CRITICAL,
            "error": PRIORITY_CRITICAL,
            "warning": PRIORITY_WARNING,
            "info": PRIORITY_LOW,
        }
        return mapping.get(severity.lower(), PRIORITY_WARNING)

    async def queue_claude_task(prompt: str, priority: int = PRIORITY_LOW, context: dict = None,
                                allowed_tools: list = None, timeout: int = 600) -> str:
        """
        Queue a task for Claude Agent with priority.
        Returns task_id for polling status.
        """
        async with httpx.AsyncClient(timeout=30.0) as client:
            payload = {
                "prompt": prompt,
                "priority": priority,
                "context": context or {},
                "allowed_tools": allowed_tools or ["Read", "Glob", "Grep", "Bash", "WebSearch"],
                "working_directory": "/workspace",
                "timeout": timeout,
            }
            response = await client.post(
                CLAUDE_AGENT_URL + "/queue/submit",
                json=payload
            )
            response.raise_for_status()
            result = response.json()
            return result.get("task_id")

    async def poll_task_status(task_id: str, poll_interval: float = 2.0, max_wait: float = 600.0) -> dict:
        """
        Poll for task completion.
        Returns task result dict with status, result, error fields.
        """
        async with httpx.AsyncClient(timeout=30.0) as client:
            elapsed = 0.0
            while elapsed < max_wait:
                response = await client.get(f"{CLAUDE_AGENT_URL}/task/{task_id}")
                response.raise_for_status()
                task = response.json()
                if task.get("status") in ["completed", "failed"]:
                    return task
                await asyncio.sleep(poll_interval)
                elapsed += poll_interval
            return {"status": "timeout", "error": f"Task did not complete within {max_wait}s"}

    async def call_claude_agent(prompt: str, context: dict = None, allowed_tools: list = None,
                                priority: int = None, use_queue: bool = False, timeout: int = 300):
        """
        DEPRECATED: Cloud LLM escalation replaced with Brain Trust (Outline) escalation.

        This function now uses Ollama (qwen2.5:7b) for analysis and escalates
        complex issues to Brain Trust (humans via Outline wiki) instead of cloud LLMs.

        For backward compatibility, this still returns a response string.
        Callers should migrate to escalate_to_brain_trust() for new code.

        Args:
            prompt: The task prompt
            context: Optional context dict
            allowed_tools: Ignored (Ollama uses procedural tool calls via LangGraph)
            priority: Queue priority (ignored)
            use_queue: Ignored
            timeout: Timeout in seconds
        """
        logger.info("Processing with Ollama (qwen2.5:7b) - cloud LLMs disabled")

        # Use Ollama via LiteLLM for analysis
        analysis = await call_litellm([
            {"role": "system", "content": AGENT_SYSTEM_PROMPT},
            {"role": "user", "content": prompt}
        ])

        # Check if Ollama indicates it needs human help
        lower_analysis = analysis.lower() if analysis else ""
        needs_human = any(phrase in lower_analysis for phrase in [
            "i cannot", "i'm not sure", "need human", "escalate", "unclear",
            "insufficient information", "requires approval", "beyond my capability"
        ])

        if needs_human and context:
            # Extract alert info from context if available
            alert = context.get("alert", {"alertname": "Unknown", "description": prompt[:200]})
            await escalate_to_brain_trust(
                alert=alert,
                context=context,
                reason="Ollama analysis indicates human review needed",
                suggestions=[{"name": "Ollama Analysis", "description": analysis[:500]}]
            )
            return f"ESCALATED TO BRAIN TRUST: {analysis[:500]}..."

        return analysis

    def should_use_claude(assessment: dict) -> bool:
        """
        DEPRECATED: Determine if task should be routed to Claude.

        This function is kept for backward compatibility but always returns False.
        Complex tasks now escalate to Brain Trust (humans via Outline) instead of cloud LLMs.
        """
        return False  # Cloud LLMs disabled - use Ollama + Brain Trust escalation

    def build_assessment_prompt(alert, similar):
        return """Analyze this alert and provide assessment:
    Alert: """ + str(alert.get("alertname", "unknown")) + """
    Severity: """ + str(alert.get("severity", "warning")) + """
    Description: """ + str(alert.get("description", "")) + """
    Namespace: """ + str(alert.get("namespace", "default")) + """

    Similar runbooks found: """ + str(similar) + """

    Respond with JSON containing: domain, complexity, requires_approval, similar_runbook, similarity_score, recommended_topic"""

    def build_solutions_prompt(alert, assessment):
        return """Generate 2-3 solutions for this alert:
    Alert: """ + str(alert.get("alertname", "unknown")) + """
    Description: """ + str(alert.get("description", "")) + """
    Assessment: """ + str(assessment) + """

    Respond with JSON array containing objects with: name, description, impact, risk, commands"""

    async def assess_alert(state):
        """Assess alert using TIERED runbook matching with Brain Trust escalation.

        Match Types:
        - EXACT (>= 0.95): Execute runbook as-is
        - SIMILAR (>= 0.80): Execute with tweaks, propose runbook update
        - NO_MATCH (< 0.80): Escalate to Brain Trust (humans via Outline)
        """
        alert = state["alert"]
        alert_text = f"{alert.get('alertname', '')} {alert.get('description', '')}"

        # Step 1: Search Qdrant for diagnostic runbooks
        logger.info("Searching for matching diagnostic runbooks...")
        runbooks = await search_runbooks_qdrant(alert_text, limit=3)

        runbook_match = None
        executor_result = None
        match_type = "NO_MATCH"

        # Step 2: Classify match using tiered thresholds
        if runbooks and runbooks[0].get("score", 0) > 0:
            match = runbooks[0]
            score = match.get("score", 0)
            match_type = classify_runbook_match(score)
            logger.info(f"Runbook match: {match.get('name', 'Unknown')} (score: {score:.2f}, type: {match_type})")

            # Only process if we have at least a SIMILAR match
            if match_type in ["EXACT", "SIMILAR"]:
                if match.get("version") == 2 and match.get("runbook"):
                    # V2 diagnostic runbook - run full diagnosis
                    runbook = match["runbook"]
                    logger.info(f"Running diagnostic runbook: {runbook.name} (match: {match_type})")

                    executor = RunbookExecutor(runbook, alert)
                    executor_result = await executor.execute()

                    runbook_match = {
                        "id": match["id"],
                        "score": score,
                        "name": runbook.name,
                        "version": 2,
                        "match_type": match_type,
                        "runbook": runbook,
                        "executor": executor,
                        "executor_result": executor_result
                    }
                else:
                    # V1 legacy runbook
                    runbook_match = {
                        "id": match["id"],
                        "score": score,
                        "name": match.get("name", "Unknown"),
                        "version": 1,
                        "match_type": match_type,
                        "steps": match.get("steps", []),
                        "fix_command": match.get("fix_command", "")
                    }

        # Step 3: Build assessment based on match type and executor results
        if executor_result:
            assessment = {
                "domain": "infrastructure",
                "complexity": "low" if executor_result.get("action") == "fix_ready" else "medium",
                "requires_approval": executor_result.get("requires_approval", True),
                "confidence": executor_result.get("confidence", 0),
                "diagnosis_result": executor_result.get("diagnosis_result", "unknown"),
                "escalated": executor_result.get("escalated", False),
                "escalation_reason": executor_result.get("escalation_reason"),
                "similar_runbook": runbook_match.get("name") if runbook_match else None,
                "similarity_score": runbook_match.get("score", 0) if runbook_match else 0,
                "match_type": match_type,
                "recommended_topic": "infrastructure"
            }

            # SIMILAR matches may need tweaks - flag for potential runbook update
            if match_type == "SIMILAR":
                assessment["needs_runbook_update"] = True
                assessment["tweaks_applied"] = []  # Will be populated during execution

        elif match_type == "NO_MATCH":
            # No matching runbook - use Ollama for basic assessment, then escalate
            prompt = build_assessment_prompt(alert, runbooks)
            logger.info("No matching runbook (score < 0.80) - assessing with Ollama...")
            result = await call_litellm([
                {"role": "system", "content": AGENT_SYSTEM_PROMPT},
                {"role": "user", "content": prompt}
            ])

            try:
                clean_result = result.strip()
                if "```json" in clean_result:
                    clean_result = clean_result.split("```json")[1].split("```")[0]
                elif "```" in clean_result:
                    clean_result = clean_result.split("```")[1].split("```")[0]
                assessment = json.loads(clean_result.strip())
            except:
                assessment = {
                    "domain": "infrastructure",
                    "complexity": "high",
                    "requires_approval": True,
                    "recommended_topic": "infrastructure"
                }

            # Mark for Brain Trust escalation
            assessment["escalated"] = True
            assessment["match_type"] = "NO_MATCH"
            assessment["escalation_reason"] = "No matching runbook found (score < 0.80) - escalating to Brain Trust"
            assessment["ollama_analysis"] = result[:1000] if result else None
        else:
            # Should not reach here, but handle edge case
            assessment = {
                "domain": "infrastructure",
                "complexity": "high",
                "requires_approval": True,
                "escalated": True,
                "match_type": match_type,
                "escalation_reason": "Unexpected assessment path",
                "recommended_topic": "infrastructure"
            }

        return {
            "assessment": assessment,
            "runbook_match": runbook_match or {},
            "match_type": match_type,
            "handled_locally": match_type in ["EXACT", "SIMILAR"] and not assessment.get("escalated", False)
        }

    async def generate_solutions(state):
        """Generate solutions using TIERED runbook matching with Brain Trust escalation.

        Match handling:
        - EXACT (>= 0.95): Execute runbook as documented
        - SIMILAR (>= 0.80): Execute with Ollama-determined tweaks, propose runbook update
        - NO_MATCH (< 0.80): Escalate to Brain Trust (humans via Outline)
        """
        alert = state["alert"]
        assessment = state["assessment"]
        runbook_match = state.get("runbook_match", {})
        match_type = state.get("match_type", runbook_match.get("match_type", "NO_MATCH"))

        # =====================================================================
        # Case 1: EXACT or SIMILAR match with V2 diagnostic runbook
        # =====================================================================
        if runbook_match.get("version") == 2 and runbook_match.get("executor_result"):
            executor_result = runbook_match["executor_result"]
            runbook = runbook_match.get("runbook")

            if executor_result.get("action") == "fix_ready":
                # Diagnosis passed, fix is ready to apply
                logger.info(f"Runbook diagnosis passed (confidence: {executor_result.get('confidence', 0):.2f}, match: {match_type})")

                solutions = [{
                    "name": f"Apply Runbook: {runbook.name}",
                    "description": f"Diagnosis: {executor_result.get('diagnosis_result', 'confirmed')} (confidence: {executor_result.get('confidence', 0):.0%})",
                    "impact": "low",
                    "risk": "low",
                    "fix_steps": executor_result.get("fix_steps", []),
                    "source": "diagnostic_runbook",
                    "match_type": match_type,
                    "runbook_id": runbook_match.get("id"),
                    "runbook": runbook,
                    "executor": runbook_match.get("executor"),
                    "gathered_info": executor_result.get("gathered_info", {})
                }]

                # For SIMILAR matches, flag for potential runbook update after execution
                if match_type == "SIMILAR":
                    solutions[0]["propose_update"] = True
                    solutions[0]["similarity_score"] = runbook_match.get("score", 0)

                return {"solutions": solutions, "handled_locally": True, "match_type": match_type}

            elif executor_result.get("action") == "escalate":
                # Diagnosis determined escalation is needed - escalate to Brain Trust
                logger.info(f"Escalating to Brain Trust: {executor_result.get('escalation_reason')}")

                # Update escalation stats
                await update_runbook_stats(runbook_match.get("id"), success=False, escalated=True)

                # Escalate to Brain Trust (humans via Outline) instead of cloud LLM
                escalation_result = await escalate_to_brain_trust(
                    alert=alert,
                    context={
                        "assessment": assessment,
                        "gathered_info": executor_result.get("gathered_info", {}),
                        "diagnosis": {
                            "result": executor_result.get("diagnosis_result"),
                            "confidence": executor_result.get("confidence"),
                            "execution_log": executor_result.get("execution_log", [])
                        }
                    },
                    reason=executor_result.get("escalation_reason", "Runbook executor determined escalation needed"),
                    suggestions=[]
                )

                # Return escalation status as solution
                solutions = [{
                    "name": f"Brain Trust Review: {alert.get('alertname', 'Unknown')}",
                    "description": f"Escalated to Brain Trust for human review. Reason: {executor_result.get('escalation_reason')}",
                    "impact": "pending",
                    "risk": "pending",
                    "source": "brain_trust_escalation",
                    "escalation_document": escalation_result.get("document_id"),
                    "requires_human_action": True
                }]
                return {"solutions": solutions, "handled_locally": False, "escalated_to_brain_trust": True}

        # =====================================================================
        # Case 2: SIMILAR or EXACT match with V1 legacy runbook
        # =====================================================================
        elif runbook_match.get("version") == 1 and match_type in ["EXACT", "SIMILAR"]:
            logger.info(f"Using legacy runbook: {runbook_match.get('name')} (match: {match_type})")

            solutions = [{
                "name": f"Apply Runbook: {runbook_match.get('name')}",
                "description": "Legacy runbook (no diagnostic verification)",
                "impact": "medium",
                "risk": "medium",
                "commands": runbook_match.get("steps", []),
                "fix_command": runbook_match.get("fix_command"),
                "source": "legacy_runbook",
                "match_type": match_type,
                "runbook_id": runbook_match.get("id")
            }]

            # For SIMILAR matches, flag for potential runbook update
            if match_type == "SIMILAR":
                solutions[0]["propose_update"] = True
                solutions[0]["similarity_score"] = runbook_match.get("score", 0)

            return {"solutions": solutions, "handled_locally": True, "match_type": match_type}

        # =====================================================================
        # Case 3: NO_MATCH - Escalate to Brain Trust
        # =====================================================================
        logger.info("No matching runbook (score < 0.80) - escalating to Brain Trust for human review")

        # Use Ollama to generate initial analysis and suggestions
        analysis_prompt = dedent(f"""\
            Analyze this alert and provide suggestions:

            ALERT: {alert.get('alertname', 'Unknown')}
            SEVERITY: {alert.get('severity', 'warning')}
            DESCRIPTION: {alert.get('description', 'No description')}
            NAMESPACE: {alert.get('namespace', 'default')}
            LABELS: {json.dumps(alert.get('labels', {}))}

            Provide:
            1. Likely root cause
            2. Recommended investigation steps
            3. Potential fix approaches (if any are obvious)

            Format as JSON with fields: root_cause, investigation_steps, fix_suggestions""")

        ollama_analysis = await call_litellm([
            {"role": "system", "content": AGENT_SYSTEM_PROMPT},
            {"role": "user", "content": analysis_prompt}
        ])

        # Try to parse Ollama's suggestions
        suggestions = []
        try:
            clean_result = ollama_analysis.strip()
            if "```json" in clean_result:
                clean_result = clean_result.split("```json")[1].split("```")[0]
            elif "```" in clean_result:
                clean_result = clean_result.split("```")[1].split("```")[0]
            parsed = json.loads(clean_result.strip())

            if parsed.get("fix_suggestions"):
                for i, fix in enumerate(parsed["fix_suggestions"][:3]):
                    suggestions.append({
                        "name": f"Suggestion {i+1}: {fix if isinstance(fix, str) else fix.get('name', 'Fix')}",
                        "description": fix if isinstance(fix, str) else fix.get('description', str(fix)),
                        "risk": "unknown"
                    })
        except:
            # If parsing fails, include raw analysis as suggestion
            suggestions = [{
                "name": "Ollama Analysis",
                "description": ollama_analysis[:800] if ollama_analysis else "Analysis failed",
                "risk": "unknown"
            }]

        # Escalate to Brain Trust
        escalation_result = await escalate_to_brain_trust(
            alert=alert,
            context={
                "assessment": assessment,
                "ollama_analysis": ollama_analysis[:2000] if ollama_analysis else None
            },
            reason="No matching runbook found (score < 0.80 threshold)",
            suggestions=suggestions
        )

        # Return escalation status as solution
        solutions = [{
            "name": f"Brain Trust Review Required: {alert.get('alertname', 'Unknown')}",
            "description": f"No matching runbook found. Escalated to Brain Trust for human analysis and runbook creation.",
            "impact": "pending",
            "risk": "pending",
            "source": "brain_trust_escalation",
            "escalation_document": escalation_result.get("document_id"),
            "ollama_suggestions": suggestions,
            "requires_human_action": True
        }]

        return {"solutions": solutions, "handled_locally": False, "escalated_to_brain_trust": True}

    def _parse_solutions_from_response(result: str, alert: dict) -> list:
        """Parse solutions from Claude's response."""
        try:
            # Look for JSON solutions block
            if "```json" in result:
                parts = result.split("```json")
                for part in parts[1:]:
                    json_str = part.split("```")[0]
                    try:
                        data = json.loads(json_str.strip())
                        if isinstance(data, list):
                            return data
                        elif isinstance(data, dict) and "fix_steps" in data:
                            # This is a runbook, convert to solution
                            return [{
                                "name": data.get("runbook_name", f"Fix: {alert.get('alertname')}"),
                                "description": data.get("runbook_description", "Apply fix from Claude"),
                                "impact": "medium",
                                "risk": "medium",
                                "commands": [s.get("command", "") for s in data.get("fix_steps", [])],
                                "source": "claude"
                            }]
                    except:
                        continue
        except:
            pass

        # Default solution if parsing fails
        return [{
            "name": f"Claude Analysis: {alert.get('alertname', 'Unknown')}",
            "description": result[:500] if result else "Review Claude's analysis",
            "impact": "medium",
            "risk": "medium",
            "commands": [],
            "source": "claude",
            "full_response": result
        }]

    async def request_approval(state):
        """Request approval via Matrix bot."""
        alert = state["alert"]
        solutions = state["solutions"]
        assessment = state["assessment"]

        pending_approvals[alert["id"]] = {
            "alert": alert,
            "solutions": solutions,
            "assessment": assessment,
            "created_at": datetime.utcnow().isoformat()
        }

        # Determine room based on severity
        severity = alert.get("severity", "warning")
        room = "#critical" if severity == "critical" else "#infrastructure"

        # Format solutions for Matrix message
        solutions_text = ""
        for i, sol in enumerate(solutions, 1):
            solutions_text += f"\n{i}. **{sol.get('name', 'Solution')}**\n"
            solutions_text += f"   {sol.get('description', '')}\n"
            solutions_text += f"   Risk: {sol.get('risk', 'unknown')}\n"

        result = await call_matrix("approval", {
            "alert_id": alert["id"],
            "room": room,
            "alert": alert,
            "solutions": solutions,
            "message": (
                f"**Alert: {alert.get('alertname', 'Unknown')}**\n"
                f"Severity: {severity}\n"
                f"Namespace: {alert.get('namespace', 'default')}\n\n"
                f"{alert.get('description', 'No description')}\n\n"
                f"**Recommended Solutions:**\n"
                f"{solutions_text}\n\n"
                f"React ✅ to approve solution 1, or reply with solution number.\n"
                f"React ❌ to reject/ignore."
            ),
            "context": {
                "similar_runbook": assessment.get("similar_runbook"),
                "similarity": assessment.get("similarity_score", 0),
                "confidence": assessment.get("confidence", 0)
            }
        })

        return {"approval_status": "pending", "thread_id": result.get("thread_id", "")}

    async def execute_solution(state):
        """Execute the selected solution - uses runbook executor for diagnostic runbooks."""
        solution = state["selected_solution"]
        if not solution:
            return {"execution_result": {"success": False, "error": "No solution selected"}}

        # Case 1: Diagnostic runbook with executor
        if solution.get("source") == "diagnostic_runbook" and solution.get("executor"):
            executor = solution["executor"]
            logger.info(f"Executing diagnostic runbook fix via executor")

            # Run the approved fix through the executor
            fix_result = await executor.execute_approved_fix()

            # Update runbook stats
            runbook_id = solution.get("runbook_id")
            if runbook_id:
                await update_runbook_stats(runbook_id, success=fix_result.get("success", False))

            return {
                "execution_result": {
                    "success": fix_result.get("success", False),
                    "action": fix_result.get("action", "unknown"),
                    "fix_result": fix_result.get("fix_result", {}),
                    "validation": fix_result.get("validation", {}),
                    "rollback": fix_result.get("rollback"),
                    "summary": f"Runbook execution: {fix_result.get('action', 'unknown')}"
                }
            }

        # Case 2: Legacy runbook or direct commands
        commands = solution.get("commands", [])
        if not commands and solution.get("fix_steps"):
            # Extract commands from fix_steps
            commands = [s.get("command", "") for s in solution.get("fix_steps", []) if s.get("command")]

        if not commands:
            return {"execution_result": {"success": False, "error": "No commands to execute"}}

        logger.info(f"Executing {len(commands)} commands directly")
        results = []
        for cmd in commands:
            if not cmd:
                continue
            async with httpx.AsyncClient(timeout=120.0) as client:
                try:
                    response = await client.post(
                        INFRASTRUCTURE_MCP_URL + "/execute",
                        json={"command": cmd, "timeout": 60}
                    )
                    result = response.json()
                    results.append(result)
                    if not result.get("success", True):
                        logger.warning(f"Command failed: {cmd[:50]}...")
                except Exception as e:
                    results.append({"error": str(e), "command": cmd})
                    logger.error(f"Command error: {e}")

        success = all(r.get("success", True) and "error" not in r for r in results)

        # Update runbook stats if applicable
        runbook_id = solution.get("runbook_id")
        if runbook_id:
            await update_runbook_stats(runbook_id, success=success)

        return {
            "execution_result": {
                "success": success,
                "results": results,
                "summary": f"Executed {len(results)} commands, {'all succeeded' if success else 'some failed'}"
            }
        }

    async def record_outcome(state):
        # 1. Record decision (existing behaviour - keep)
        await call_knowledge_mcp("record", "POST", {
            "collection": "decisions",
            "data": {
                "alert_id": state["alert"]["id"],
                "alert": state["alert"],
                "assessment": state["assessment"],
                "solution": state["selected_solution"],
                "result": state["execution_result"],
                "approval_status": state["approval_status"],
                "timestamp": datetime.utcnow().isoformat()
            }
        })

        # 2. Log execution event with rich context (Project 03 - Gap 1 fix)
        event_result = await call_mcp_tool(KNOWLEDGE_MCP_URL, "log_event", {
            "event_type": "runbook.execution",
            "description": f"Executed solution for {state['alert'].get('alertname', state['alert'].get('name', 'unknown alert'))}",
            "source_agent": "langgraph",
            "metadata": {
                "alert_id": state["alert"]["id"],
                "alert_name": state["alert"].get("alertname", state["alert"].get("name")),
                "runbook_id": state.get("runbook_id"),
                "service_name": state["alert"].get("labels", {}).get("service"),
                "namespace": state["alert"].get("namespace", state["alert"].get("labels", {}).get("namespace")),
                "alert_fingerprint": state["alert"].get("fingerprint", state["alert"].get("id")),
                "endpoint": state["alert"].get("labels", {}).get("endpoint"),
                "match_type": state["assessment"].get("match_type"),
                "confidence": state["assessment"].get("confidence"),
                "success": state["execution_result"].get("success", False),
            },
            "resolution": "completed" if state["execution_result"].get("success") else "failed",
        })

        # 3. Trigger validation with 5-min cooling period (Gemini R2)
        event_id = event_result.get("event_id")
        if event_id:
            initial_state = {
                "event_id": event_id,
                "triggered_by": "langgraph-auto",
                "event": {},
                "runbook": None,
                "reported_success": state["execution_result"].get("success", False),
                "ground_truth": {},
                "verdict": "pending",
                "confidence": 0.0,
                "actual_success": None,
                "signal_count": 0,
                "validation_recorded": False,
            }
            asyncio.create_task(_delayed_validation(initial_state, delay_seconds=300))

        return {}

    def should_request_approval(state):
        assessment = state.get("assessment", {})
        if assessment.get("requires_approval", True):
            return "request_approval"
        return "execute_solution"

    # ============================================================================
    # VALIDATION SUBGRAPH - Project 03: Evaluator System
    # 4-node workflow: fetch_context → gather_truth → compute_verdict → record_validation
    # ============================================================================

    # Consolidated MCP URLs for observability tools
    OBSERVABILITY_MCP_URL = os.environ.get("OBSERVABILITY_MCP_URL", "http://observability-mcp:8000")

    class ValidationState(TypedDict):
        event_id: str
        triggered_by: str  # langgraph-auto, human, pattern-detector
        event: dict
        runbook: Optional[dict]
        reported_success: bool
        ground_truth: dict
        verdict: str  # pending, confirmed, false_positive, false_negative, uncertain
        confidence: float
        actual_success: Optional[bool]
        signal_count: int
        validation_recorded: bool

    async def call_mcp_tool(mcp_url: str, tool_name: str, arguments: dict = None) -> dict:
        """Generic MCP tool caller using JSON-RPC protocol."""
        async with httpx.AsyncClient(timeout=30.0) as client:
            try:
                response = await client.post(
                    mcp_url + "/mcp",
                    json={
                        "jsonrpc": "2.0",
                        "id": 1,
                        "method": "tools/call",
                        "params": {"name": tool_name, "arguments": arguments or {}}
                    }
                )
                if response.status_code == 200:
                    result = response.json()
                    content = result.get("result", {}).get("content", [])
                    if content:
                        return json.loads(content[0].get("text", "{}"))
                return {}
            except Exception as e:
                logger.warning(f"MCP tool call failed ({tool_name}): {e}")
                return {}

    async def fetch_validation_context(state: ValidationState) -> dict:
        """Node 1: Fetch event and runbook from knowledge-mcp."""
        event_id = state["event_id"]
        logger.info(f"[VALIDATE] Fetching context for event {event_id}")

        # Get event details
        event = await call_mcp_tool(KNOWLEDGE_MCP_URL, "get_event", {"event_id": event_id})
        if not event or event.get("error"):
            logger.error(f"[VALIDATE] Event {event_id} not found: {event}")
            return {
                "event": {},
                "runbook": None,
                "reported_success": False,
                "verdict": "uncertain",
                "confidence": 0.0,
            }

        # Get runbook if referenced
        runbook = None
        runbook_id = event.get("metadata", {}).get("runbook_id")
        if runbook_id:
            runbook = await call_mcp_tool(KNOWLEDGE_MCP_URL, "get_runbook", {"runbook_id": runbook_id})

        reported_success = event.get("metadata", {}).get("success", event.get("resolution") == "completed")

        return {
            "event": event,
            "runbook": runbook,
            "reported_success": bool(reported_success),
        }

    async def gather_ground_truth(state: ValidationState) -> dict:
        """Node 2: Query observability sources for actual state."""
        event = state.get("event", {})
        metadata = event.get("metadata", {})
        ground_truth = {}
        signal_count = 0

        if not event:
            return {"ground_truth": {}, "signal_count": 0}

        # Check Keep alert state
        alert_fingerprint = metadata.get("alert_fingerprint") or metadata.get("alert_id")
        if alert_fingerprint:
            try:
                alert_state = await call_mcp_tool(OBSERVABILITY_MCP_URL, "keep_get_alert", {"alert_id": alert_fingerprint})
                if alert_state and not alert_state.get("error"):
                    ground_truth["alert_state"] = {
                        "status": alert_state.get("status"),
                        "resolved": alert_state.get("status") in ("resolved", "inactive"),
                    }
                    signal_count += 1
            except Exception as e:
                logger.warning(f"[VALIDATE] Keep check failed: {e}")

        # Check Coroot service health
        service_name = metadata.get("service_name")
        if service_name:
            try:
                service_metrics = await call_mcp_tool(OBSERVABILITY_MCP_URL, "coroot_get_service_metrics", {"service": service_name})
                if service_metrics and not service_metrics.get("error"):
                    ground_truth["service_health"] = service_metrics
                    signal_count += 1
            except Exception as e:
                logger.warning(f"[VALIDATE] Coroot check failed: {e}")

        # Check Gatus endpoint status
        endpoint = metadata.get("endpoint")
        if endpoint:
            try:
                gatus_status = await call_mcp_tool(OBSERVABILITY_MCP_URL, "gatus_get_endpoint_status", {"endpoint": endpoint})
                if gatus_status and not gatus_status.get("error"):
                    ground_truth["endpoint_status"] = gatus_status
                    signal_count += 1
            except Exception as e:
                logger.warning(f"[VALIDATE] Gatus check failed: {e}")

        # Check Kubernetes pod state if namespace/service available
        namespace = metadata.get("namespace")
        if namespace and service_name:
            try:
                pods = await call_mcp_tool(
                    INFRASTRUCTURE_MCP_URL, "kubectl_get_pods",
                    {"namespace": namespace, "label_selector": f"app={service_name}"}
                )
                if pods and not pods.get("error"):
                    pod_list = pods if isinstance(pods, list) else pods.get("pods", [])
                    running_count = sum(1 for p in pod_list if isinstance(p, dict) and p.get("status") == "Running")
                    ground_truth["kubernetes"] = {
                        "total_pods": len(pod_list),
                        "running_pods": running_count,
                        "healthy": running_count > 0,
                    }
                    signal_count += 1
            except Exception as e:
                logger.warning(f"[VALIDATE] Kubernetes check failed: {e}")

        logger.info(f"[VALIDATE] Gathered {signal_count} ground truth signals")
        return {"ground_truth": ground_truth, "signal_count": signal_count}

    async def compute_verdict(state: ValidationState) -> dict:
        """Node 3: Deterministic verdict first, Ollama fallback if uncertain (Gemini R1)."""
        reported_success = state.get("reported_success", False)
        ground_truth = state.get("ground_truth", {})
        signal_count = state.get("signal_count", 0)

        # No signals = uncertain
        if signal_count == 0:
            logger.info("[VALIDATE] No ground truth signals, verdict: uncertain")
            return {
                "verdict": "uncertain",
                "confidence": 0.3,
                "actual_success": None,
            }

        # Deterministic evaluation
        success_signals = 0
        failure_signals = 0

        # Check alert state
        alert = ground_truth.get("alert_state", {})
        if alert:
            if alert.get("resolved"):
                success_signals += 1
            else:
                failure_signals += 1

        # Check service health
        service = ground_truth.get("service_health", {})
        if service:
            health_status = service.get("status", "")
            if health_status in ("OK", "healthy", "ok"):
                success_signals += 1
            elif health_status:
                failure_signals += 1

        # Check endpoint
        endpoint = ground_truth.get("endpoint_status", {})
        if endpoint:
            if endpoint.get("healthy", endpoint.get("status") == "up"):
                success_signals += 1
            else:
                failure_signals += 1

        # Check kubernetes
        k8s = ground_truth.get("kubernetes", {})
        if k8s:
            if k8s.get("healthy"):
                success_signals += 1
            else:
                failure_signals += 1

        # Determine actual success from signals
        total_signals = success_signals + failure_signals
        if total_signals == 0:
            actual_success = None
            verdict = "uncertain"
            confidence = 0.3
        elif success_signals > failure_signals:
            actual_success = True
            confidence = min(0.5 + (success_signals / signal_count) * 0.5, 1.0)
            verdict = "confirmed" if reported_success else "false_negative"
        else:
            actual_success = False
            confidence = min(0.5 + (failure_signals / signal_count) * 0.5, 1.0)
            verdict = "false_positive" if reported_success else "confirmed"

        # If confidence is low, try Ollama for nuanced analysis (Gemini R1)
        if confidence < 0.7 and signal_count > 0:
            try:
                analysis_prompt = f"""Analyze this runbook execution validation:

    Reported success: {reported_success}
    Ground truth signals: {json.dumps(ground_truth, indent=2, default=str)}

    Based on the ground truth data, did the execution actually succeed?
    Respond with JSON: {{"actual_success": true/false/null, "confidence": 0.0-1.0, "reasoning": "..."}}"""

                llm_result = await call_gemini(analysis_prompt)
                if llm_result:
                    try:
                        parsed = json.loads(llm_result)
                        llm_success = parsed.get("actual_success")
                        llm_confidence = parsed.get("confidence", 0.5)
                        if llm_confidence > confidence:
                            actual_success = llm_success
                            confidence = llm_confidence
                            if actual_success is True:
                                verdict = "confirmed" if reported_success else "false_negative"
                            elif actual_success is False:
                                verdict = "false_positive" if reported_success else "confirmed"
                            else:
                                verdict = "uncertain"
                            logger.info(f"[VALIDATE] LLM refined verdict: {verdict} (confidence {confidence})")
                    except (json.JSONDecodeError, KeyError):
                        pass
            except Exception as e:
                logger.warning(f"[VALIDATE] LLM fallback failed: {e}")

        logger.info(f"[VALIDATE] Verdict: {verdict}, confidence: {confidence:.2f}, actual_success: {actual_success}")
        return {
            "verdict": verdict,
            "confidence": confidence,
            "actual_success": actual_success,
        }

    async def record_validation(state: ValidationState) -> dict:
        """Node 4: Record validation result back to knowledge-mcp."""
        event_id = state["event_id"]
        verdict = state.get("verdict", "uncertain")
        confidence = state.get("confidence", 0.0)

        validation_payload = {
            "validated": True,
            "validated_at": datetime.utcnow().isoformat(),
            "validated_by": state.get("triggered_by", "langgraph-auto"),
            "verdict": verdict,
            "confidence": confidence,
            "actual_success": state.get("actual_success"),
            "signal_count": state.get("signal_count", 0),
            "ground_truth": state.get("ground_truth", {}),
        }

        # Update event with validation
        result = await call_mcp_tool(KNOWLEDGE_MCP_URL, "update_event", {
            "event_id": event_id,
            "validation": validation_payload,
            "resolution": "validated" if verdict == "confirmed" else "needs_review",
        })

        # If false positive, block runbook from autonomous execution (Gemini R3)
        if verdict == "false_positive":
            event = state.get("event", {})
            runbook_id = event.get("metadata", {}).get("runbook_id")
            if runbook_id:
                logger.warning(f"[VALIDATE] FALSE POSITIVE detected for runbook {runbook_id}, blocking autonomous execution")
                # Log autonomy.blocked event
                await call_mcp_tool(KNOWLEDGE_MCP_URL, "log_event", {
                    "event_type": "autonomy.blocked",
                    "description": f"Runbook {runbook_id} blocked from autonomous execution due to false positive in event {event_id}",
                    "source_agent": "validation-workflow",
                    "metadata": {
                        "runbook_id": runbook_id,
                        "event_id": event_id,
                        "verdict": verdict,
                        "confidence": confidence,
                    },
                })
                # Downgrade runbook to manual
                await call_mcp_tool(KNOWLEDGE_MCP_URL, "update_runbook", {
                    "runbook_id": runbook_id,
                    "automation_level": "manual",
                })

        # Log validation completed event
        await call_mcp_tool(KNOWLEDGE_MCP_URL, "log_event", {
            "event_type": "validation.completed",
            "description": f"Validation of event {event_id}: {verdict} (confidence {confidence:.2f})",
            "source_agent": "validation-workflow",
            "metadata": {
                "event_id": event_id,
                "verdict": verdict,
                "confidence": confidence,
                "triggered_by": state.get("triggered_by", "langgraph-auto"),
            },
        })

        logger.info(f"[VALIDATE] Recorded validation for {event_id}: {verdict}")
        return {"validation_recorded": True}

    def create_validation_workflow():
        """Create the 4-node validation SubGraph."""
        workflow = StateGraph(ValidationState)
        workflow.add_node("fetch_context", fetch_validation_context)
        workflow.add_node("gather_truth", gather_ground_truth)
        workflow.add_node("compute_verdict", compute_verdict)
        workflow.add_node("record_validation", record_validation)
        workflow.set_entry_point("fetch_context")
        workflow.add_edge("fetch_context", "gather_truth")
        workflow.add_edge("gather_truth", "compute_verdict")
        workflow.add_edge("compute_verdict", "record_validation")
        workflow.add_edge("record_validation", END)
        return workflow.compile()

    validation_graph = create_validation_workflow()

    async def _delayed_validation(initial_state: dict, delay_seconds: int = 300):
        """Run validation after a cooling period (Gemini R2)."""
        logger.info(f"[VALIDATE] Scheduled validation for event {initial_state['event_id']} in {delay_seconds}s")
        await asyncio.sleep(delay_seconds)
        try:
            await validation_graph.ainvoke(initial_state)
            logger.info(f"[VALIDATE] Delayed validation completed for {initial_state['event_id']}")
        except Exception as e:
            logger.error(f"[VALIDATE] Delayed validation failed: {e}")

    def create_workflow():
        workflow = StateGraph(AgentState)
        workflow.add_node("assess_alert", assess_alert)
        workflow.add_node("generate_solutions", generate_solutions)
        workflow.add_node("request_approval", request_approval)
        workflow.add_node("execute_solution", execute_solution)
        workflow.add_node("record_outcome", record_outcome)
        workflow.set_entry_point("assess_alert")
        workflow.add_edge("assess_alert", "generate_solutions")
        workflow.add_conditional_edges(
            "generate_solutions",
            should_request_approval,
            {"request_approval": "request_approval", "execute_solution": "execute_solution"}
        )
        workflow.add_edge("request_approval", "record_outcome")
        workflow.add_edge("execute_solution", "record_outcome")
        workflow.add_edge("record_outcome", END)
        return workflow.compile()

    graph = create_workflow()

    async def _process_alert_background(alert_dict: dict, initial_state: dict):
        """Background task to process alert through the graph."""
        try:
            logger.info("Processing alert %s in background", alert_dict["id"])
            result = await graph.ainvoke(initial_state)
            logger.info("Alert %s processed: status=%s", alert_dict["id"], result.get("approval_status", "unknown"))
        except Exception as e:
            logger.error("Background graph execution failed for %s: %s", alert_dict["id"], e)

    @app.post("/alert", status_code=202)
    async def process_alert(alert: AlertInput):
        """Receive alert and process asynchronously. Returns immediately with 202 Accepted."""
        alert_dict = alert.model_dump()
        alert_dict["id"] = alert_dict.get("id") or "alert-" + datetime.utcnow().strftime("%Y%m%d%H%M%S")
        initial_state = {
            "messages": [],
            "alert": alert_dict,
            "assessment": {},
            "solutions": [],
            "selected_solution": {},
            "approval_status": "pending",
            "execution_result": {},
            "runbook_id": "",
            "topic_id": 0,
            "thread_id": alert_dict["id"],
            "runbook_match": {},
            "handled_locally": False
        }
        # Spawn background task - returns immediately
        asyncio.create_task(_process_alert_background(alert_dict, initial_state))
        logger.info("Alert %s accepted for async processing", alert_dict["id"])
        return {
            "alert_id": alert_dict["id"],
            "status": "accepted",
            "message": "Alert queued for processing"
        }

    @app.post("/keep-alert-debug", status_code=200)
    async def keep_alert_debug(request: Request):
        """Debug endpoint to see what Keep sends."""
        body = await request.body()
        query_params = dict(request.query_params)
        content_type = request.headers.get('content-type', 'none')
        logger.info("Keep debug - content-type: %s", content_type)
        logger.info("Keep debug - query params: %s", query_params)
        logger.info("Keep debug - raw body: %s", body.decode('utf-8', errors='replace')[:1000])
        return {"content_type": content_type, "query_params": query_params, "body": body.decode('utf-8', errors='replace')[:500]}

    @app.post("/keep-alert", status_code=202)
    async def process_keep_alert(alert: KeepAlert):
        """Receive alert from Keep and process. Transforms Keep format to internal format."""
        # Transform Keep alert to internal format
        alert_id = alert.fingerprint or alert.id or f"keep-{datetime.utcnow().strftime('%Y%m%d%H%M%S')}"
        namespace = "default"
        if alert.labels:
            namespace = alert.labels.get("namespace", "default")

        alert_dict = {
            "id": alert_id,
            "alertname": alert.name or alert.alertname or "unknown",
            "severity": alert.severity or "warning",
            "namespace": namespace,
            "description": alert.description or alert.message or "",
            "labels": alert.labels or {},
            "annotations": {},
            "source": alert.source,
            "keep_status": alert.status
        }

        initial_state = {
            "messages": [],
            "alert": alert_dict,
            "assessment": {},
            "solutions": [],
            "selected_solution": {},
            "approval_status": "pending",
            "execution_result": {},
            "runbook_id": "",
            "topic_id": 0,
            "thread_id": alert_dict["id"],
            "runbook_match": {},
            "handled_locally": False
        }
        # Spawn background task - returns immediately
        asyncio.create_task(_process_alert_background(alert_dict, initial_state))
        logger.info("Keep alert %s (%s) accepted for async processing", alert_dict["alertname"], alert_dict["id"])
        return {
            "alert_id": alert_dict["id"],
            "status": "accepted",
            "message": "Keep alert queued for processing"
        }

    @app.post("/approve")
    async def approve_action(request: ApprovalRequest):
        if request.alert_id not in pending_approvals:
            raise HTTPException(status_code=404, detail="Approval not found or expired")
        pending = pending_approvals[request.alert_id]
        solutions = pending["solutions"]
        if request.solution_index < 1 or request.solution_index > len(solutions):
            raise HTTPException(status_code=400, detail="Invalid solution index")
        selected = solutions[request.solution_index - 1]
        state = {
            "messages": [],
            "alert": pending["alert"],
            "assessment": pending["assessment"],
            "solutions": solutions,
            "selected_solution": selected,
            "approval_status": "approved",
            "execution_result": {},
            "runbook_id": "",
            "topic_id": 0,
            "thread_id": request.alert_id
        }
        result = await execute_solution(state)
        state.update(result)
        await record_outcome(state)
        del pending_approvals[request.alert_id]
        return {
            "success": result["execution_result"].get("success", False),
            "summary": result["execution_result"].get("summary", ""),
            "runbook_id": "runbook-" + request.alert_id[:8]
        }

    @app.post("/ignore")
    async def ignore_action(request: IgnoreRequest):
        if request.alert_id in pending_approvals:
            pending = pending_approvals[request.alert_id]
            await call_knowledge_mcp("record", "POST", {
                "collection": "decisions",
                "data": {
                    "alert_id": request.alert_id,
                    "alert": pending["alert"],
                    "action": "ignored",
                    "ignored_by": request.ignored_by,
                    "timestamp": datetime.utcnow().isoformat()
                }
            })
            del pending_approvals[request.alert_id]
        return {"status": "ignored"}

    @app.get("/pending/{alert_id}")
    async def get_pending(alert_id: str):
        if alert_id not in pending_approvals:
            return None
        return pending_approvals[alert_id]

    @app.get("/status")
    async def get_status():
        return {"pending_count": len(pending_approvals), "auto_executed": 0, "active_runbooks": 0, "learning_queue": 0}

    # ============================================================================
    # VALIDATION ENDPOINTS - Project 03: Evaluator System
    # ============================================================================

    class ValidateRequest(BaseModel):
        event_id: str
        triggered_by: str = "human"

    class ValidateCandidatesRequest(BaseModel):
        max_candidates: int = 10

    @app.post("/validate", status_code=202)
    async def validate_event(request: ValidateRequest):
        """Trigger validation workflow for a specific event.

        For human-triggered: runs immediately (no cooling period).
        For langgraph-auto: uses 5-minute cooling period.
        """
        initial_state = {
            "event_id": request.event_id,
            "triggered_by": request.triggered_by,
            "event": {},
            "runbook": None,
            "reported_success": False,
            "ground_truth": {},
            "verdict": "pending",
            "confidence": 0.0,
            "actual_success": None,
            "signal_count": 0,
            "validation_recorded": False,
        }

        if request.triggered_by == "langgraph-auto":
            # Delayed validation with 5-minute cooling period (Gemini R2)
            asyncio.create_task(_delayed_validation(initial_state, delay_seconds=300))
            return {"status": "scheduled", "event_id": request.event_id, "delay_seconds": 300}
        else:
            # Immediate validation for human-triggered
            asyncio.create_task(validation_graph.ainvoke(initial_state))
            return {"status": "processing", "event_id": request.event_id}

    @app.post("/validate-candidates", status_code=202)
    async def validate_candidates(request: ValidateCandidatesRequest):
        """Trigger validation for autonomy upgrade candidates.

        Called by pattern-detector to validate candidates before notification.
        """
        # Get autonomy candidates from knowledge-mcp
        candidates = await call_mcp_tool(KNOWLEDGE_MCP_URL, "list_autonomy_candidates", {
            "min_executions": 5,
            "min_success_rate": 0.7,
        })

        if not candidates or isinstance(candidates, dict) and candidates.get("error"):
            return {"status": "no_candidates", "candidates": 0, "validations_queued": 0}

        # Limit candidates
        candidates = candidates[:request.max_candidates] if isinstance(candidates, list) else []

        queued = 0
        for candidate in candidates:
            # Get recent execution events for this runbook
            runbook_id = candidate.get("id")
            if not runbook_id:
                continue

            # Find recent execution events for this runbook
            recent_events = await call_mcp_tool(KNOWLEDGE_MCP_URL, "list_recent_events", {
                "event_type": "runbook.execution",
                "limit": 5,
            })

            if isinstance(recent_events, list):
                for event in recent_events:
                    event_meta = event.get("metadata", {})
                    if event_meta.get("runbook_id") == runbook_id:
                        # Check if already validated
                        if not event.get("validation"):
                            event_id = event.get("id") or event.get("event_id")
                            if event_id:
                                initial_state = {
                                    "event_id": event_id,
                                    "triggered_by": "pattern-detector",
                                    "event": {},
                                    "runbook": None,
                                    "reported_success": event_meta.get("success", False),
                                    "ground_truth": {},
                                    "verdict": "pending",
                                    "confidence": 0.0,
                                    "actual_success": None,
                                    "signal_count": 0,
                                    "validation_recorded": False,
                                }
                                asyncio.create_task(validation_graph.ainvoke(initial_state))
                                queued += 1

        logger.info(f"[VALIDATE] Queued {queued} validations for {len(candidates)} candidates")
        return {
            "status": "processing",
            "candidates": len(candidates),
            "validations_queued": queued,
        }

    class QueryRequest(BaseModel):
        prompt: str
        use_claude: bool = False
        context: Optional[dict] = None
        messages: Optional[List[dict]] = None  # Conversation history [{"role": "user/assistant", "content": "..."}]
        conversation_id: Optional[str] = None  # For tracking conversations

    async def build_query_context(prompt: str) -> dict:
        """Build context from MCPs based on query keywords."""
        context = {}
        errors = []
        prompt_lower = prompt.lower()

        async with httpx.AsyncClient(timeout=15.0, verify=False) as client:
            # UniFi context for network queries
            if any(kw in prompt_lower for kw in ["unifi", "network", "device", "update", "firmware", "ap", "switch", "gateway", "wifi"]):
                try:
                    # Use REST API endpoints (not MCP protocol)
                    resp = await client.get("http://unifi-mcp:8000/api/devices")
                    if resp.status_code == 200:
                        data = resp.json()
                        if data.get("status") == "ok":
                            context["unifi_devices"] = data.get("data", [])
                            logger.info(f"Got {len(context.get('unifi_devices', []))} UniFi devices")
                        else:
                            errors.append(f"UniFi API error: {data.get('error', 'unknown')}")
                    resp = await client.get("http://unifi-mcp:8000/api/health")
                    if resp.status_code == 200:
                        data = resp.json()
                        if data.get("status") == "ok":
                            context["unifi_health"] = data.get("data", {})
                        else:
                            errors.append(f"UniFi health error: {data.get('error', 'unknown')}")
                except Exception as e:
                    logger.warning(f"Failed to get UniFi context: {e}")
                    errors.append(f"UniFi connection failed: {str(e)}")

            # Infrastructure context for cluster queries
            if any(kw in prompt_lower for kw in ["pod", "kubernetes", "k8s", "deployment", "cluster", "container", "issue", "problem"]):
                try:
                    resp = await client.get(INFRASTRUCTURE_MCP_URL + "/api/cluster")
                    if resp.status_code == 200:
                        data = resp.json()
                        if data.get("status") == "ok":
                            context["cluster_state"] = data.get("data", {})
                        else:
                            errors.append(f"Infrastructure API error: {data.get('error', 'unknown')}")
                    else:
                        errors.append(f"Infrastructure MCP returned {resp.status_code} - cluster data not available")
                except Exception as e:
                    logger.warning(f"Failed to get cluster context: {e}")
                    errors.append(f"Failed to connect to infrastructure monitoring: {str(e)}")

            # Entity discovery context from Qdrant
            if any(kw in prompt_lower for kw in ["service", "server", "ip", "host", "device"]):
                try:
                    resp = await client.post(
                        QDRANT_URL + "/collections/entities/points/scroll",
                        json={"limit": 50, "with_payload": True}
                    )
                    if resp.status_code == 200:
                        data = resp.json()
                        points = data.get("result", {}).get("points", [])
                        context["entities"] = [p.get("payload", {}) for p in points]
                except:
                    pass

            # TrueNAS context for storage queries
            if any(kw in prompt_lower for kw in ["storage", "nas", "truenas", "zfs", "pool", "dataset", "disk", "share", "smb", "nfs"]):
                try:
                    resp = await client.get("http://truenas-mcp:8000/api/pools")
                    if resp.status_code == 200:
                        data = resp.json()
                        if data.get("status") == "ok":
                            context["truenas_pools"] = data.get("data", [])
                    resp = await client.get("http://truenas-mcp:8000/api/datasets")
                    if resp.status_code == 200:
                        data = resp.json()
                        if data.get("status") == "ok":
                            context["truenas_datasets"] = data.get("data", [])
                    resp = await client.get("http://truenas-mcp:8000/api/disks")
                    if resp.status_code == 200:
                        data = resp.json()
                        if data.get("status") == "ok":
                            context["truenas_disks"] = data.get("data", [])
                except Exception as e:
                    logger.warning(f"Failed to get TrueNAS context: {e}")
                    errors.append(f"TrueNAS connection failed: {str(e)}")

            # Home Assistant context for smart home queries
            if any(kw in prompt_lower for kw in ["home", "light", "sensor", "climate", "temperature", "humidity", "automation", "smart", "zigbee", "z-wave"]):
                try:
                    resp = await client.get("http://home-assistant-mcp:8000/api/states")
                    if resp.status_code == 200:
                        data = resp.json()
                        if data.get("status") == "ok":
                            context["home_assistant_states"] = data.get("data", [])
                except Exception as e:
                    logger.warning(f"Failed to get Home Assistant context: {e}")
                    errors.append(f"Home Assistant connection failed: {str(e)}")

            # Proxmox context for VM queries
            if any(kw in prompt_lower for kw in ["vm", "virtual", "proxmox", "container", "lxc", "qemu", "hypervisor"]):
                try:
                    resp = await client.get("http://proxmox-mcp:8000/api/vms")
                    if resp.status_code == 200:
                        data = resp.json()
                        if data.get("status") == "ok":
                            context["proxmox_vms"] = data.get("data", [])
                    resp = await client.get("http://proxmox-mcp:8000/api/nodes")
                    if resp.status_code == 200:
                        data = resp.json()
                        if data.get("status") == "ok":
                            context["proxmox_nodes"] = data.get("data", [])
                except Exception as e:
                    logger.warning(f"Failed to get Proxmox context: {e}")
                    errors.append(f"Proxmox connection failed: {str(e)}")

            # OPNsense context for firewall queries
            if any(kw in prompt_lower for kw in ["firewall", "opnsense", "gateway", "route", "dhcp", "dns", "rule", "nat", "vpn"]):
                try:
                    resp = await client.get("http://opnsense-mcp:8000/api/gateway_status")
                    if resp.status_code == 200:
                        data = resp.json()
                        if data.get("status") == "ok":
                            context["opnsense_gateways"] = data.get("data", [])
                    resp = await client.get("http://opnsense-mcp:8000/api/dhcp_leases")
                    if resp.status_code == 200:
                        data = resp.json()
                        if data.get("status") == "ok":
                            context["opnsense_dhcp_leases"] = data.get("data", [])
                except Exception as e:
                    logger.warning(f"Failed to get OPNsense context: {e}")
                    errors.append(f"OPNsense connection failed: {str(e)}")

            # Coroot context for monitoring queries
            if any(kw in prompt_lower for kw in ["metric", "alert", "anomaly", "monitoring", "performance", "latency", "error rate", "health"]):
                try:
                    resp = await client.post(
                        COROOT_MCP_URL + "/mcp",
                        json={"tool": "get_recent_anomalies", "arguments": {"hours": 24}}
                    )
                    if resp.status_code == 200:
                        data = resp.json()
                        if data.get("success"):
                            context["coroot_anomalies"] = data.get("result", [])
                    resp = await client.post(
                        COROOT_MCP_URL + "/mcp",
                        json={"tool": "get_alerts", "arguments": {}}
                    )
                    if resp.status_code == 200:
                        data = resp.json()
                        if data.get("success"):
                            context["coroot_alerts"] = data.get("result", [])
                except Exception as e:
                    logger.warning(f"Failed to get Coroot context: {e}")
                    errors.append(f"Coroot connection failed: {str(e)}")

            # AdGuard context for DNS/ad blocking queries
            if any(kw in prompt_lower for kw in ["adguard", "dns", "blocking", "ad", "filter", "query log"]):
                try:
                    resp = await client.get("http://adguard-mcp:8000/api/stats")
                    if resp.status_code == 200:
                        data = resp.json()
                        if data.get("status") == "ok":
                            context["adguard_stats"] = data.get("data", {})
                except Exception as e:
                    logger.warning(f"Failed to get AdGuard context: {e}")
                    errors.append(f"AdGuard connection failed: {str(e)}")

            # Runbooks from Qdrant for troubleshooting context
            if any(kw in prompt_lower for kw in ["troubleshoot", "fix", "error", "problem", "issue", "runbook", "how to"]):
                try:
                    resp = await client.post(
                        QDRANT_URL + "/collections/runbooks/points/scroll",
                        json={"limit": 10, "with_payload": True}
                    )
                    if resp.status_code == 200:
                        data = resp.json()
                        points = data.get("result", {}).get("points", [])
                        context["available_runbooks"] = [
                            {"name": p.get("payload", {}).get("name"), "description": p.get("payload", {}).get("description")}
                            for p in points
                        ]
                except:
                    pass

            # Web search for research queries (explicitly requested searches)
            if any(kw in prompt_lower for kw in ["search for", "look up", "find online", "research", "what is the latest", "news about"]):
                try:
                    # Extract search query from prompt
                    search_query = prompt  # Use full prompt as search query
                    resp = await client.post(
                        "http://web-search-mcp:8000/mcp",
                        json={"tool": "web_search", "arguments": {"query": search_query, "num_results": 5}}
                    )
                    if resp.status_code == 200:
                        data = resp.json()
                        if data.get("success"):
                            context["web_search_results"] = data.get("result", [])
                except Exception as e:
                    logger.warning(f"Failed to get web search results: {e}")
                    errors.append(f"Web search failed: {str(e)}")

            # Arr Suite context for media queries
            if any(kw in prompt_lower for kw in ["sonarr", "radarr", "media", "movie", "tv show", "download", "arr"]):
                try:
                    resp = await client.get("http://arr-suite-mcp:8000/api/status")
                    if resp.status_code == 200:
                        data = resp.json()
                        if data.get("status") == "ok":
                            context["arr_suite_status"] = data.get("data", {})
                except Exception as e:
                    logger.warning(f"Failed to get Arr Suite context: {e}")
                    errors.append(f"Arr Suite connection failed: {str(e)}")

        # Include errors so Gemini knows when data wasn't available
        if errors:
            context["_data_fetch_errors"] = errors

        return context

    @app.post("/query")
    async def query_llm(request: QueryRequest):
        """
        Direct query endpoint for LLM access.
        Builds context from MCPs before querying Gemini.
        """
        # Build context from MCPs based on query
        mcp_context = await build_query_context(request.prompt)

        # Merge with any provided context
        full_context = {**(request.context or {}), **mcp_context}

        if request.use_claude:
            result = await call_claude_agent(
                request.prompt,
                context=full_context,
                allowed_tools=["Read", "Glob", "Grep", "WebSearch", "WebFetch"],
                priority=PRIORITY_USER
            )
        else:
            # Build context-enriched prompt for Gemini
            context_str = ""
            errors_str = ""
            has_real_data = False

            if full_context:
                # Extract errors if any
                fetch_errors = full_context.pop("_data_fetch_errors", [])
                if fetch_errors:
                    errors_str = "\n\n## DATA FETCH ERRORS - TELL USER ABOUT THESE:\n" + "\n".join(f"- {e}" for e in fetch_errors) + "\n"
                if full_context:
                    has_real_data = True
                    context_str = f"\n\n## ACTUAL DATA FROM HOMELAB:\n{json.dumps(full_context, indent=2, default=str)}\n\n"

            # Build messages list with conversation history
            llm_messages = []

            # Always include comprehensive system prompt with anti-hallucination rules
            system_content = AGENT_SYSTEM_PROMPT + "\n\n## IMPORTANT RULES FOR THIS QUERY:\n"
            if has_real_data:
                system_content += "- Answer based ONLY on the actual data provided below\n"
                system_content += "- Do not invent any device names, IPs, versions, or statuses\n"
            else:
                system_content += "- CRITICAL: Could not retrieve homelab data\n"
                system_content += "- Tell the user what systems failed to respond\n"
                system_content += "- DO NOT make up, guess, or hallucinate any information\n"
            llm_messages.append({"role": "system", "content": system_content})

            # Build user message with context
            if has_real_data:
                enriched_prompt = f"{errors_str}{context_str}USER QUERY: {request.prompt}"
            else:
                enriched_prompt = (
                    f"CRITICAL: I could not retrieve any data from the homelab systems.\n"
                    f"{errors_str}\n"
                    f"USER QUERY: {request.prompt}\n\n"
                    f"Explain what systems failed to respond and suggest troubleshooting steps."
                )

            # Add conversation history if provided
            if request.messages:
                for msg in request.messages[:-1]:  # Exclude last message (current prompt)
                    llm_messages.append(msg)

            # Add current message with context
            current_content = enriched_prompt if not request.messages else f"{context_str}{errors_str}USER: {request.prompt}"
            llm_messages.append({"role": "user", "content": current_content})

            result = await call_litellm(llm_messages)

        return {"response": result, "conversation_id": request.conversation_id}

    @app.post("/claude/run")
    async def run_claude_task(prompt: str, context: Optional[dict] = None):
        """Direct endpoint to run a Claude Agent task with user priority."""
        result = await call_claude_agent(
            prompt,
            context=context,
            priority=PRIORITY_USER  # User requests get highest priority
        )
        return {"response": result}

    @app.post("/runbook/store")
    async def store_runbook_api(runbook_data: dict):
        """Store a diagnostic runbook via API."""
        try:
            runbook = Runbook(**runbook_data)
            success = await store_runbook_qdrant(runbook)
            if success:
                return {"status": "stored", "runbook_id": runbook.id, "name": runbook.name}
            else:
                raise HTTPException(status_code=500, detail="Failed to store runbook")
        except Exception as e:
            logger.error(f"Failed to store runbook: {e}")
            raise HTTPException(status_code=400, detail=str(e))

    @app.get("/runbook/{runbook_id}")
    async def get_runbook(runbook_id: str):
        """Get a runbook by ID."""
        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                response = await client.get(
                    QDRANT_URL + f"/collections/runbooks/points/{runbook_id}"
                )
                if response.status_code == 200:
                    payload = response.json().get("result", {}).get("payload", {})
                    return payload
                return None
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))

    @app.get("/runbooks")
    async def list_runbooks(limit: int = 20):
        """List all runbooks."""
        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                response = await client.post(
                    QDRANT_URL + "/collections/runbooks/points/scroll",
                    json={"limit": limit, "with_payload": True}
                )
                if response.status_code == 200:
                    points = response.json().get("result", {}).get("points", [])
                    return {
                        "runbooks": [
                            {
                                "id": p.get("id"),
                                "name": p.get("payload", {}).get("name", "Unknown"),
                                "description": p.get("payload", {}).get("description", ""),
                                "version": 2 if "diagnosis" in p.get("payload", {}) else 1,
                                "success_count": p.get("payload", {}).get("metadata", {}).get("success_count", 0),
                                "auto_approve": p.get("payload", {}).get("metadata", {}).get("auto_approve_eligible", False)
                            }
                            for p in points
                        ],
                        "count": len(points)
                    }
                return {"runbooks": [], "count": 0}
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))

    @app.delete("/runbook/{runbook_id}")
    async def delete_runbook(runbook_id: str):
        """Delete a runbook by ID."""
        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                response = await client.post(
                    QDRANT_URL + "/collections/runbooks/points/delete",
                    json={"points": [runbook_id]}
                )
                return {"status": "deleted" if response.status_code == 200 else "failed"}
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))

    class SimpleRunbookRequest(BaseModel):
        """Simple runbook creation - Claude provides all details, no LLM needed."""
        name: str = Field(..., description="Runbook name")
        description: str = Field(..., description="What this runbook handles")
        keywords: List[str] = Field(..., description="Keywords for matching alerts")
        diagnosis_command: str = Field(default="echo 'Check manually'", description="Command to verify issue exists")
        fix_commands: List[str] = Field(default_factory=list, description="Commands to fix the issue")
        validation_command: str = Field(default="echo 'OK'", description="Command to verify fix worked")
        escalate_if: List[str] = Field(default_factory=lambda: ["confidence < 0.7"], description="Conditions to escalate")

    @app.post("/runbook/create-simple")
    async def create_simple_runbook(request: SimpleRunbookRequest):
        """
        Create a runbook from Claude-provided details. No LLM call needed.

        Claude (the architect) creates runbooks after fixing issues.
        Gemini (the operator) uses these runbooks to handle future occurrences.
        """
        logger.info(f"Creating runbook: {request.name}")

        try:
            runbook = Runbook(
                name=request.name,
                description=request.description,
                triggers=TriggerConfig(
                    alert_patterns=[],
                    keywords=request.keywords
                ),
                diagnosis=DiagnosisConfig(
                    checks=[DiagnosisCheck(name="Verify issue", command=request.diagnosis_command)],
                    confidence_rules=[]
                ),
                decision=DecisionRules(
                    escalate_if=request.escalate_if,
                    handle_locally_if=["confidence >= 0.8"]
                ),
                fix=FixConfig(
                    steps=[
                        FixStep(name=f"Step {i+1}", action="command", command=cmd)
                        for i, cmd in enumerate(request.fix_commands)
                    ] if request.fix_commands else [
                        FixStep(name="Manual fix", action="command", command="echo 'Apply fix manually'")
                    ],
                    validation=[ValidationCheck(name="Verify fix", command=request.validation_command)]
                ),
                metadata=RunbookMetadata(
                    created_by="claude-code",
                    created_from="fix-session",
                    auto_approve_eligible=False
                )
            )

            success = await store_runbook_qdrant(runbook)

            if success:
                logger.info(f"Created runbook: {runbook.name} ({runbook.id})")
                return {
                    "status": "created",
                    "runbook_id": runbook.id,
                    "name": runbook.name,
                    "description": runbook.description,
                    "keywords": request.keywords
                }
            else:
                raise HTTPException(status_code=500, detail="Failed to store runbook")

        except Exception as e:
            logger.error(f"Failed to create runbook: {e}")
            raise HTTPException(status_code=500, detail=str(e))

    @app.get("/health")
    async def health():
        return {"status": "healthy"}

    def main():
        port = int(os.environ.get("PORT", "8000"))
        logger.info("Starting LangGraph orchestrator on port %d", port)
        uvicorn.run(app, host="0.0.0.0", port=port)

    if __name__ == "__main__":
        main()

  requirements.txt: |
    langgraph>=0.2.0
    langchain-core>=0.3.0
    langchain-openai>=0.2.0
    asyncpg>=0.30.0
    psycopg>=3.2.0
    psycopg-pool>=3.2.0
    redis>=5.2.0
    fastapi>=0.115.0
    uvicorn>=0.34.0
    httpx>=0.28.0
    pydantic>=2.11.0
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: langgraph
  namespace: ai-platform
  labels:
    app: langgraph
    component: orchestrator
spec:
  replicas: 1
  selector:
    matchLabels:
      app: langgraph
  template:
    metadata:
      labels:
        app: langgraph
        component: orchestrator
    spec:
      initContainers:
        - name: install-deps
          image: python:3.11-slim
          command: ['sh', '-c', 'pip install --target=/app/deps -r /code/requirements.txt']
          volumeMounts:
            - name: code
              mountPath: /code
            - name: deps
              mountPath: /app/deps
      containers:
        - name: langgraph
          image: python:3.11-slim
          command: ['sh', '-c', 'cd /app && PYTHONPATH=/app/deps python /code/main.py']
          ports:
            - containerPort: 8000
              name: http
          env:
            - name: PORT
              value: "8000"
            # Core services
            - name: POSTGRES_URL
              value: "postgresql://postgres:postgres@postgres:5432/langgraph"
            - name: REDIS_URL
              value: "redis://redis:6379"
            - name: LITELLM_URL
              value: "http://litellm:4000"
            - name: QDRANT_URL
              value: "http://qdrant:6333"
            # Claude services (validator runs separately as daily cronjob)
            - name: CLAUDE_AGENT_URL
              value: "http://claude-agent:8000"
            # Matrix bot (replaces Telegram)
            - name: MATRIX_BOT_URL
              value: "http://matrix-bot:8000"
            # Model configuration - Gemini only
            - name: GEMINI_MODEL
              value: "gemini/gemini-2.0-pro"
            - name: GEMINI_FLASH_MODEL
              value: "gemini/gemini-2.0-flash"
            - name: EMBEDDING_MODEL
              value: "embeddings"
            # Tiered Runbook Matching Thresholds
            # >= EXACT: Execute as-is
            # >= SIMILAR (but < EXACT): Execute with tweaks, propose runbook update
            # < SIMILAR: Escalate to Brain Trust (humans via Outline)
            - name: RUNBOOK_EXACT_THRESHOLD
              value: "0.95"
            - name: RUNBOOK_SIMILAR_THRESHOLD
              value: "0.80"
            # Legacy threshold (backward compatibility, equals SIMILAR)
            - name: RUNBOOK_MATCH_THRESHOLD
              value: "0.80"
            - name: CONTEXT_CACHE_TTL
              value: "3600"
            # Brain Trust Escalation (Outline wiki)
            - name: BRAIN_TRUST_COLLECTION_ID
              value: "9bd8c738-4505-494a-a867-74349a887dc6"
            - name: OUTLINE_MCP_URL
              value: "http://outline-mcp:8000"
          # Import ALL MCP server URLs from shared ConfigMap
          # Source: /home/.mcp.json via sync-mcp-config.sh
          envFrom:
            - configMapRef:
                name: mcp-servers-config
          volumeMounts:
            - name: code
              mountPath: /code
            - name: deps
              mountPath: /app/deps
          resources:
            requests:
              memory: "256Mi"
              cpu: "200m"
            limits:
              memory: "512Mi"
              cpu: "1000m"
          readinessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 60
            periodSeconds: 10
            timeoutSeconds: 5
          livenessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 90
            periodSeconds: 30
            timeoutSeconds: 5
      volumes:
        - name: code
          configMap:
            name: langgraph-code
        - name: deps
          emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: langgraph
  namespace: ai-platform
  labels:
    app: langgraph
    component: orchestrator
spec:
  selector:
    app: langgraph
  ports:
    - port: 8000
      targetPort: 8000
      name: http
