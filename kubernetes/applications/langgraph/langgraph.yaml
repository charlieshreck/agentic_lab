apiVersion: v1
kind: ConfigMap
metadata:
  name: langgraph-code
  namespace: ai-platform
  labels:
    app: langgraph
data:
  main.py: |
    #!/usr/bin/env python3
    """LangGraph Orchestrator for Agentic AI Platform with Diagnostic Runbooks."""
    import os
    import logging
    import json
    import re
    import uuid as uuid_mod
    from typing import TypedDict, Annotated, Sequence, Optional, List, Dict, Any
    from datetime import datetime, timedelta
    import httpx
    import asyncio

    from langgraph.graph import StateGraph, END
    from langchain_core.messages import BaseMessage

    from fastapi import FastAPI, HTTPException, Request
    from pydantic import BaseModel, Field
    import uvicorn

    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

    # Environment configuration
    # Core service URLs
    POSTGRES_URL = os.environ.get("POSTGRES_URL", "postgresql://postgres:postgres@postgres:5432/langgraph")
    REDIS_URL = os.environ.get("REDIS_URL", "redis://redis:6379")
    LITELLM_URL = os.environ.get("LITELLM_URL", "http://litellm:4000")
    QDRANT_URL = os.environ.get("QDRANT_URL", "http://qdrant:6333")

    # Claude services - agent for escalation (validator runs separately as daily cronjob)
    CLAUDE_AGENT_URL = os.environ.get("CLAUDE_AGENT_URL", "http://claude-agent:8000")

    # MCP servers
    KNOWLEDGE_MCP_URL = os.environ.get("KNOWLEDGE_MCP_URL", "http://knowledge-mcp:8000")
    INFRASTRUCTURE_MCP_URL = os.environ.get("INFRASTRUCTURE_MCP_URL", "http://infrastructure-mcp:8000")
    COROOT_MCP_URL = os.environ.get("COROOT_MCP_URL", "http://coroot-mcp:8000")

    # Matrix bot for notifications (replaces Telegram)
    MATRIX_BOT_URL = os.environ.get("MATRIX_BOT_URL", "http://matrix-bot:8000")

    # Model configuration - Gemini only (via LiteLLM)
    # Using 2.0-flash for everything - best free tier limits (1500 RPD, 10 RPM)
    GEMINI_MODEL = os.environ.get("GEMINI_MODEL", "gemini/gemini-2.0-flash")
    GEMINI_FLASH_MODEL = os.environ.get("GEMINI_FLASH_MODEL", "gemini/gemini-2.0-flash")
    EMBEDDING_MODEL = os.environ.get("EMBEDDING_MODEL", "embeddings")

    # Thresholds and configuration
    RUNBOOK_MATCH_THRESHOLD = float(os.environ.get("RUNBOOK_MATCH_THRESHOLD", "0.75"))
    MIN_CONFIDENCE_FOR_LOCAL_FIX = float(os.environ.get("MIN_CONFIDENCE_FOR_LOCAL_FIX", "0.7"))
    AUTO_APPROVE_MIN_SUCCESSES = int(os.environ.get("AUTO_APPROVE_MIN_SUCCESSES", "5"))
    CONTEXT_CACHE_TTL = int(os.environ.get("CONTEXT_CACHE_TTL", "3600"))

    # Static context cache (refreshed hourly for cost savings)
    _static_context_cache = {}
    _static_context_timestamp = None

    # ============================================================================
    # SYSTEM PROMPT - Injected into all LLM calls
    # ============================================================================

    AGENT_SYSTEM_PROMPT = """
    You are the AI assistant for Charlie's Agentic Homelab Platform.

    Your Capabilities:

    You have access to real-time data from these MCP servers (data is fetched automatically based on your query):

    Infrastructure & Network:
    - unifi-mcp - WiFi clients, access points, switches, network health
    - opnsense-mcp - Firewall rules, DHCP leases, gateway status, VPN
    - proxmox-mcp - VMs, LXCs, hypervisor nodes, resource usage
    - truenas-mcp - Storage pools, datasets, ZFS health, shares
    - infrastructure-mcp - Kubernetes cluster state, pods, deployments

    Monitoring & Observability:
    - coroot-mcp - Service metrics, anomaly detection, dependencies
    - adguard-mcp - DNS stats, query logs, blocking rates

    Smart Home & Media:
    - home-assistant-mcp - Lights, sensors, climate, automations
    - arr-suite-mcp - Sonarr, Radarr, media library status

    Knowledge & Search:
    - knowledge-mcp - Qdrant vector DB with runbooks, documentation, network entities
    - web-search-mcp - Internet search via SearXNG

    Qdrant Collections Available:
    - runbooks - Operational procedures for common issues
    - documentation - Architecture docs and guides
    - entities - Every device on the network (searchable by IP, MAC, hostname, type)
    - decisions - Historical decisions and outcomes
    - device_types - How to control different device types (APIs, protocols)

    Guidelines:
    - Answer based on ACTUAL DATA provided - never invent device names, IPs, or statuses
    - If data retrieval fails, tell the user what failed and suggest troubleshooting
    - Be concise but thorough - include relevant metrics and specifics
    - When suggesting fixes, provide confidence level and reasoning
    - For complex issues, recommend escalating to Claude Code session

    Network Overview:
    - 10.10.0.0/24 - Production network (Proxmox, K8s prod cluster)
    - 10.20.0.0/24 - Agentic platform (this AI system)
    - 10.30.0.0/24 - Monitoring cluster (Grafana, Prometheus, Coroot)
    - 10.40.0.0/24 - Storage network (TrueNAS NFS)
    """

    # ============================================================================
    # RUNBOOK SCHEMA - Pydantic Models for Diagnostic Workflows
    # ============================================================================

    class ExtractField(BaseModel):
        """Extract a value from command output."""
        field: str = Field(..., description="JSONPath or regex pattern to extract")
        expected: Optional[Any] = Field(None, description="Expected value for confidence")
        store_as: str = Field(..., description="Variable name to store result")

    class LookForPattern(BaseModel):
        """Pattern to look for in command output."""
        pattern: str = Field(..., description="Regex pattern to search for")
        confirms: Optional[str] = Field(None, description="What finding this confirms")
        indicates: Optional[str] = Field(None, description="What finding indicates")

    class DiagnosisCheck(BaseModel):
        """A diagnostic check to verify the error."""
        name: str
        command: str = Field(..., description="Command with {{variable}} placeholders")
        extract: Optional[List[ExtractField]] = None
        look_for: Optional[List[LookForPattern]] = None
        store_as: Optional[str] = Field(None, description="Store raw output as variable")
        timeout: int = 30

    class ConfidenceRule(BaseModel):
        """Rule for calculating diagnostic confidence."""
        condition: str = Field(..., description="Python expression using stored variables")
        confidence: float = Field(..., ge=0, le=1)
        diagnosis: str
        note: Optional[str] = None

    class GatherCommand(BaseModel):
        """Command to gather additional context."""
        name: str
        command: str
        store_as: str
        on_failure: str = "continue"
        timeout: int = 30

    class Precondition(BaseModel):
        """Precondition before applying fix."""
        check: str = Field(..., description="Python expression that must be true")
        reason: str

    class FixStep(BaseModel):
        """A step in the fix process."""
        name: str
        action: str = Field(..., description="command, compute, or wait")
        command: Optional[str] = None
        formula: Optional[str] = None
        store_as: Optional[str] = None
        rollback: Optional[str] = None
        timeout: int = 60
        on_failure: str = "abort"
        max_value: Optional[str] = None

    class ValidationCheck(BaseModel):
        """Validation after fix is applied."""
        name: str
        command: str
        expected: Optional[str] = None
        should_not_contain: Optional[str] = None
        wait: int = 0
        retries: int = 1

    class DecisionRules(BaseModel):
        """Rules for deciding fix locally vs escalate."""
        escalate_if: List[str] = Field(default_factory=list)
        handle_locally_if: List[str] = Field(default_factory=list)

    class TriggerConfig(BaseModel):
        """What triggers this runbook."""
        alert_patterns: List[str] = Field(default_factory=list)
        keywords: List[str] = Field(default_factory=list)
        severity: List[str] = Field(default_factory=lambda: ["warning", "critical"])

    class DiagnosisConfig(BaseModel):
        """Diagnosis phase configuration."""
        description: str = ""
        checks: List[DiagnosisCheck] = Field(default_factory=list)
        confidence_rules: List[ConfidenceRule] = Field(default_factory=list)

    class InfoGatherConfig(BaseModel):
        """Information gathering phase."""
        description: str = ""
        commands: List[GatherCommand] = Field(default_factory=list)

    class FixConfig(BaseModel):
        """Fix phase configuration."""
        description: str = ""
        preconditions: List[Precondition] = Field(default_factory=list)
        steps: List[FixStep] = Field(default_factory=list)
        validation: List[ValidationCheck] = Field(default_factory=list)

    class RunbookMetadata(BaseModel):
        """Runbook metadata for learning."""
        created_by: str = "system"
        created_at: str = Field(default_factory=lambda: datetime.utcnow().isoformat())
        updated_at: Optional[str] = None
        success_count: int = 0
        failure_count: int = 0
        false_positive_count: int = 0
        escalation_count: int = 0
        avg_resolution_time: Optional[float] = None
        auto_approve_eligible: bool = False
        tags: List[str] = Field(default_factory=list)
        related_runbooks: List[str] = Field(default_factory=list)

    class Runbook(BaseModel):
        """Complete diagnostic runbook."""
        id: str = Field(default_factory=lambda: f"rb-{uuid_mod.uuid4().hex[:8]}")
        name: str
        version: int = 1
        description: str = ""
        triggers: TriggerConfig = Field(default_factory=TriggerConfig)
        diagnosis: DiagnosisConfig = Field(default_factory=DiagnosisConfig)
        information_gathering: InfoGatherConfig = Field(default_factory=InfoGatherConfig)
        decision: DecisionRules = Field(default_factory=DecisionRules)
        fix: FixConfig = Field(default_factory=FixConfig)
        escalation_template: str = ""
        metadata: RunbookMetadata = Field(default_factory=RunbookMetadata)

    # ============================================================================
    # RUNBOOK EXECUTOR - Executes diagnostic workflows
    # ============================================================================

    class RunbookExecutor:
        """Executes runbook diagnostic workflows."""

        def __init__(self, runbook: Runbook, alert: dict):
            self.runbook = runbook
            self.alert = alert
            self.variables: Dict[str, Any] = {}
            self.diagnosis_result: str = "unknown"
            self.confidence: float = 0.0
            self.escalation_reason: Optional[str] = None
            self.execution_log: List[str] = []
            self.rollback_stack: List[str] = []

            # Initialize variables from alert
            self.variables["alertname"] = alert.get("alertname", "")
            self.variables["severity"] = alert.get("severity", "warning")
            self.variables["description"] = alert.get("description", "")
            self.variables["namespace"] = alert.get("namespace", "default")
            self.variables["pod_name"] = self._extract_pod_name(alert)
            self.variables["node_name"] = (alert.get("labels") or {}).get("node", "")

        def _extract_pod_name(self, alert: dict) -> str:
            """Extract pod name from alert."""
            labels = alert.get("labels") or {}
            if "pod" in labels:
                return labels["pod"]
            desc = alert.get("description", "")
            match = re.search(r'pod[:\s]+([a-z0-9-]+)', desc, re.I)
            return match.group(1) if match else ""

        def _substitute_variables(self, text: str) -> str:
            """Replace {{variable}} placeholders with values."""
            def replacer(match):
                var_name = match.group(1)
                return str(self.variables.get(var_name, f"{{unknown:{var_name}}}"))
            return re.sub(r'\{\{(\w+)\}\}', replacer, text)

        async def _execute_command(self, command: str, timeout: int = 30) -> tuple:
            """Execute a command via infrastructure MCP and return (success, output)."""
            cmd = self._substitute_variables(command)
            self.execution_log.append(f"EXEC: {cmd}")

            async with httpx.AsyncClient(timeout=float(timeout + 10)) as client:
                try:
                    response = await client.post(
                        INFRASTRUCTURE_MCP_URL + "/execute",
                        json={"command": cmd, "timeout": timeout}
                    )
                    if response.status_code == 200:
                        result = response.json()
                        output = result.get("output", result.get("stdout", ""))
                        success = result.get("success", result.get("exit_code", 1) == 0)
                        self.execution_log.append(f"  -> {'OK' if success else 'FAIL'}: {output[:200]}")
                        return success, output
                    # Command execution not available - escalate
                    self.execution_log.append(f"  -> INFRA MCP unavailable ({response.status_code})")
                    return False, f"Command execution unavailable (HTTP {response.status_code})"
                except Exception as e:
                    self.execution_log.append(f"  -> ERROR: {e}")
                    return False, str(e)

        def _extract_jsonpath(self, data: str, path: str) -> Any:
            """Extract value using simplified JSONPath."""
            try:
                obj = json.loads(data)
                parts = path.replace("[", ".").replace("]", "").split(".")
                for part in parts:
                    if not part:
                        continue
                    if part.isdigit():
                        obj = obj[int(part)]
                    else:
                        obj = obj.get(part, None)
                    if obj is None:
                        return None
                return obj
            except:
                return None

        def _check_pattern(self, text: str, pattern: str) -> bool:
            """Check if pattern exists in text."""
            try:
                return bool(re.search(pattern, text, re.IGNORECASE | re.MULTILINE))
            except:
                return pattern.lower() in text.lower()

        def _evaluate_condition(self, condition: str) -> bool:
            """Evaluate a condition using stored variables."""
            try:
                # Create safe evaluation context - convert None to empty string for string checks
                safe_vars = {k: (v if v is not None else "") for k, v in self.variables.items()}
                safe_vars["true"] = True
                safe_vars["false"] = False
                # Handle common patterns
                cond = condition
                # Handle 'literal' in variable pattern (e.g., 'OOMKill' in recent_events)
                # Substitute the actual variable value at substitution time
                cond = re.sub(r"'([^']+)'\s+in\s+(\w+)",
                    lambda m: f"'{m.group(1)}' in '{str(self.variables.get(m.group(2), '') or '').replace(chr(39), chr(92)+chr(39))}'", cond)
                cond = re.sub(r"(\w+)\s+in\s+\[([^\]]+)\]",
                    lambda m: f"'{self.variables.get(m.group(1), '')}' in [{m.group(2)}]", cond)
                cond = re.sub(r"(\w+)\s*==\s*'([^']+)'",
                    lambda m: f"'{self.variables.get(m.group(1), '')}' == '{m.group(2)}'", cond)
                cond = re.sub(r"(\w+)\s*!=\s*'([^']+)'",
                    lambda m: f"'{self.variables.get(m.group(1), '')}' != '{m.group(2)}'", cond)
                cond = re.sub(r"(\w+)\s*([<>]=?)\s*(\d+)",
                    lambda m: f"{self.variables.get(m.group(1), 0) or 0} {m.group(2)} {m.group(3)}", cond)
                cond = cond.replace(" AND ", " and ").replace(" OR ", " or ")
                return eval(cond, {"__builtins__": {}}, safe_vars)
            except Exception as e:
                logger.warning(f"Condition eval failed: {condition} - {e}")
                return False

        async def run_diagnosis(self) -> dict:
            """Run diagnostic checks to verify the error."""
            logger.info(f"Running diagnosis for {self.runbook.name}")
            results = {"checks": [], "confirmations": set(), "indications": set()}

            for check in self.runbook.diagnosis.checks:
                logger.info(f"  Check: {check.name}")
                success, output = await self._execute_command(check.command, check.timeout)

                check_result = {"name": check.name, "success": success, "output": output[:500]}

                if check.store_as:
                    self.variables[check.store_as] = output

                if check.extract:
                    for ext in check.extract:
                        value = self._extract_jsonpath(output, ext.field)
                        self.variables[ext.store_as] = value
                        check_result[ext.store_as] = value
                        if ext.expected is not None:
                            matches = str(value) == str(ext.expected)
                            check_result[f"{ext.store_as}_matches"] = matches

                if check.look_for:
                    for lf in check.look_for:
                        found = self._check_pattern(output, lf.pattern)
                        if found:
                            if lf.confirms:
                                results["confirmations"].add(lf.confirms)
                            if lf.indicates:
                                results["indications"].add(lf.indicates)

                results["checks"].append(check_result)

            # Convert sets to lists for JSON serialization
            results["confirmations"] = list(results["confirmations"])
            results["indications"] = list(results["indications"])
            return results

        def calculate_confidence(self) -> tuple:
            """Calculate confidence based on diagnosis results."""
            for rule in self.runbook.diagnosis.confidence_rules:
                if self._evaluate_condition(rule.condition):
                    self.confidence = rule.confidence
                    self.diagnosis_result = rule.diagnosis
                    logger.info(f"Confidence: {self.confidence} ({self.diagnosis_result})")
                    if rule.note:
                        self.execution_log.append(f"NOTE: {rule.note}")
                    return self.confidence, self.diagnosis_result

            # Default if no rules match
            self.confidence = 0.3
            self.diagnosis_result = "unclear"
            return self.confidence, self.diagnosis_result

        async def gather_information(self) -> dict:
            """Gather additional context information."""
            logger.info("Gathering additional information...")
            gathered = {}

            for cmd in self.runbook.information_gathering.commands:
                logger.info(f"  Gather: {cmd.name}")
                success, output = await self._execute_command(cmd.command, cmd.timeout)

                if success or cmd.on_failure == "continue":
                    self.variables[cmd.store_as] = output
                    gathered[cmd.store_as] = output[:1000]
                else:
                    gathered[cmd.store_as] = f"FAILED: {output}"

            return gathered

        def should_escalate(self) -> tuple:
            """Determine if we should escalate to Claude."""
            # Check escalation rules
            for rule in self.runbook.decision.escalate_if:
                if self._evaluate_condition(rule):
                    self.escalation_reason = rule
                    logger.info(f"Escalating due to: {rule}")
                    return True, rule

            # Check if confidence is too low
            if self.confidence < MIN_CONFIDENCE_FOR_LOCAL_FIX:
                self.escalation_reason = f"confidence ({self.confidence}) < threshold ({MIN_CONFIDENCE_FOR_LOCAL_FIX})"
                return True, self.escalation_reason

            # Check handle locally rules
            for rule in self.runbook.decision.handle_locally_if:
                if self._evaluate_condition(rule):
                    logger.info(f"Handling locally: {rule}")
                    return False, None

            # Default: escalate if uncertain
            return True, "no matching local handling rule"

        def check_preconditions(self) -> tuple:
            """Check if preconditions for fix are met."""
            for pre in self.runbook.fix.preconditions:
                if not self._evaluate_condition(pre.check):
                    return False, pre.reason
            return True, None

        async def run_fix(self) -> dict:
            """Execute the fix steps."""
            logger.info("Executing fix steps...")
            results = {"steps": [], "success": True}

            for step in self.runbook.fix.steps:
                logger.info(f"  Step: {step.name}")
                step_result = {"name": step.name, "action": step.action}

                if step.action == "command":
                    success, output = await self._execute_command(step.command, step.timeout)
                    step_result["success"] = success
                    step_result["output"] = output[:500]

                    if step.rollback:
                        self.rollback_stack.append(step.rollback)

                    if not success and step.on_failure == "abort":
                        results["success"] = False
                        results["error"] = f"Step '{step.name}' failed: {output}"
                        results["steps"].append(step_result)
                        break

                elif step.action == "compute":
                    try:
                        value = eval(self._substitute_variables(step.formula),
                                   {"__builtins__": {}}, self.variables)
                        if step.max_value:
                            max_val = self._parse_resource(step.max_value)
                            if self._parse_resource(str(value)) > max_val:
                                value = step.max_value
                        if step.store_as:
                            self.variables[step.store_as] = value
                        step_result["success"] = True
                        step_result["value"] = value
                    except Exception as e:
                        step_result["success"] = False
                        step_result["error"] = str(e)

                elif step.action == "wait":
                    await asyncio.sleep(step.timeout)
                    step_result["success"] = True

                results["steps"].append(step_result)

            return results

        def _parse_resource(self, value: str) -> int:
            """Parse resource string like '4Gi' to bytes."""
            value = str(value).strip()
            units = {"Ki": 1024, "Mi": 1024**2, "Gi": 1024**3, "Ti": 1024**4}
            for suffix, mult in units.items():
                if value.endswith(suffix):
                    return int(float(value[:-2]) * mult)
            return int(value)

        async def run_validation(self) -> dict:
            """Validate the fix worked."""
            logger.info("Running validation...")
            results = {"checks": [], "success": True}

            for val in self.runbook.fix.validation:
                logger.info(f"  Validate: {val.name}")

                if val.wait:
                    await asyncio.sleep(val.wait)

                for attempt in range(val.retries):
                    success, output = await self._execute_command(val.command, 30)
                    passed = success

                    if val.expected:
                        expected = self._substitute_variables(val.expected)
                        passed = passed and (expected in output or output.strip() == expected)

                    if val.should_not_contain:
                        forbidden = self._substitute_variables(val.should_not_contain)
                        passed = passed and (forbidden not in output)

                    if passed:
                        break

                    if attempt < val.retries - 1:
                        await asyncio.sleep(5)

                results["checks"].append({
                    "name": val.name,
                    "success": passed,
                    "output": output[:200]
                })

                if not passed:
                    results["success"] = False

            return results

        async def run_rollback(self) -> dict:
            """Execute rollback commands in reverse order."""
            logger.info("Running rollback...")
            results = []

            for cmd in reversed(self.rollback_stack):
                success, output = await self._execute_command(cmd, 60)
                results.append({"command": cmd, "success": success})

            return {"rollback_results": results}

        def format_escalation(self) -> str:
            """Format escalation message for Claude."""
            template = self.runbook.escalation_template or self._default_escalation_template()
            return self._substitute_variables(template)

        def _default_escalation_template(self) -> str:
            """Default escalation template if none provided."""
            return """## Alert Escalation: {{alertname}}

    ### Original Alert
    - Name: {{alertname}}
    - Severity: {{severity}}
    - Description: {{description}}
    - Namespace: {{namespace}}

    ### Diagnosis Attempted
    - Runbook: """ + self.runbook.name + """
    - Confidence: """ + str(self.confidence) + """
    - Diagnosis: """ + self.diagnosis_result + """
    - Escalation Reason: """ + str(self.escalation_reason) + """

    ### Gathered Information
    """ + "\n".join([f"**{k}:**\n```\n{str(v)[:500]}\n```" for k, v in self.variables.items() if k not in ["alertname", "severity", "description", "namespace"]]) + """

    ### Execution Log
    ```
    """ + "\n".join(self.execution_log[-20:]) + """
    ```

    ### Request
    Please analyze this issue and provide:
    1. Root cause analysis
    2. Recommended fix with commands
    3. A runbook definition for future occurrences

    Return the runbook as JSON at the end with this structure:
    ```json
    {
      "runbook_name": "...",
      "runbook_description": "...",
      "triggers": {"alert_patterns": [...], "keywords": [...]},
      "diagnosis_checks": [{"name": "...", "command": "...", "look_for": [...]}],
      "fix_steps": [{"name": "...", "command": "..."}],
      "validation": [{"name": "...", "command": "...", "expected": "..."}]
    }
    ```"""

        async def execute(self) -> dict:
            """Main execution entry point."""
            start_time = datetime.utcnow()
            result = {
                "runbook_id": self.runbook.id,
                "runbook_name": self.runbook.name,
                "alert": self.alert,
                "started_at": start_time.isoformat()
            }

            try:
                # Phase 1: Diagnosis
                diag_results = await self.run_diagnosis()
                result["diagnosis"] = diag_results
                self.calculate_confidence()
                result["confidence"] = self.confidence
                result["diagnosis_result"] = self.diagnosis_result

                # Phase 2: Gather information (always)
                gathered = await self.gather_information()
                result["gathered_info"] = gathered

                # Phase 3: Decision
                should_esc, reason = self.should_escalate()
                result["escalated"] = should_esc

                if should_esc:
                    result["escalation_reason"] = reason
                    result["escalation_message"] = self.format_escalation()
                    result["action"] = "escalate"
                    return result

                # Phase 4: Check preconditions
                pre_ok, pre_reason = self.check_preconditions()
                if not pre_ok:
                    result["action"] = "escalate"
                    result["escalation_reason"] = f"Precondition failed: {pre_reason}"
                    result["escalation_message"] = self.format_escalation()
                    return result

                # Phase 5: Execute fix (requires approval first)
                result["action"] = "fix_ready"
                result["fix_steps"] = [s.model_dump() for s in self.runbook.fix.steps]
                result["requires_approval"] = not self.runbook.metadata.auto_approve_eligible

                return result

            except Exception as e:
                logger.error(f"Runbook execution error: {e}")
                result["action"] = "error"
                result["error"] = str(e)
                return result

        async def execute_approved_fix(self) -> dict:
            """Execute the fix after approval."""
            result = {"runbook_id": self.runbook.id}

            try:
                fix_result = await self.run_fix()
                result["fix_result"] = fix_result

                if fix_result["success"]:
                    val_result = await self.run_validation()
                    result["validation"] = val_result

                    if val_result["success"]:
                        result["action"] = "completed"
                        result["success"] = True
                    else:
                        # Validation failed, rollback
                        rollback_result = await self.run_rollback()
                        result["rollback"] = rollback_result
                        result["action"] = "rolled_back"
                        result["success"] = False
                else:
                    # Fix failed, rollback
                    rollback_result = await self.run_rollback()
                    result["rollback"] = rollback_result
                    result["action"] = "rolled_back"
                    result["success"] = False

                return result

            except Exception as e:
                result["action"] = "error"
                result["error"] = str(e)
                result["success"] = False
                return result

    class AgentState(TypedDict):
        messages: Annotated[Sequence[BaseMessage], lambda x, y: x + y]
        alert: dict
        assessment: dict
        solutions: list
        selected_solution: dict
        approval_status: str
        execution_result: dict
        runbook_id: str
        topic_id: int
        thread_id: str
        runbook_match: dict  # Matched runbook from Qdrant if found
        handled_locally: bool  # Whether issue was handled with existing runbook

    pending_approvals = {}
    app = FastAPI(title="LangGraph Orchestrator")

    class AlertInput(BaseModel):
        id: str
        alertname: str
        severity: str = "warning"
        namespace: str = "default"
        description: str = ""
        labels: Optional[dict] = None
        annotations: Optional[dict] = None

    class KeepAlert(BaseModel):
        """Keep alert format - maps to AlertInput internally."""
        fingerprint: Optional[str] = None
        id: Optional[str] = None
        name: Optional[str] = None
        alertname: Optional[str] = None  # Alternative field name
        status: str = "firing"
        severity: str = "warning"
        lastReceived: Optional[str] = None
        description: Optional[str] = None
        message: Optional[str] = None
        labels: Optional[dict] = None
        source: Optional[list] = None
        # Accept any additional fields Keep might send
        class Config:
            extra = "allow"

    class ApprovalRequest(BaseModel):
        alert_id: str
        solution_index: int
        approved_by: str

    class IgnoreRequest(BaseModel):
        alert_id: str
        ignored_by: str

    async def call_gemini(prompt: str, context: dict = None, model: str = None) -> str:
        """
        Call Gemini via LiteLLM for operational tasks.
        Uses Gemini Pro by default (1M token context window).
        """
        model = model or GEMINI_MODEL
        messages = []

        # Always include comprehensive system prompt
        system_content = AGENT_SYSTEM_PROMPT
        if context:
            system_content += "\n\n## CURRENT CONTEXT:\n" + json.dumps(context, indent=2, default=str)
        messages.append({"role": "system", "content": system_content})

        messages.append({"role": "user", "content": prompt})

        async with httpx.AsyncClient(timeout=120.0) as client:
            try:
                response = await client.post(
                    LITELLM_URL + "/v1/chat/completions",
                    json={"model": model, "messages": messages}
                )
                response.raise_for_status()
                return response.json()["choices"][0]["message"]["content"]
            except Exception as e:
                logger.error("Gemini call failed: %s", e)
                return "Error: " + str(e)

    # Alias for backward compatibility
    async def call_litellm(messages, model=None):
        model = model or GEMINI_MODEL
        async with httpx.AsyncClient(timeout=120.0) as client:
            try:
                response = await client.post(
                    LITELLM_URL + "/v1/chat/completions",
                    json={"model": model, "messages": messages}
                )
                response.raise_for_status()
                return response.json()["choices"][0]["message"]["content"]
            except Exception as e:
                logger.error("LiteLLM call failed: %s", e)
                return "Error: " + str(e)

    async def call_knowledge_mcp(endpoint, method="GET", data=None):
        async with httpx.AsyncClient(timeout=30.0) as client:
            try:
                url = KNOWLEDGE_MCP_URL + "/" + endpoint
                if method == "GET":
                    response = await client.get(url)
                else:
                    response = await client.post(url, json=data)
                response.raise_for_status()
                return response.json()
            except Exception as e:
                logger.error("Knowledge MCP call failed: %s", e)
                return {"error": str(e)}

    async def call_matrix(endpoint: str, data: dict) -> dict:
        """Send notifications and requests to Matrix bot."""
        async with httpx.AsyncClient(timeout=30.0) as client:
            try:
                response = await client.post(MATRIX_BOT_URL + "/" + endpoint, json=data)
                response.raise_for_status()
                return response.json()
            except Exception as e:
                logger.error("Matrix bot call failed: %s", e)
                return {"error": str(e)}

    async def call_coroot_mcp(tool_name: str, **kwargs) -> dict:
        """Call Coroot MCP for metrics and anomaly data."""
        async with httpx.AsyncClient(timeout=30.0) as client:
            try:
                response = await client.post(
                    COROOT_MCP_URL + "/mcp",
                    json={
                        "jsonrpc": "2.0",
                        "id": 1,
                        "method": "tools/call",
                        "params": {"name": tool_name, "arguments": kwargs}
                    }
                )
                if response.status_code == 200:
                    result = response.json()
                    content = result.get("result", {}).get("content", [])
                    if content:
                        return json.loads(content[0].get("text", "{}"))
                return {}
            except Exception as e:
                logger.warning(f"Coroot MCP call failed: {e}")
                return {}

    async def discover_service(service_name: str) -> dict:
        """
        Discover service endpoint and credentials from Qdrant entities collection.
        Use this before calling infrastructure services to get their endpoints.
        """
        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                # Search entities collection in Qdrant
                response = await client.post(
                    QDRANT_URL + "/collections/entities/points/scroll",
                    json={
                        "limit": 100,
                        "with_payload": True,
                        "filter": {
                            "should": [
                                {"key": "hostname", "match": {"text": service_name}},
                                {"key": "type", "match": {"text": service_name}}
                            ]
                        } if service_name != "all" else None
                    }
                )
                if response.status_code == 200:
                    result = response.json()
                    points = result.get("result", {}).get("points", [])
                    return {"entities": [p.get("payload", {}) for p in points]}
                return {}
            except Exception as e:
                logger.warning(f"Service discovery failed for {service_name}: {e}")
                return {}

    async def get_credentials_path(service_name: str) -> str:
        """Get Infisical credentials path for a service from Qdrant device_types."""
        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                # Search device_types collection for credentials path
                response = await client.post(
                    QDRANT_URL + "/collections/device_types/points/scroll",
                    json={
                        "limit": 10,
                        "with_payload": True,
                        "filter": {
                            "must": [
                                {"key": "name", "match": {"text": service_name}}
                            ]
                        }
                    }
                )
                if response.status_code == 200:
                    result = response.json()
                    points = result.get("result", {}).get("points", [])
                    if points:
                        return points[0].get("payload", {}).get("default_credentials_path", "")
                return ""
            except Exception as e:
                logger.warning(f"Credentials path lookup failed for {service_name}: {e}")
                return ""

    async def get_embedding(text: str) -> list:
        """Get embedding from Gemini via LiteLLM."""
        async with httpx.AsyncClient(timeout=30.0) as client:
            try:
                response = await client.post(
                    LITELLM_URL + "/v1/embeddings",
                    json={"model": EMBEDDING_MODEL, "input": text}
                )
                if response.status_code == 200:
                    data = response.json()
                    if data.get("data") and len(data["data"]) > 0:
                        return data["data"][0].get("embedding", [])
                logger.warning("Embedding API returned %d", response.status_code)
                return []
            except Exception as e:
                logger.error("Embedding error: %s", e)
                return []

    async def build_comprehensive_context(alert: dict) -> dict:
        """
        Build comprehensive context for Gemini using its 1M token window.
        Combines static cached context with dynamic per-request context.
        """
        global _static_context_cache, _static_context_timestamp

        alert_text = f"{alert.get('alertname', '')} {alert.get('description', '')}"
        alert_embedding = await get_embedding(alert_text)

        # Check if static context needs refresh (hourly)
        now = datetime.utcnow()
        if (_static_context_timestamp is None or
            (now - _static_context_timestamp).total_seconds() > CONTEXT_CACHE_TTL):
            logger.info("Refreshing static context cache...")
            _static_context_cache = await _fetch_static_context()
            _static_context_timestamp = now

        # Fetch dynamic context (always fresh)
        dynamic_context = await _fetch_dynamic_context(alert, alert_embedding)

        return {
            **_static_context_cache,
            **dynamic_context,
            "alert": alert
        }

    async def _fetch_static_context() -> dict:
        """Fetch context that changes infrequently (cached hourly)."""
        context = {}

        # Get all runbooks from Qdrant
        async with httpx.AsyncClient(timeout=30.0) as client:
            try:
                response = await client.post(
                    QDRANT_URL + "/collections/runbooks/points/scroll",
                    json={"limit": 100, "with_payload": True}
                )
                if response.status_code == 200:
                    points = response.json().get("result", {}).get("points", [])
                    context["runbook_library"] = [p.get("payload", {}) for p in points]
            except Exception as e:
                logger.warning(f"Failed to fetch runbooks: {e}")
                context["runbook_library"] = []

        # Get documentation from knowledge MCP
        try:
            docs = await call_knowledge_mcp("documentation", "GET")
            context["documentation"] = docs if not docs.get("error") else []
        except:
            context["documentation"] = []

        # Get entity inventory from Qdrant
        try:
            inventory = await discover_service("all")
            context["entity_inventory"] = inventory
        except:
            context["entity_inventory"] = {}

        return context

    async def _fetch_dynamic_context(alert: dict, alert_embedding: list) -> dict:
        """Fetch fresh context for each request."""
        context = {}

        # Get cluster state from infrastructure MCP
        async with httpx.AsyncClient(timeout=15.0) as client:
            try:
                response = await client.get(INFRASTRUCTURE_MCP_URL + "/cluster/status")
                if response.status_code == 200:
                    context["cluster_state"] = response.json()
            except:
                context["cluster_state"] = {}

        # Get Coroot metrics and anomalies for affected service
        service = alert.get("labels", {}).get("service") or alert.get("namespace", "")
        if service:
            context["coroot_metrics"] = await call_coroot_mcp("get_service_metrics", service=service)
            context["coroot_anomalies"] = await call_coroot_mcp("get_recent_anomalies", hours=24)
            context["service_dependencies"] = await call_coroot_mcp("get_service_dependencies", service=service)

        # Get similar past decisions from Qdrant
        if alert_embedding:
            async with httpx.AsyncClient(timeout=10.0) as client:
                try:
                    response = await client.post(
                        QDRANT_URL + "/collections/decisions/points/search",
                        json={"vector": alert_embedding, "limit": 5, "with_payload": True}
                    )
                    if response.status_code == 200:
                        hits = response.json().get("result", [])
                        context["similar_past_decisions"] = [h.get("payload", {}) for h in hits]
                except:
                    context["similar_past_decisions"] = []

        # Get recent decisions (last 24h)
        context["recent_decisions_24h"] = await _get_recent_decisions(hours=24)

        return context

    async def _get_recent_decisions(hours: int = 24) -> list:
        """Get recent decisions from Qdrant."""
        cutoff = (datetime.utcnow() - timedelta(hours=hours)).isoformat()
        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                response = await client.post(
                    QDRANT_URL + "/collections/decisions/points/scroll",
                    json={
                        "limit": 50,
                        "with_payload": True,
                        "filter": {
                            "must": [
                                {"key": "timestamp", "range": {"gte": cutoff}}
                            ]
                        }
                    }
                )
                if response.status_code == 200:
                    points = response.json().get("result", {}).get("points", [])
                    return [p.get("payload", {}) for p in points]
            except:
                pass
        return []

    async def detect_capability_gap(gemini_response: str, alert: dict) -> Optional[dict]:
        """Detect if Gemini indicates a missing capability."""
        gap_signals = [
            "I don't have access to",
            "I cannot query",
            "No MCP available for",
            "I need a tool to",
            "Missing capability:",
            "Unable to retrieve",
        ]

        for signal in gap_signals:
            if signal.lower() in gemini_response.lower():
                # Extract description of what's needed
                gap = {
                    "description": f"Capability gap detected while processing alert: {alert.get('alertname')}",
                    "tools_needed": [],
                    "triggering_alerts": [alert.get("id", "")],
                    "gemini_response_excerpt": gemini_response[:500]
                }

                # Store to Qdrant for Claude Validator to process
                await store_capability_gap(gap)
                return gap

        return None

    async def store_capability_gap(gap: dict):
        """Store capability gap to Qdrant for later processing."""
        embedding = await get_embedding(gap.get("description", ""))
        if not embedding:
            return

        gap_id = f"cap-gap-{uuid_mod.uuid4().hex[:8]}"
        gap["id"] = gap_id
        gap["created_at"] = datetime.utcnow().isoformat()
        gap["status"] = "pending"

        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                await client.put(
                    QDRANT_URL + "/collections/capability_gaps/points",
                    json={
                        "points": [{
                            "id": str(uuid_mod.uuid5(uuid_mod.NAMESPACE_DNS, gap_id)),
                            "vector": embedding,
                            "payload": gap
                        }]
                    }
                )
                logger.info(f"Stored capability gap: {gap_id}")
            except Exception as e:
                logger.error(f"Failed to store capability gap: {e}")

    async def search_runbooks_qdrant(query: str, limit: int = 3):
        """Search Qdrant for relevant diagnostic runbooks."""
        embedding = await get_embedding(query)
        if not embedding:
            return []
        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                response = await client.post(
                    QDRANT_URL + "/collections/runbooks/points/search",
                    json={"vector": embedding, "limit": limit, "with_payload": True, "score_threshold": 0.5}
                )
                if response.status_code == 200:
                    results = []
                    for hit in response.json().get("result", []):
                        payload = hit.get("payload", {})
                        # Check if it's a v2 diagnostic runbook or legacy
                        if "diagnosis" in payload or "triggers" in payload:
                            # V2 diagnostic runbook - parse into Runbook model
                            try:
                                runbook = Runbook(**payload)
                                results.append({
                                    "id": hit.get("id"),
                                    "score": hit.get("score"),
                                    "runbook": runbook,
                                    "version": 2
                                })
                            except Exception as e:
                                logger.warning(f"Failed to parse runbook: {e}")
                        else:
                            # Legacy simple runbook
                            results.append({
                                "id": hit.get("id"),
                                "score": hit.get("score"),
                                "name": payload.get("name", "Unknown"),
                                "description": payload.get("description", ""),
                                "steps": payload.get("steps", []),
                                "fix_command": payload.get("fix_command", ""),
                                "version": 1
                            })
                    return results
                return []
            except Exception as e:
                logger.error("Runbook search error: %s", e)
                return []

    async def store_runbook_qdrant(runbook: Runbook) -> bool:
        """Store a diagnostic runbook in Qdrant."""
        search_text = f"{runbook.name} {runbook.description} {' '.join(runbook.triggers.keywords)}"
        embedding = await get_embedding(search_text)
        if not embedding:
            logger.error("Failed to generate embedding for runbook")
            return False
        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                payload = runbook.model_dump()
                # Generate a proper UUID for Qdrant (it doesn't accept arbitrary strings)
                point_id = str(uuid_mod.uuid5(uuid_mod.NAMESPACE_DNS, runbook.id))
                response = await client.put(
                    QDRANT_URL + "/collections/runbooks/points",
                    json={
                        "points": [{
                            "id": point_id,
                            "vector": embedding,
                            "payload": payload
                        }]
                    }
                )
                if response.status_code == 200:
                    logger.info(f"Stored runbook: {runbook.name} ({runbook.id}) -> point {point_id}")
                    return True
                else:
                    logger.error(f"Qdrant store failed: {response.status_code} - {response.text}")
                    return False
            except Exception as e:
                logger.error("Runbook store error: %s", e)
                return False

    async def update_runbook_stats(runbook_id: str, success: bool, escalated: bool = False):
        """Update runbook success/failure statistics."""
        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                # Get current runbook
                response = await client.get(
                    QDRANT_URL + f"/collections/runbooks/points/{runbook_id}"
                )
                if response.status_code != 200:
                    return False

                payload = response.json().get("result", {}).get("payload", {})
                metadata = payload.get("metadata", {})

                if success:
                    metadata["success_count"] = metadata.get("success_count", 0) + 1
                else:
                    metadata["failure_count"] = metadata.get("failure_count", 0) + 1
                if escalated:
                    metadata["escalation_count"] = metadata.get("escalation_count", 0) + 1

                # Check if eligible for auto-approve
                if (metadata.get("success_count", 0) >= AUTO_APPROVE_MIN_SUCCESSES and
                    metadata.get("failure_count", 0) == 0):
                    metadata["auto_approve_eligible"] = True
                    logger.info(f"Runbook {runbook_id} now eligible for auto-approve!")

                metadata["updated_at"] = datetime.utcnow().isoformat()
                payload["metadata"] = metadata

                # Update in Qdrant
                embedding = await get_embedding(f"{payload.get('name', '')} {payload.get('description', '')}")
                if embedding:
                    await client.put(
                        QDRANT_URL + "/collections/runbooks/points",
                        json={"points": [{"id": runbook_id, "vector": embedding, "payload": payload}]}
                    )
                return True
            except Exception as e:
                logger.error(f"Failed to update runbook stats: {e}")
                return False

    def parse_runbook_from_claude(response: str, alert: dict) -> Optional[Runbook]:
        """Parse a runbook definition from Claude's response."""
        try:
            # Find JSON block
            if "```json" in response:
                json_str = response.split("```json")[-1].split("```")[0]
            elif "```" in response:
                json_str = response.split("```")[-2] if response.count("```") >= 2 else response.split("```")[1]
                json_str = json_str.split("```")[0]
            else:
                return None

            data = json.loads(json_str.strip())

            # Build Runbook from Claude's response
            runbook = Runbook(
                name=data.get("runbook_name", f"Alert: {alert.get('alertname', 'Unknown')}"),
                description=data.get("runbook_description", alert.get("description", "")),
                triggers=TriggerConfig(
                    alert_patterns=data.get("triggers", {}).get("alert_patterns", [alert.get("alertname", "")]),
                    keywords=data.get("triggers", {}).get("keywords", [])
                ),
                diagnosis=DiagnosisConfig(
                    description="Verify the issue",
                    checks=[
                        DiagnosisCheck(
                            name=c.get("name", "Check"),
                            command=c.get("command", "echo 'no command'"),
                            look_for=[LookForPattern(pattern=p, confirms="issue") for p in c.get("look_for", [])]
                        )
                        for c in data.get("diagnosis_checks", [])
                    ],
                    confidence_rules=[
                        ConfidenceRule(condition="true", confidence=0.8, diagnosis="likely_match")
                    ]
                ),
                information_gathering=InfoGatherConfig(
                    description="Gather context",
                    commands=[
                        GatherCommand(name="Pod describe", command="kubectl describe pod {{pod_name}} -n {{namespace}}", store_as="pod_describe"),
                        GatherCommand(name="Pod logs", command="kubectl logs {{pod_name}} -n {{namespace}} --tail=50", store_as="pod_logs", on_failure="continue")
                    ]
                ),
                decision=DecisionRules(
                    escalate_if=["confidence < 0.7"],
                    handle_locally_if=["confidence >= 0.7"]
                ),
                fix=FixConfig(
                    description=data.get("runbook_description", "Apply fix"),
                    steps=[
                        FixStep(
                            name=s.get("name", "Step"),
                            action="command",
                            command=s.get("command", "echo 'no command'")
                        )
                        for s in data.get("fix_steps", [])
                    ],
                    validation=[
                        ValidationCheck(
                            name=v.get("name", "Validate"),
                            command=v.get("command", "echo 'ok'"),
                            expected=v.get("expected")
                        )
                        for v in data.get("validation", [])
                    ]
                ),
                metadata=RunbookMetadata(
                    created_by="claude",
                    tags=["auto-generated"]
                )
            )
            return runbook
        except Exception as e:
            logger.warning(f"Failed to parse runbook from Claude: {e}")
            return None

    # Priority levels (match claude-agent values)
    PRIORITY_USER = 1       # User-initiated requests (highest)
    PRIORITY_CRITICAL = 2   # Critical alerts
    PRIORITY_WARNING = 3    # Warning alerts
    PRIORITY_LOW = 4        # Background tasks (lowest)

    def severity_to_priority(severity: str) -> int:
        """Map alert severity to queue priority."""
        mapping = {
            "critical": PRIORITY_CRITICAL,
            "error": PRIORITY_CRITICAL,
            "warning": PRIORITY_WARNING,
            "info": PRIORITY_LOW,
        }
        return mapping.get(severity.lower(), PRIORITY_WARNING)

    async def queue_claude_task(prompt: str, priority: int = PRIORITY_LOW, context: dict = None,
                                allowed_tools: list = None, timeout: int = 600) -> str:
        """
        Queue a task for Claude Agent with priority.
        Returns task_id for polling status.
        """
        async with httpx.AsyncClient(timeout=30.0) as client:
            payload = {
                "prompt": prompt,
                "priority": priority,
                "context": context or {},
                "allowed_tools": allowed_tools or ["Read", "Glob", "Grep", "Bash", "WebSearch"],
                "working_directory": "/workspace",
                "timeout": timeout,
            }
            response = await client.post(
                CLAUDE_AGENT_URL + "/queue/submit",
                json=payload
            )
            response.raise_for_status()
            result = response.json()
            return result.get("task_id")

    async def poll_task_status(task_id: str, poll_interval: float = 2.0, max_wait: float = 600.0) -> dict:
        """
        Poll for task completion.
        Returns task result dict with status, result, error fields.
        """
        async with httpx.AsyncClient(timeout=30.0) as client:
            elapsed = 0.0
            while elapsed < max_wait:
                response = await client.get(f"{CLAUDE_AGENT_URL}/task/{task_id}")
                response.raise_for_status()
                task = response.json()
                if task.get("status") in ["completed", "failed"]:
                    return task
                await asyncio.sleep(poll_interval)
                elapsed += poll_interval
            return {"status": "timeout", "error": f"Task did not complete within {max_wait}s"}

    async def call_claude_agent(prompt: str, context: dict = None, allowed_tools: list = None,
                                priority: int = None, use_queue: bool = False, timeout: int = 300):
        """
        Call Claude Agent for complex agentic tasks.

        Args:
            prompt: The task prompt
            context: Optional context dict
            allowed_tools: List of allowed tools
            priority: Queue priority (1=user, 2=critical, 3=warning, 4=low). If set, enables queue mode.
            use_queue: If True, use priority queue (async) instead of sync mode
            timeout: Timeout in seconds

        Uses subscription-based Claude Code for full tool access.
        """
        # If priority is set or use_queue is True, use async queue mode
        if priority is not None or use_queue:
            priority = priority or PRIORITY_LOW
            try:
                task_id = await queue_claude_task(prompt, priority, context, allowed_tools, timeout)
                logger.info(f"Queued Claude task {task_id} with priority {priority}")
                result = await poll_task_status(task_id, max_wait=float(timeout))
                if result.get("status") == "completed":
                    return result.get("result", "")
                else:
                    error = result.get("error", "Task failed or timed out")
                    logger.error(f"Claude task {task_id} failed: {error}")
                    return f"Error: {error}"
            except Exception as e:
                logger.error(f"Claude queue failed: {e}")
                return await call_litellm([{"role": "user", "content": prompt}])

        # Sync mode - direct call (for interactive/user requests)
        async with httpx.AsyncClient(timeout=float(timeout)) as client:
            try:
                payload = {
                    "prompt": prompt,
                    "context": context or {},
                    "allowed_tools": allowed_tools or ["Read", "Glob", "Grep", "Bash", "WebSearch"],
                    "max_turns": 10,
                    "working_directory": "/workspace"
                }
                response = await client.post(
                    CLAUDE_AGENT_URL + "/agent/run",
                    json=payload
                )
                response.raise_for_status()
                result = response.json()
                if result.get("success"):
                    return result.get("result", "")
                else:
                    logger.error("Claude Agent error: %s", result.get("error"))
                    return "Error: " + result.get("error", "Unknown error")
            except Exception as e:
                logger.error("Claude Agent call failed: %s", e)
                # Fallback to local LLM
                return await call_litellm([{"role": "user", "content": prompt}])

    def should_use_claude(assessment: dict) -> bool:
        """Determine if task should be routed to Claude based on complexity."""
        complexity = assessment.get("complexity", "medium")
        threshold = CLAUDE_COMPLEXITY_THRESHOLD

        complexity_levels = {"low": 1, "medium": 2, "high": 3, "critical": 4}
        threshold_levels = {"low": 1, "medium": 2, "high": 3, "critical": 4}

        return complexity_levels.get(complexity, 2) >= threshold_levels.get(threshold, 3)

    def build_assessment_prompt(alert, similar):
        return """Analyze this alert and provide assessment:
    Alert: """ + str(alert.get("alertname", "unknown")) + """
    Severity: """ + str(alert.get("severity", "warning")) + """
    Description: """ + str(alert.get("description", "")) + """
    Namespace: """ + str(alert.get("namespace", "default")) + """

    Similar runbooks found: """ + str(similar) + """

    Respond with JSON containing: domain, complexity, requires_approval, similar_runbook, similarity_score, recommended_topic"""

    def build_solutions_prompt(alert, assessment):
        return """Generate 2-3 solutions for this alert:
    Alert: """ + str(alert.get("alertname", "unknown")) + """
    Description: """ + str(alert.get("description", "")) + """
    Assessment: """ + str(assessment) + """

    Respond with JSON array containing objects with: name, description, impact, risk, commands"""

    async def assess_alert(state):
        """Assess alert by searching for diagnostic runbooks and running diagnosis."""
        alert = state["alert"]
        alert_text = f"{alert.get('alertname', '')} {alert.get('description', '')}"

        # Step 1: Search Qdrant for diagnostic runbooks
        logger.info("Searching for matching diagnostic runbooks...")
        runbooks = await search_runbooks_qdrant(alert_text, limit=3)

        runbook_match = None
        executor_result = None

        # Step 2: If we found a v2 diagnostic runbook, run the executor
        if runbooks and runbooks[0].get("score", 0) >= RUNBOOK_MATCH_THRESHOLD:
            match = runbooks[0]
            logger.info(f"Found matching runbook (score: {match.get('score'):.2f})")

            if match.get("version") == 2 and match.get("runbook"):
                # V2 diagnostic runbook - run full diagnosis
                runbook = match["runbook"]
                logger.info(f"Running diagnostic runbook: {runbook.name}")

                executor = RunbookExecutor(runbook, alert)
                executor_result = await executor.execute()

                runbook_match = {
                    "id": match["id"],
                    "score": match["score"],
                    "name": runbook.name,
                    "version": 2,
                    "runbook": runbook,
                    "executor": executor,
                    "executor_result": executor_result
                }
            else:
                # V1 legacy runbook
                runbook_match = {
                    "id": match["id"],
                    "score": match["score"],
                    "name": match.get("name", "Unknown"),
                    "version": 1,
                    "steps": match.get("steps", []),
                    "fix_command": match.get("fix_command", "")
                }

        # Step 3: Build assessment from executor results or LLM
        if executor_result:
            assessment = {
                "domain": "infrastructure",
                "complexity": "low" if executor_result.get("action") == "fix_ready" else "high",
                "requires_approval": executor_result.get("requires_approval", True),
                "confidence": executor_result.get("confidence", 0),
                "diagnosis_result": executor_result.get("diagnosis_result", "unknown"),
                "escalated": executor_result.get("escalated", False),
                "escalation_reason": executor_result.get("escalation_reason"),
                "similar_runbook": runbook_match.get("name") if runbook_match else None,
                "similarity_score": runbook_match.get("score", 0) if runbook_match else 0,
                "recommended_topic": "infrastructure"
            }
        else:
            # No matching runbook - use LLM for basic assessment
            prompt = build_assessment_prompt(alert, runbooks)
            logger.info("No diagnostic runbook found, assessing with local LLM...")
            result = await call_litellm([{"role": "user", "content": prompt}])

            try:
                clean_result = result.strip()
                if "```json" in clean_result:
                    clean_result = clean_result.split("```json")[1].split("```")[0]
                elif "```" in clean_result:
                    clean_result = clean_result.split("```")[1].split("```")[0]
                assessment = json.loads(clean_result.strip())
            except:
                assessment = {
                    "domain": "infrastructure",
                    "complexity": "high",  # Unknown = escalate to Claude
                    "requires_approval": True,
                    "recommended_topic": "infrastructure"
                }
            assessment["escalated"] = True  # No runbook means escalate
            assessment["escalation_reason"] = "No matching diagnostic runbook found"

        return {
            "assessment": assessment,
            "runbook_match": runbook_match or {},
            "handled_locally": False
        }

    async def generate_solutions(state):
        """Generate solutions using diagnostic runbook results or escalate to Claude."""
        alert = state["alert"]
        assessment = state["assessment"]
        runbook_match = state.get("runbook_match", {})

        # Case 1: V2 diagnostic runbook with executor result
        if runbook_match.get("version") == 2 and runbook_match.get("executor_result"):
            executor_result = runbook_match["executor_result"]
            runbook = runbook_match.get("runbook")

            if executor_result.get("action") == "fix_ready":
                # Diagnosis passed, fix is ready to apply
                logger.info(f"Runbook diagnosis passed (confidence: {executor_result.get('confidence', 0):.2f})")
                solutions = [{
                    "name": f"Apply Runbook: {runbook.name}",
                    "description": f"Diagnosis: {executor_result.get('diagnosis_result', 'confirmed')} (confidence: {executor_result.get('confidence', 0):.0%})",
                    "impact": "low",
                    "risk": "low",
                    "fix_steps": executor_result.get("fix_steps", []),
                    "source": "diagnostic_runbook",
                    "runbook_id": runbook_match.get("id"),
                    "runbook": runbook,
                    "executor": runbook_match.get("executor"),
                    "gathered_info": executor_result.get("gathered_info", {})
                }]
                return {"solutions": solutions, "handled_locally": True}

            elif executor_result.get("action") == "escalate":
                # Diagnosis requires escalation - send to Claude with all gathered info
                logger.info(f"Escalating to Claude: {executor_result.get('escalation_reason')}")
                escalation_msg = executor_result.get("escalation_message", "")

                # Use priority queue based on alert severity
                severity = alert.get("severity", "warning")
                result = await call_claude_agent(
                    escalation_msg,
                    context={
                        "alert": alert,
                        "assessment": assessment,
                        "gathered_info": executor_result.get("gathered_info", {}),
                        "diagnosis": executor_result.get("diagnosis", {})
                    },
                    allowed_tools=["Read", "Glob", "Grep", "Bash", "WebSearch"],
                    priority=severity_to_priority(severity)
                )

                # Parse and store new runbook from Claude
                new_runbook = parse_runbook_from_claude(result, alert)
                if new_runbook:
                    await store_runbook_qdrant(new_runbook)
                    logger.info(f"Stored new diagnostic runbook: {new_runbook.name}")

                # Update escalation stats
                await update_runbook_stats(runbook_match.get("id"), success=False, escalated=True)

                # Parse solutions from Claude
                solutions = _parse_solutions_from_response(result, alert)
                return {"solutions": solutions, "handled_locally": False, "escalated_to_claude": True}

        # Case 2: V1 legacy runbook
        elif runbook_match.get("version") == 1 and runbook_match.get("score", 0) >= RUNBOOK_MATCH_THRESHOLD:
            logger.info(f"Using legacy runbook: {runbook_match.get('name')}")
            solutions = [{
                "name": f"Apply Runbook: {runbook_match.get('name')}",
                "description": "Legacy runbook (no diagnostic verification)",
                "impact": "medium",
                "risk": "medium",
                "commands": runbook_match.get("steps", []),
                "fix_command": runbook_match.get("fix_command"),
                "source": "legacy_runbook",
                "runbook_id": runbook_match.get("id")
            }]
            return {"solutions": solutions, "handled_locally": True}

        # Case 3: No runbook - escalate to Claude for new runbook creation
        logger.info("No matching runbook - escalating to Claude for analysis and runbook creation")
        escalation_prompt = "NEW ALERT - NO EXISTING RUNBOOK\n\n"
        escalation_prompt += "ALERT DETAILS:\n"
        escalation_prompt += f"- Name: {alert.get('alertname', 'Unknown')}\n"
        escalation_prompt += f"- Severity: {alert.get('severity', 'warning')}\n"
        escalation_prompt += f"- Description: {alert.get('description', 'No description')}\n"
        escalation_prompt += f"- Namespace: {alert.get('namespace', 'default')}\n"
        escalation_prompt += f"- Labels: {json.dumps(alert.get('labels', {}))}\n\n"
        escalation_prompt += "TASK:\n"
        escalation_prompt += "1. Investigate this alert type\n"
        escalation_prompt += "2. Determine the root cause and best fix approach\n"
        escalation_prompt += "3. Create a diagnostic runbook for future occurrences\n\n"
        escalation_prompt += "REQUIRED RUNBOOK FORMAT:\n"
        escalation_prompt += "Return a runbook definition as JSON at the end with this structure:\n"
        escalation_prompt += '{"runbook_name": "...", "runbook_description": "...", '
        escalation_prompt += '"triggers": {"alert_patterns": [...], "keywords": [...]}, '
        escalation_prompt += '"diagnosis_checks": [{"name": "...", "command": "kubectl ...", "look_for": [...]}], '
        escalation_prompt += '"fix_steps": [{"name": "...", "command": "..."}], '
        escalation_prompt += '"validation": [{"name": "...", "command": "...", "expected": "..."}]}\n\n'
        escalation_prompt += "Provide your analysis first, then the runbook JSON."

        # Use priority queue based on alert severity
        severity = alert.get("severity", "warning")
        result = await call_claude_agent(
            escalation_prompt,
            context={"alert": alert},
            allowed_tools=["Read", "Glob", "Grep", "Bash", "WebSearch"],
            priority=severity_to_priority(severity)
        )

        # Parse and store new runbook from Claude
        new_runbook = parse_runbook_from_claude(result, alert)
        if new_runbook:
            await store_runbook_qdrant(new_runbook)
            logger.info(f"Stored new diagnostic runbook: {new_runbook.name}")

        # Parse solutions from Claude
        solutions = _parse_solutions_from_response(result, alert)
        return {"solutions": solutions, "handled_locally": False, "escalated_to_claude": True}

    def _parse_solutions_from_response(result: str, alert: dict) -> list:
        """Parse solutions from Claude's response."""
        try:
            # Look for JSON solutions block
            if "```json" in result:
                parts = result.split("```json")
                for part in parts[1:]:
                    json_str = part.split("```")[0]
                    try:
                        data = json.loads(json_str.strip())
                        if isinstance(data, list):
                            return data
                        elif isinstance(data, dict) and "fix_steps" in data:
                            # This is a runbook, convert to solution
                            return [{
                                "name": data.get("runbook_name", f"Fix: {alert.get('alertname')}"),
                                "description": data.get("runbook_description", "Apply fix from Claude"),
                                "impact": "medium",
                                "risk": "medium",
                                "commands": [s.get("command", "") for s in data.get("fix_steps", [])],
                                "source": "claude"
                            }]
                    except:
                        continue
        except:
            pass

        # Default solution if parsing fails
        return [{
            "name": f"Claude Analysis: {alert.get('alertname', 'Unknown')}",
            "description": result[:500] if result else "Review Claude's analysis",
            "impact": "medium",
            "risk": "medium",
            "commands": [],
            "source": "claude",
            "full_response": result
        }]

    async def request_approval(state):
        """Request approval via Matrix bot."""
        alert = state["alert"]
        solutions = state["solutions"]
        assessment = state["assessment"]

        pending_approvals[alert["id"]] = {
            "alert": alert,
            "solutions": solutions,
            "assessment": assessment,
            "created_at": datetime.utcnow().isoformat()
        }

        # Determine room based on severity
        severity = alert.get("severity", "warning")
        room = "#critical" if severity == "critical" else "#infrastructure"

        # Format solutions for Matrix message
        solutions_text = ""
        for i, sol in enumerate(solutions, 1):
            solutions_text += f"\n{i}. **{sol.get('name', 'Solution')}**\n"
            solutions_text += f"   {sol.get('description', '')}\n"
            solutions_text += f"   Risk: {sol.get('risk', 'unknown')}\n"

        result = await call_matrix("approval", {
            "alert_id": alert["id"],
            "room": room,
            "alert": alert,
            "solutions": solutions,
            "message": (
                f"**Alert: {alert.get('alertname', 'Unknown')}**\n"
                f"Severity: {severity}\n"
                f"Namespace: {alert.get('namespace', 'default')}\n\n"
                f"{alert.get('description', 'No description')}\n\n"
                f"**Recommended Solutions:**\n"
                f"{solutions_text}\n\n"
                f"React  to approve solution 1, or reply with solution number.\n"
                f"React  to reject/ignore."
            ),
            "context": {
                "similar_runbook": assessment.get("similar_runbook"),
                "similarity": assessment.get("similarity_score", 0),
                "confidence": assessment.get("confidence", 0)
            }
        })

        return {"approval_status": "pending", "thread_id": result.get("thread_id", "")}

    async def execute_solution(state):
        """Execute the selected solution - uses runbook executor for diagnostic runbooks."""
        solution = state["selected_solution"]
        if not solution:
            return {"execution_result": {"success": False, "error": "No solution selected"}}

        # Case 1: Diagnostic runbook with executor
        if solution.get("source") == "diagnostic_runbook" and solution.get("executor"):
            executor = solution["executor"]
            logger.info(f"Executing diagnostic runbook fix via executor")

            # Run the approved fix through the executor
            fix_result = await executor.execute_approved_fix()

            # Update runbook stats
            runbook_id = solution.get("runbook_id")
            if runbook_id:
                await update_runbook_stats(runbook_id, success=fix_result.get("success", False))

            return {
                "execution_result": {
                    "success": fix_result.get("success", False),
                    "action": fix_result.get("action", "unknown"),
                    "fix_result": fix_result.get("fix_result", {}),
                    "validation": fix_result.get("validation", {}),
                    "rollback": fix_result.get("rollback"),
                    "summary": f"Runbook execution: {fix_result.get('action', 'unknown')}"
                }
            }

        # Case 2: Legacy runbook or direct commands
        commands = solution.get("commands", [])
        if not commands and solution.get("fix_steps"):
            # Extract commands from fix_steps
            commands = [s.get("command", "") for s in solution.get("fix_steps", []) if s.get("command")]

        if not commands:
            return {"execution_result": {"success": False, "error": "No commands to execute"}}

        logger.info(f"Executing {len(commands)} commands directly")
        results = []
        for cmd in commands:
            if not cmd:
                continue
            async with httpx.AsyncClient(timeout=120.0) as client:
                try:
                    response = await client.post(
                        INFRASTRUCTURE_MCP_URL + "/execute",
                        json={"command": cmd, "timeout": 60}
                    )
                    result = response.json()
                    results.append(result)
                    if not result.get("success", True):
                        logger.warning(f"Command failed: {cmd[:50]}...")
                except Exception as e:
                    results.append({"error": str(e), "command": cmd})
                    logger.error(f"Command error: {e}")

        success = all(r.get("success", True) and "error" not in r for r in results)

        # Update runbook stats if applicable
        runbook_id = solution.get("runbook_id")
        if runbook_id:
            await update_runbook_stats(runbook_id, success=success)

        return {
            "execution_result": {
                "success": success,
                "results": results,
                "summary": f"Executed {len(results)} commands, {'all succeeded' if success else 'some failed'}"
            }
        }

    async def record_outcome(state):
        await call_knowledge_mcp("record", "POST", {
            "collection": "decisions",
            "data": {
                "alert_id": state["alert"]["id"],
                "alert": state["alert"],
                "assessment": state["assessment"],
                "solution": state["selected_solution"],
                "result": state["execution_result"],
                "approval_status": state["approval_status"],
                "timestamp": datetime.utcnow().isoformat()
            }
        })
        return {}

    def should_request_approval(state):
        assessment = state.get("assessment", {})
        if assessment.get("requires_approval", True):
            return "request_approval"
        return "execute_solution"

    def create_workflow():
        workflow = StateGraph(AgentState)
        workflow.add_node("assess_alert", assess_alert)
        workflow.add_node("generate_solutions", generate_solutions)
        workflow.add_node("request_approval", request_approval)
        workflow.add_node("execute_solution", execute_solution)
        workflow.add_node("record_outcome", record_outcome)
        workflow.set_entry_point("assess_alert")
        workflow.add_edge("assess_alert", "generate_solutions")
        workflow.add_conditional_edges(
            "generate_solutions",
            should_request_approval,
            {"request_approval": "request_approval", "execute_solution": "execute_solution"}
        )
        workflow.add_edge("request_approval", "record_outcome")
        workflow.add_edge("execute_solution", "record_outcome")
        workflow.add_edge("record_outcome", END)
        return workflow.compile()

    graph = create_workflow()

    async def _process_alert_background(alert_dict: dict, initial_state: dict):
        """Background task to process alert through the graph."""
        try:
            logger.info("Processing alert %s in background", alert_dict["id"])
            result = await graph.ainvoke(initial_state)
            logger.info("Alert %s processed: status=%s", alert_dict["id"], result.get("approval_status", "unknown"))
        except Exception as e:
            logger.error("Background graph execution failed for %s: %s", alert_dict["id"], e)

    @app.post("/alert", status_code=202)
    async def process_alert(alert: AlertInput):
        """Receive alert and process asynchronously. Returns immediately with 202 Accepted."""
        alert_dict = alert.model_dump()
        alert_dict["id"] = alert_dict.get("id") or "alert-" + datetime.utcnow().strftime("%Y%m%d%H%M%S")
        initial_state = {
            "messages": [],
            "alert": alert_dict,
            "assessment": {},
            "solutions": [],
            "selected_solution": {},
            "approval_status": "pending",
            "execution_result": {},
            "runbook_id": "",
            "topic_id": 0,
            "thread_id": alert_dict["id"],
            "runbook_match": {},
            "handled_locally": False
        }
        # Spawn background task - returns immediately
        asyncio.create_task(_process_alert_background(alert_dict, initial_state))
        logger.info("Alert %s accepted for async processing", alert_dict["id"])
        return {
            "alert_id": alert_dict["id"],
            "status": "accepted",
            "message": "Alert queued for processing"
        }

    @app.post("/keep-alert-debug", status_code=200)
    async def keep_alert_debug(request: Request):
        """Debug endpoint to see what Keep sends."""
        body = await request.body()
        logger.info("Keep debug - raw body: %s", body.decode('utf-8', errors='replace'))
        return {"received": body.decode('utf-8', errors='replace')[:500]}

    @app.post("/keep-alert", status_code=202)
    async def process_keep_alert(alert: KeepAlert):
        """Receive alert from Keep and process. Transforms Keep format to internal format."""
        # Transform Keep alert to internal format
        alert_id = alert.fingerprint or alert.id or f"keep-{datetime.utcnow().strftime('%Y%m%d%H%M%S')}"
        namespace = "default"
        if alert.labels:
            namespace = alert.labels.get("namespace", "default")

        alert_dict = {
            "id": alert_id,
            "alertname": alert.name or alert.alertname or "unknown",
            "severity": alert.severity or "warning",
            "namespace": namespace,
            "description": alert.description or alert.message or "",
            "labels": alert.labels or {},
            "annotations": {},
            "source": alert.source,
            "keep_status": alert.status
        }

        initial_state = {
            "messages": [],
            "alert": alert_dict,
            "assessment": {},
            "solutions": [],
            "selected_solution": {},
            "approval_status": "pending",
            "execution_result": {},
            "runbook_id": "",
            "topic_id": 0,
            "thread_id": alert_dict["id"],
            "runbook_match": {},
            "handled_locally": False
        }
        # Spawn background task - returns immediately
        asyncio.create_task(_process_alert_background(alert_dict, initial_state))
        logger.info("Keep alert %s (%s) accepted for async processing", alert_dict["alertname"], alert_dict["id"])
        return {
            "alert_id": alert_dict["id"],
            "status": "accepted",
            "message": "Keep alert queued for processing"
        }

    @app.post("/approve")
    async def approve_action(request: ApprovalRequest):
        if request.alert_id not in pending_approvals:
            raise HTTPException(status_code=404, detail="Approval not found or expired")
        pending = pending_approvals[request.alert_id]
        solutions = pending["solutions"]
        if request.solution_index < 1 or request.solution_index > len(solutions):
            raise HTTPException(status_code=400, detail="Invalid solution index")
        selected = solutions[request.solution_index - 1]
        state = {
            "messages": [],
            "alert": pending["alert"],
            "assessment": pending["assessment"],
            "solutions": solutions,
            "selected_solution": selected,
            "approval_status": "approved",
            "execution_result": {},
            "runbook_id": "",
            "topic_id": 0,
            "thread_id": request.alert_id
        }
        result = await execute_solution(state)
        state.update(result)
        await record_outcome(state)
        del pending_approvals[request.alert_id]
        return {
            "success": result["execution_result"].get("success", False),
            "summary": result["execution_result"].get("summary", ""),
            "runbook_id": "runbook-" + request.alert_id[:8]
        }

    @app.post("/ignore")
    async def ignore_action(request: IgnoreRequest):
        if request.alert_id in pending_approvals:
            pending = pending_approvals[request.alert_id]
            await call_knowledge_mcp("record", "POST", {
                "collection": "decisions",
                "data": {
                    "alert_id": request.alert_id,
                    "alert": pending["alert"],
                    "action": "ignored",
                    "ignored_by": request.ignored_by,
                    "timestamp": datetime.utcnow().isoformat()
                }
            })
            del pending_approvals[request.alert_id]
        return {"status": "ignored"}

    @app.get("/pending/{alert_id}")
    async def get_pending(alert_id: str):
        if alert_id not in pending_approvals:
            return None
        return pending_approvals[alert_id]

    @app.get("/status")
    async def get_status():
        return {"pending_count": len(pending_approvals), "auto_executed": 0, "active_runbooks": 0, "learning_queue": 0}

    class QueryRequest(BaseModel):
        prompt: str
        use_claude: bool = False
        context: Optional[dict] = None
        messages: Optional[List[dict]] = None  # Conversation history [{"role": "user/assistant", "content": "..."}]
        conversation_id: Optional[str] = None  # For tracking conversations

    async def build_query_context(prompt: str) -> dict:
        """Build context from MCPs based on query keywords."""
        context = {}
        errors = []
        prompt_lower = prompt.lower()

        async with httpx.AsyncClient(timeout=15.0, verify=False) as client:
            # UniFi context for network queries
            if any(kw in prompt_lower for kw in ["unifi", "network", "device", "update", "firmware", "ap", "switch", "gateway", "wifi"]):
                try:
                    # Use REST API endpoints (not MCP protocol)
                    resp = await client.get("http://unifi-mcp:8000/api/devices")
                    if resp.status_code == 200:
                        data = resp.json()
                        if data.get("status") == "ok":
                            context["unifi_devices"] = data.get("data", [])
                            logger.info(f"Got {len(context.get('unifi_devices', []))} UniFi devices")
                        else:
                            errors.append(f"UniFi API error: {data.get('error', 'unknown')}")
                    resp = await client.get("http://unifi-mcp:8000/api/health")
                    if resp.status_code == 200:
                        data = resp.json()
                        if data.get("status") == "ok":
                            context["unifi_health"] = data.get("data", {})
                        else:
                            errors.append(f"UniFi health error: {data.get('error', 'unknown')}")
                except Exception as e:
                    logger.warning(f"Failed to get UniFi context: {e}")
                    errors.append(f"UniFi connection failed: {str(e)}")

            # Infrastructure context for cluster queries
            if any(kw in prompt_lower for kw in ["pod", "kubernetes", "k8s", "deployment", "cluster", "container", "issue", "problem"]):
                try:
                    resp = await client.get(INFRASTRUCTURE_MCP_URL + "/api/cluster")
                    if resp.status_code == 200:
                        data = resp.json()
                        if data.get("status") == "ok":
                            context["cluster_state"] = data.get("data", {})
                        else:
                            errors.append(f"Infrastructure API error: {data.get('error', 'unknown')}")
                    else:
                        errors.append(f"Infrastructure MCP returned {resp.status_code} - cluster data not available")
                except Exception as e:
                    logger.warning(f"Failed to get cluster context: {e}")
                    errors.append(f"Failed to connect to infrastructure monitoring: {str(e)}")

            # Entity discovery context from Qdrant
            if any(kw in prompt_lower for kw in ["service", "server", "ip", "host", "device"]):
                try:
                    resp = await client.post(
                        QDRANT_URL + "/collections/entities/points/scroll",
                        json={"limit": 50, "with_payload": True}
                    )
                    if resp.status_code == 200:
                        data = resp.json()
                        points = data.get("result", {}).get("points", [])
                        context["entities"] = [p.get("payload", {}) for p in points]
                except:
                    pass

            # TrueNAS context for storage queries
            if any(kw in prompt_lower for kw in ["storage", "nas", "truenas", "zfs", "pool", "dataset", "disk", "share", "smb", "nfs"]):
                try:
                    resp = await client.get("http://truenas-mcp:8000/api/pools")
                    if resp.status_code == 200:
                        data = resp.json()
                        if data.get("status") == "ok":
                            context["truenas_pools"] = data.get("data", [])
                    resp = await client.get("http://truenas-mcp:8000/api/datasets")
                    if resp.status_code == 200:
                        data = resp.json()
                        if data.get("status") == "ok":
                            context["truenas_datasets"] = data.get("data", [])
                    resp = await client.get("http://truenas-mcp:8000/api/disks")
                    if resp.status_code == 200:
                        data = resp.json()
                        if data.get("status") == "ok":
                            context["truenas_disks"] = data.get("data", [])
                except Exception as e:
                    logger.warning(f"Failed to get TrueNAS context: {e}")
                    errors.append(f"TrueNAS connection failed: {str(e)}")

            # Home Assistant context for smart home queries
            if any(kw in prompt_lower for kw in ["home", "light", "sensor", "climate", "temperature", "humidity", "automation", "smart", "zigbee", "z-wave"]):
                try:
                    resp = await client.get("http://home-assistant-mcp:8000/api/states")
                    if resp.status_code == 200:
                        data = resp.json()
                        if data.get("status") == "ok":
                            context["home_assistant_states"] = data.get("data", [])
                except Exception as e:
                    logger.warning(f"Failed to get Home Assistant context: {e}")
                    errors.append(f"Home Assistant connection failed: {str(e)}")

            # Proxmox context for VM queries
            if any(kw in prompt_lower for kw in ["vm", "virtual", "proxmox", "container", "lxc", "qemu", "hypervisor"]):
                try:
                    resp = await client.get("http://proxmox-mcp:8000/api/vms")
                    if resp.status_code == 200:
                        data = resp.json()
                        if data.get("status") == "ok":
                            context["proxmox_vms"] = data.get("data", [])
                    resp = await client.get("http://proxmox-mcp:8000/api/nodes")
                    if resp.status_code == 200:
                        data = resp.json()
                        if data.get("status") == "ok":
                            context["proxmox_nodes"] = data.get("data", [])
                except Exception as e:
                    logger.warning(f"Failed to get Proxmox context: {e}")
                    errors.append(f"Proxmox connection failed: {str(e)}")

            # OPNsense context for firewall queries
            if any(kw in prompt_lower for kw in ["firewall", "opnsense", "gateway", "route", "dhcp", "dns", "rule", "nat", "vpn"]):
                try:
                    resp = await client.get("http://opnsense-mcp:8000/api/gateway_status")
                    if resp.status_code == 200:
                        data = resp.json()
                        if data.get("status") == "ok":
                            context["opnsense_gateways"] = data.get("data", [])
                    resp = await client.get("http://opnsense-mcp:8000/api/dhcp_leases")
                    if resp.status_code == 200:
                        data = resp.json()
                        if data.get("status") == "ok":
                            context["opnsense_dhcp_leases"] = data.get("data", [])
                except Exception as e:
                    logger.warning(f"Failed to get OPNsense context: {e}")
                    errors.append(f"OPNsense connection failed: {str(e)}")

            # Coroot context for monitoring queries
            if any(kw in prompt_lower for kw in ["metric", "alert", "anomaly", "monitoring", "performance", "latency", "error rate", "health"]):
                try:
                    resp = await client.post(
                        COROOT_MCP_URL + "/mcp",
                        json={"tool": "get_recent_anomalies", "arguments": {"hours": 24}}
                    )
                    if resp.status_code == 200:
                        data = resp.json()
                        if data.get("success"):
                            context["coroot_anomalies"] = data.get("result", [])
                    resp = await client.post(
                        COROOT_MCP_URL + "/mcp",
                        json={"tool": "get_alerts", "arguments": {}}
                    )
                    if resp.status_code == 200:
                        data = resp.json()
                        if data.get("success"):
                            context["coroot_alerts"] = data.get("result", [])
                except Exception as e:
                    logger.warning(f"Failed to get Coroot context: {e}")
                    errors.append(f"Coroot connection failed: {str(e)}")

            # AdGuard context for DNS/ad blocking queries
            if any(kw in prompt_lower for kw in ["adguard", "dns", "blocking", "ad", "filter", "query log"]):
                try:
                    resp = await client.get("http://adguard-mcp:8000/api/stats")
                    if resp.status_code == 200:
                        data = resp.json()
                        if data.get("status") == "ok":
                            context["adguard_stats"] = data.get("data", {})
                except Exception as e:
                    logger.warning(f"Failed to get AdGuard context: {e}")
                    errors.append(f"AdGuard connection failed: {str(e)}")

            # Runbooks from Qdrant for troubleshooting context
            if any(kw in prompt_lower for kw in ["troubleshoot", "fix", "error", "problem", "issue", "runbook", "how to"]):
                try:
                    resp = await client.post(
                        QDRANT_URL + "/collections/runbooks/points/scroll",
                        json={"limit": 10, "with_payload": True}
                    )
                    if resp.status_code == 200:
                        data = resp.json()
                        points = data.get("result", {}).get("points", [])
                        context["available_runbooks"] = [
                            {"name": p.get("payload", {}).get("name"), "description": p.get("payload", {}).get("description")}
                            for p in points
                        ]
                except:
                    pass

            # Web search for research queries (explicitly requested searches)
            if any(kw in prompt_lower for kw in ["search for", "look up", "find online", "research", "what is the latest", "news about"]):
                try:
                    # Extract search query from prompt
                    search_query = prompt  # Use full prompt as search query
                    resp = await client.post(
                        "http://web-search-mcp:8000/mcp",
                        json={"tool": "web_search", "arguments": {"query": search_query, "num_results": 5}}
                    )
                    if resp.status_code == 200:
                        data = resp.json()
                        if data.get("success"):
                            context["web_search_results"] = data.get("result", [])
                except Exception as e:
                    logger.warning(f"Failed to get web search results: {e}")
                    errors.append(f"Web search failed: {str(e)}")

            # Arr Suite context for media queries
            if any(kw in prompt_lower for kw in ["sonarr", "radarr", "media", "movie", "tv show", "download", "arr"]):
                try:
                    resp = await client.get("http://arr-suite-mcp:8000/api/status")
                    if resp.status_code == 200:
                        data = resp.json()
                        if data.get("status") == "ok":
                            context["arr_suite_status"] = data.get("data", {})
                except Exception as e:
                    logger.warning(f"Failed to get Arr Suite context: {e}")
                    errors.append(f"Arr Suite connection failed: {str(e)}")

        # Include errors so Gemini knows when data wasn't available
        if errors:
            context["_data_fetch_errors"] = errors

        return context

    @app.post("/query")
    async def query_llm(request: QueryRequest):
        """
        Direct query endpoint for LLM access.
        Builds context from MCPs before querying Gemini.
        """
        # Build context from MCPs based on query
        mcp_context = await build_query_context(request.prompt)

        # Merge with any provided context
        full_context = {**(request.context or {}), **mcp_context}

        if request.use_claude:
            result = await call_claude_agent(
                request.prompt,
                context=full_context,
                allowed_tools=["Read", "Glob", "Grep", "WebSearch", "WebFetch"],
                priority=PRIORITY_USER
            )
        else:
            # Build context-enriched prompt for Gemini
            context_str = ""
            errors_str = ""
            has_real_data = False

            if full_context:
                # Extract errors if any
                fetch_errors = full_context.pop("_data_fetch_errors", [])
                if fetch_errors:
                    errors_str = "\n\n## DATA FETCH ERRORS - TELL USER ABOUT THESE:\n" + "\n".join(f"- {e}" for e in fetch_errors) + "\n"
                if full_context:
                    has_real_data = True
                    context_str = f"\n\n## ACTUAL DATA FROM HOMELAB:\n{json.dumps(full_context, indent=2, default=str)}\n\n"

            # Build messages list with conversation history
            llm_messages = []

            # Always include comprehensive system prompt with anti-hallucination rules
            system_content = AGENT_SYSTEM_PROMPT + "\n\n## IMPORTANT RULES FOR THIS QUERY:\n"
            if has_real_data:
                system_content += "- Answer based ONLY on the actual data provided below\n"
                system_content += "- Do not invent any device names, IPs, versions, or statuses\n"
            else:
                system_content += "- CRITICAL: Could not retrieve homelab data\n"
                system_content += "- Tell the user what systems failed to respond\n"
                system_content += "- DO NOT make up, guess, or hallucinate any information\n"
            llm_messages.append({"role": "system", "content": system_content})

            # Build user message with context
            if has_real_data:
                enriched_prompt = f"{errors_str}{context_str}USER QUERY: {request.prompt}"
            else:
                enriched_prompt = (
                    f"CRITICAL: I could not retrieve any data from the homelab systems.\n"
                    f"{errors_str}\n"
                    f"USER QUERY: {request.prompt}\n\n"
                    f"Explain what systems failed to respond and suggest troubleshooting steps."
                )

            # Add conversation history if provided
            if request.messages:
                for msg in request.messages[:-1]:  # Exclude last message (current prompt)
                    llm_messages.append(msg)

            # Add current message with context
            current_content = enriched_prompt if not request.messages else f"{context_str}{errors_str}USER: {request.prompt}"
            llm_messages.append({"role": "user", "content": current_content})

            result = await call_litellm(llm_messages)

        return {"response": result, "conversation_id": request.conversation_id}

    @app.post("/claude/run")
    async def run_claude_task(prompt: str, context: Optional[dict] = None):
        """Direct endpoint to run a Claude Agent task with user priority."""
        result = await call_claude_agent(
            prompt,
            context=context,
            priority=PRIORITY_USER  # User requests get highest priority
        )
        return {"response": result}

    @app.post("/runbook/store")
    async def store_runbook_api(runbook_data: dict):
        """Store a diagnostic runbook via API."""
        try:
            runbook = Runbook(**runbook_data)
            success = await store_runbook_qdrant(runbook)
            if success:
                return {"status": "stored", "runbook_id": runbook.id, "name": runbook.name}
            else:
                raise HTTPException(status_code=500, detail="Failed to store runbook")
        except Exception as e:
            logger.error(f"Failed to store runbook: {e}")
            raise HTTPException(status_code=400, detail=str(e))

    @app.get("/runbook/{runbook_id}")
    async def get_runbook(runbook_id: str):
        """Get a runbook by ID."""
        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                response = await client.get(
                    QDRANT_URL + f"/collections/runbooks/points/{runbook_id}"
                )
                if response.status_code == 200:
                    payload = response.json().get("result", {}).get("payload", {})
                    return payload
                return None
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))

    @app.get("/runbooks")
    async def list_runbooks(limit: int = 20):
        """List all runbooks."""
        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                response = await client.post(
                    QDRANT_URL + "/collections/runbooks/points/scroll",
                    json={"limit": limit, "with_payload": True}
                )
                if response.status_code == 200:
                    points = response.json().get("result", {}).get("points", [])
                    return {
                        "runbooks": [
                            {
                                "id": p.get("id"),
                                "name": p.get("payload", {}).get("name", "Unknown"),
                                "description": p.get("payload", {}).get("description", ""),
                                "version": 2 if "diagnosis" in p.get("payload", {}) else 1,
                                "success_count": p.get("payload", {}).get("metadata", {}).get("success_count", 0),
                                "auto_approve": p.get("payload", {}).get("metadata", {}).get("auto_approve_eligible", False)
                            }
                            for p in points
                        ],
                        "count": len(points)
                    }
                return {"runbooks": [], "count": 0}
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))

    @app.delete("/runbook/{runbook_id}")
    async def delete_runbook(runbook_id: str):
        """Delete a runbook by ID."""
        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                response = await client.post(
                    QDRANT_URL + "/collections/runbooks/points/delete",
                    json={"points": [runbook_id]}
                )
                return {"status": "deleted" if response.status_code == 200 else "failed"}
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))

    class SimpleRunbookRequest(BaseModel):
        """Simple runbook creation - Claude provides all details, no LLM needed."""
        name: str = Field(..., description="Runbook name")
        description: str = Field(..., description="What this runbook handles")
        keywords: List[str] = Field(..., description="Keywords for matching alerts")
        diagnosis_command: str = Field(default="echo 'Check manually'", description="Command to verify issue exists")
        fix_commands: List[str] = Field(default_factory=list, description="Commands to fix the issue")
        validation_command: str = Field(default="echo 'OK'", description="Command to verify fix worked")
        escalate_if: List[str] = Field(default_factory=lambda: ["confidence < 0.7"], description="Conditions to escalate")

    @app.post("/runbook/create-simple")
    async def create_simple_runbook(request: SimpleRunbookRequest):
        """
        Create a runbook from Claude-provided details. No LLM call needed.

        Claude (the architect) creates runbooks after fixing issues.
        Gemini (the operator) uses these runbooks to handle future occurrences.
        """
        logger.info(f"Creating runbook: {request.name}")

        try:
            runbook = Runbook(
                name=request.name,
                description=request.description,
                triggers=TriggerConfig(
                    alert_patterns=[],
                    keywords=request.keywords
                ),
                diagnosis=DiagnosisConfig(
                    checks=[DiagnosisCheck(name="Verify issue", command=request.diagnosis_command)],
                    confidence_rules=[]
                ),
                decision=DecisionRules(
                    escalate_if=request.escalate_if,
                    handle_locally_if=["confidence >= 0.8"]
                ),
                fix=FixConfig(
                    steps=[
                        FixStep(name=f"Step {i+1}", action="command", command=cmd)
                        for i, cmd in enumerate(request.fix_commands)
                    ] if request.fix_commands else [
                        FixStep(name="Manual fix", action="command", command="echo 'Apply fix manually'")
                    ],
                    validation=[ValidationCheck(name="Verify fix", command=request.validation_command)]
                ),
                metadata=RunbookMetadata(
                    created_by="claude-code",
                    created_from="fix-session",
                    auto_approve_eligible=False
                )
            )

            success = await store_runbook_qdrant(runbook)

            if success:
                logger.info(f"Created runbook: {runbook.name} ({runbook.id})")
                return {
                    "status": "created",
                    "runbook_id": runbook.id,
                    "name": runbook.name,
                    "description": runbook.description,
                    "keywords": request.keywords
                }
            else:
                raise HTTPException(status_code=500, detail="Failed to store runbook")

        except Exception as e:
            logger.error(f"Failed to create runbook: {e}")
            raise HTTPException(status_code=500, detail=str(e))

    @app.get("/health")
    async def health():
        return {"status": "healthy"}

    def main():
        port = int(os.environ.get("PORT", "8000"))
        logger.info("Starting LangGraph orchestrator on port %d", port)
        uvicorn.run(app, host="0.0.0.0", port=port)

    if __name__ == "__main__":
        main()

  requirements.txt: |
    langgraph>=0.2.0
    langchain-core>=0.3.0
    langchain-openai>=0.2.0
    asyncpg>=0.30.0
    psycopg>=3.2.0
    psycopg-pool>=3.2.0
    redis>=5.2.0
    fastapi>=0.115.0
    uvicorn>=0.34.0
    httpx>=0.28.0
    pydantic>=2.11.0
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: langgraph
  namespace: ai-platform
  labels:
    app: langgraph
    component: orchestrator
spec:
  replicas: 1
  selector:
    matchLabels:
      app: langgraph
  template:
    metadata:
      labels:
        app: langgraph
        component: orchestrator
    spec:
      initContainers:
        - name: install-deps
          image: python:3.11-slim
          command: ['sh', '-c', 'pip install --target=/app/deps -r /code/requirements.txt']
          volumeMounts:
            - name: code
              mountPath: /code
            - name: deps
              mountPath: /app/deps
      containers:
        - name: langgraph
          image: python:3.11-slim
          command: ['sh', '-c', 'cd /app && PYTHONPATH=/app/deps python /code/main.py']
          ports:
            - containerPort: 8000
              name: http
          env:
            - name: PORT
              value: "8000"
            # Core services
            - name: POSTGRES_URL
              value: "postgresql://postgres:postgres@postgres:5432/langgraph"
            - name: REDIS_URL
              value: "redis://redis:6379"
            - name: LITELLM_URL
              value: "http://litellm:4000"
            - name: QDRANT_URL
              value: "http://qdrant:6333"
            # Claude services (validator runs separately as daily cronjob)
            - name: CLAUDE_AGENT_URL
              value: "http://claude-agent:8000"
            # MCP servers
            - name: KNOWLEDGE_MCP_URL
              value: "http://knowledge-mcp:8000"
            - name: INFRASTRUCTURE_MCP_URL
              value: "http://infrastructure-mcp:8000"
            - name: COROOT_MCP_URL
              value: "http://coroot-mcp:8000"
            # Matrix bot (replaces Telegram)
            - name: MATRIX_BOT_URL
              value: "http://matrix-bot:8000"
            # Model configuration - Gemini only
            - name: GEMINI_MODEL
              value: "gemini/gemini-2.0-pro"
            - name: GEMINI_FLASH_MODEL
              value: "gemini/gemini-2.0-flash"
            - name: EMBEDDING_MODEL
              value: "embeddings"
            # Thresholds
            - name: RUNBOOK_MATCH_THRESHOLD
              value: "0.75"
            - name: CONTEXT_CACHE_TTL
              value: "3600"
          volumeMounts:
            - name: code
              mountPath: /code
            - name: deps
              mountPath: /app/deps
          resources:
            requests:
              memory: "256Mi"
              cpu: "200m"
            limits:
              memory: "512Mi"
              cpu: "1000m"
          readinessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 60
            periodSeconds: 10
            timeoutSeconds: 5
          livenessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 90
            periodSeconds: 30
            timeoutSeconds: 5
      volumes:
        - name: code
          configMap:
            name: langgraph-code
        - name: deps
          emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: langgraph
  namespace: ai-platform
  labels:
    app: langgraph
    component: orchestrator
spec:
  selector:
    app: langgraph
  ports:
    - port: 8000
      targetPort: 8000
      name: http
