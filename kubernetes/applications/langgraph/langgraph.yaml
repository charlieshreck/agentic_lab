apiVersion: v1
kind: ConfigMap
metadata:
  name: langgraph-code
  namespace: ai-platform
  labels:
    app: langgraph
data:
  main.py: |
    #!/usr/bin/env python3
    """LangGraph Orchestrator for Agentic AI Platform."""
    import os
    import logging
    import json
    from typing import TypedDict, Annotated, Sequence, Optional
    from datetime import datetime
    import httpx

    from langgraph.graph import StateGraph, END
    from langchain_core.messages import BaseMessage

    from fastapi import FastAPI, HTTPException
    from pydantic import BaseModel
    import uvicorn

    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

    POSTGRES_URL = os.environ.get("POSTGRES_URL", "postgresql://postgres:postgres@postgres:5432/langgraph")
    REDIS_URL = os.environ.get("REDIS_URL", "redis://redis:6379")
    LITELLM_URL = os.environ.get("LITELLM_URL", "http://litellm:4000")
    QDRANT_URL = os.environ.get("QDRANT_URL", "http://qdrant:6333")
    CLAUDE_AGENT_URL = os.environ.get("CLAUDE_AGENT_URL", "http://claude-agent:8000")
    KNOWLEDGE_MCP_URL = os.environ.get("KNOWLEDGE_MCP_URL", "http://knowledge-mcp:8000")
    INFRASTRUCTURE_MCP_URL = os.environ.get("INFRASTRUCTURE_MCP_URL", "http://infrastructure-mcp:8000")
    TELEGRAM_SERVICE_URL = os.environ.get("TELEGRAM_SERVICE_URL", "http://telegram-service:8000")
    OLLAMA_URL = os.environ.get("OLLAMA_URL", "http://ollama:11434")
    EMBEDDING_MODEL = os.environ.get("EMBEDDING_MODEL", "nomic-embed-text")

    # Complexity threshold for routing to Claude vs local LLM
    CLAUDE_COMPLEXITY_THRESHOLD = os.environ.get("CLAUDE_COMPLEXITY_THRESHOLD", "high")
    RUNBOOK_MATCH_THRESHOLD = float(os.environ.get("RUNBOOK_MATCH_THRESHOLD", "0.75"))

    class AgentState(TypedDict):
        messages: Annotated[Sequence[BaseMessage], lambda x, y: x + y]
        alert: dict
        assessment: dict
        solutions: list
        selected_solution: dict
        approval_status: str
        execution_result: dict
        runbook_id: str
        topic_id: int
        thread_id: str
        runbook_match: dict  # Matched runbook from Qdrant if found
        handled_locally: bool  # Whether issue was handled with existing runbook

    pending_approvals = {}
    app = FastAPI(title="LangGraph Orchestrator")

    class AlertInput(BaseModel):
        id: str
        alertname: str
        severity: str = "warning"
        namespace: str = "default"
        description: str = ""
        labels: Optional[dict] = None
        annotations: Optional[dict] = None

    class ApprovalRequest(BaseModel):
        alert_id: str
        solution_index: int
        approved_by: str

    class IgnoreRequest(BaseModel):
        alert_id: str
        ignored_by: str

    async def call_litellm(messages, model="local/qwen2.5:7b"):
        async with httpx.AsyncClient(timeout=60.0) as client:
            try:
                response = await client.post(
                    LITELLM_URL + "/v1/chat/completions",
                    json={"model": model, "messages": messages}
                )
                response.raise_for_status()
                return response.json()["choices"][0]["message"]["content"]
            except Exception as e:
                logger.error("LiteLLM call failed: %s", e)
                return "Error: " + str(e)

    async def call_knowledge_mcp(endpoint, method="GET", data=None):
        async with httpx.AsyncClient(timeout=30.0) as client:
            try:
                url = KNOWLEDGE_MCP_URL + "/" + endpoint
                if method == "GET":
                    response = await client.get(url)
                else:
                    response = await client.post(url, json=data)
                response.raise_for_status()
                return response.json()
            except Exception as e:
                logger.error("Knowledge MCP call failed: %s", e)
                return {"error": str(e)}

    async def call_telegram(endpoint, data):
        async with httpx.AsyncClient(timeout=30.0) as client:
            try:
                response = await client.post(TELEGRAM_SERVICE_URL + "/" + endpoint, json=data)
                response.raise_for_status()
                return response.json()
            except Exception as e:
                logger.error("Telegram service call failed: %s", e)
                return {"error": str(e)}

    async def get_embedding(text: str):
        """Get embedding from Ollama directly (bypasses LiteLLM bug)."""
        async with httpx.AsyncClient(timeout=30.0) as client:
            try:
                # Call Ollama embeddings API directly
                response = await client.post(
                    OLLAMA_URL + "/api/embeddings",
                    json={"model": EMBEDDING_MODEL, "prompt": text}
                )
                if response.status_code == 200:
                    return response.json().get("embedding", [])
                logger.warning("Ollama embedding returned %d", response.status_code)
                return []
            except Exception as e:
                logger.error("Embedding error: %s", e)
                return []

    async def search_runbooks_qdrant(query: str, limit: int = 3):
        """Search Qdrant for relevant runbooks."""
        embedding = await get_embedding(query)
        if not embedding:
            return []
        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                response = await client.post(
                    QDRANT_URL + "/collections/runbooks/points/search",
                    json={"vector": embedding, "limit": limit, "with_payload": True, "score_threshold": 0.5}
                )
                if response.status_code == 200:
                    return [
                        {
                            "id": hit.get("id"),
                            "score": hit.get("score"),
                            "name": hit.get("payload", {}).get("name", "Unknown"),
                            "description": hit.get("payload", {}).get("description", ""),
                            "steps": hit.get("payload", {}).get("steps", []),
                            "fix_command": hit.get("payload", {}).get("fix_command", "")
                        }
                        for hit in response.json().get("result", [])
                    ]
                return []
            except Exception as e:
                logger.error("Runbook search error: %s", e)
                return []

    async def store_runbook_qdrant(name: str, description: str, steps: list, fix_command: str = None):
        """Store a new runbook in Qdrant."""
        import uuid as uuid_mod
        embedding = await get_embedding(f"{name} {description}")
        if not embedding:
            return False
        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                response = await client.put(
                    QDRANT_URL + "/collections/runbooks/points",
                    json={
                        "points": [{
                            "id": str(uuid_mod.uuid4()),
                            "vector": embedding,
                            "payload": {
                                "name": name,
                                "description": description,
                                "steps": steps,
                                "fix_command": fix_command,
                                "source": "langgraph",
                                "created_at": datetime.utcnow().isoformat()
                            }
                        }]
                    }
                )
                return response.status_code == 200
            except Exception as e:
                logger.error("Runbook store error: %s", e)
                return False

    async def call_claude_agent(prompt: str, context: dict = None, allowed_tools: list = None):
        """
        Call Claude Agent for complex agentic tasks.
        Uses subscription-based Claude Code for full tool access.
        """
        async with httpx.AsyncClient(timeout=300.0) as client:
            try:
                payload = {
                    "prompt": prompt,
                    "context": context or {},
                    "allowed_tools": allowed_tools or ["Read", "Glob", "Grep", "Bash", "WebSearch"],
                    "max_turns": 10,
                    "working_directory": "/workspace"
                }
                response = await client.post(
                    CLAUDE_AGENT_URL + "/agent/run",
                    json=payload
                )
                response.raise_for_status()
                result = response.json()
                if result.get("success"):
                    return result.get("result", "")
                else:
                    logger.error("Claude Agent error: %s", result.get("error"))
                    return "Error: " + result.get("error", "Unknown error")
            except Exception as e:
                logger.error("Claude Agent call failed: %s", e)
                # Fallback to local LLM
                return await call_litellm([{"role": "user", "content": prompt}])

    def should_use_claude(assessment: dict) -> bool:
        """Determine if task should be routed to Claude based on complexity."""
        complexity = assessment.get("complexity", "medium")
        threshold = CLAUDE_COMPLEXITY_THRESHOLD

        complexity_levels = {"low": 1, "medium": 2, "high": 3, "critical": 4}
        threshold_levels = {"low": 1, "medium": 2, "high": 3, "critical": 4}

        return complexity_levels.get(complexity, 2) >= threshold_levels.get(threshold, 3)

    def build_assessment_prompt(alert, similar):
        return """Analyze this alert and provide assessment:
    Alert: """ + str(alert.get("alertname", "unknown")) + """
    Severity: """ + str(alert.get("severity", "warning")) + """
    Description: """ + str(alert.get("description", "")) + """
    Namespace: """ + str(alert.get("namespace", "default")) + """

    Similar runbooks found: """ + str(similar) + """

    Respond with JSON containing: domain, complexity, requires_approval, similar_runbook, similarity_score, recommended_topic"""

    def build_solutions_prompt(alert, assessment):
        return """Generate 2-3 solutions for this alert:
    Alert: """ + str(alert.get("alertname", "unknown")) + """
    Description: """ + str(alert.get("description", "")) + """
    Assessment: """ + str(assessment) + """

    Respond with JSON array containing objects with: name, description, impact, risk, commands"""

    async def assess_alert(state):
        """Assess alert using local LLM first, search for existing runbooks."""
        alert = state["alert"]
        alert_text = f"{alert.get('alertname', '')} {alert.get('description', '')}"

        # Step 1: Search Qdrant for existing runbooks (semantic search)
        logger.info("Searching for matching runbooks...")
        runbooks = await search_runbooks_qdrant(alert_text, limit=3)

        runbook_match = None
        if runbooks and runbooks[0].get("score", 0) >= RUNBOOK_MATCH_THRESHOLD:
            runbook_match = runbooks[0]
            logger.info(f"Found matching runbook: {runbook_match.get('name')} (score: {runbook_match.get('score'):.2f})")

        # Step 2: Use local LLM to assess complexity and domain
        prompt = build_assessment_prompt(alert, runbooks)
        logger.info("Assessing alert with local LLM...")
        result = await call_litellm([{"role": "user", "content": prompt}])

        try:
            # Parse JSON from response
            clean_result = result.strip()
            if "```json" in clean_result:
                clean_result = clean_result.split("```json")[1].split("```")[0]
            elif "```" in clean_result:
                clean_result = clean_result.split("```")[1].split("```")[0]
            assessment = json.loads(clean_result.strip())
        except:
            assessment = {
                "domain": "infrastructure",
                "complexity": "medium",
                "requires_approval": True,
                "similar_runbook": runbook_match.get("name") if runbook_match else None,
                "similarity_score": runbook_match.get("score", 0) if runbook_match else 0,
                "recommended_topic": "infrastructure"
            }

        # If we have a high-confidence runbook match, mark for local handling
        if runbook_match:
            assessment["similar_runbook"] = runbook_match.get("name")
            assessment["similarity_score"] = runbook_match.get("score", 0)
            if runbook_match.get("score", 0) >= RUNBOOK_MATCH_THRESHOLD:
                assessment["complexity"] = "low"  # Known issue = low complexity
                assessment["requires_approval"] = True  # Still ask for approval

        return {
            "assessment": assessment,
            "runbook_match": runbook_match or {},
            "handled_locally": False
        }

    async def generate_solutions(state):
        """Generate solutions - use existing runbook if matched, otherwise local LLM or Claude."""
        alert = state["alert"]
        assessment = state["assessment"]
        runbook_match = state.get("runbook_match", {})

        # If we have a high-confidence runbook match, use it directly
        if runbook_match and runbook_match.get("score", 0) >= RUNBOOK_MATCH_THRESHOLD:
            logger.info(f"Using existing runbook: {runbook_match.get('name')}")
            solutions = [{
                "name": f"Apply Runbook: {runbook_match.get('name')}",
                "description": runbook_match.get("description", "Apply existing runbook"),
                "impact": "low",
                "risk": "low",
                "commands": runbook_match.get("steps", []),
                "fix_command": runbook_match.get("fix_command"),
                "source": "runbook",
                "runbook_id": runbook_match.get("id")
            }]
            return {"solutions": solutions, "handled_locally": True}

        # No runbook match - generate solutions
        prompt = build_solutions_prompt(alert, assessment)

        # Route to Claude for complex/unknown tasks, local LLM for simple ones
        if should_use_claude(assessment):
            logger.info("Routing to Claude Agent for complex solution generation")
            # Ask Claude to also create a runbook for future use
            escalation_prompt = (
                f"{prompt}\n\n"
                "Also provide a runbook for future reference. End with a JSON block containing: "
                "runbook_name, runbook_description, runbook_steps (array), fix_command (optional)."
            )
            result = await call_claude_agent(
                escalation_prompt,
                context={"alert": alert, "assessment": assessment},
                allowed_tools=["Read", "Glob", "Grep", "Bash"]
            )

            # Try to extract and store runbook from Claude's response
            if "runbook_name" in result or "```json" in result:
                try:
                    if "```json" in result:
                        json_str = result.split("```json")[-1].split("```")[0]
                        runbook_data = json.loads(json_str.strip())
                        await store_runbook_qdrant(
                            name=runbook_data.get("runbook_name", f"Alert: {alert.get('alertname')}"),
                            description=runbook_data.get("runbook_description", alert.get("description", "")),
                            steps=runbook_data.get("runbook_steps", []),
                            fix_command=runbook_data.get("fix_command")
                        )
                        logger.info("Stored new runbook from Claude's response")
                except Exception as e:
                    logger.warning(f"Could not extract runbook from response: {e}")
        else:
            logger.info("Using local LLM for solution generation")
            result = await call_litellm([{"role": "user", "content": prompt}])

        try:
            clean_result = result.strip()
            if "```json" in clean_result:
                # Get the first JSON block (solutions), not the runbook
                clean_result = clean_result.split("```json")[1].split("```")[0]
            elif "```" in clean_result:
                clean_result = clean_result.split("```")[1].split("```")[0]
            solutions = json.loads(clean_result.strip())
            if not isinstance(solutions, list):
                solutions = [solutions]
        except:
            solutions = [{
                "name": "Manual Investigation",
                "description": "Investigate the issue manually",
                "impact": "low",
                "risk": "low",
                "commands": []
            }]

        return {"solutions": solutions, "handled_locally": False}

    async def request_approval(state):
        alert = state["alert"]
        solutions = state["solutions"]
        assessment = state["assessment"]
        pending_approvals[alert["id"]] = {
            "alert": alert,
            "solutions": solutions,
            "assessment": assessment,
            "created_at": datetime.utcnow().isoformat()
        }
        result = await call_telegram("approval", {
            "alert_id": alert["id"],
            "alert": alert,
            "solutions": solutions,
            "context": {
                "similar_runbook": assessment.get("similar_runbook"),
                "similarity": assessment.get("similarity_score", 0)
            },
            "topic": assessment.get("recommended_topic", "infrastructure")
        })
        return {"approval_status": "pending", "topic_id": result.get("topic_id", 0)}

    async def execute_solution(state):
        solution = state["selected_solution"]
        if not solution or not solution.get("commands"):
            return {"execution_result": {"success": False, "error": "No commands to execute"}}
        results = []
        for cmd in solution["commands"]:
            async with httpx.AsyncClient(timeout=120.0) as client:
                try:
                    response = await client.post(
                        INFRASTRUCTURE_MCP_URL + "/execute",
                        json={"command": cmd}
                    )
                    results.append(response.json())
                except Exception as e:
                    results.append({"error": str(e)})
        success = all(r.get("success", False) or "error" not in r for r in results)
        return {"execution_result": {"success": success, "results": results, "summary": "Executed " + str(len(results)) + " commands"}}

    async def record_outcome(state):
        await call_knowledge_mcp("record", "POST", {
            "collection": "decisions",
            "data": {
                "alert_id": state["alert"]["id"],
                "alert": state["alert"],
                "assessment": state["assessment"],
                "solution": state["selected_solution"],
                "result": state["execution_result"],
                "approval_status": state["approval_status"],
                "timestamp": datetime.utcnow().isoformat()
            }
        })
        return {}

    def should_request_approval(state):
        assessment = state.get("assessment", {})
        if assessment.get("requires_approval", True):
            return "request_approval"
        return "execute_solution"

    def create_workflow():
        workflow = StateGraph(AgentState)
        workflow.add_node("assess_alert", assess_alert)
        workflow.add_node("generate_solutions", generate_solutions)
        workflow.add_node("request_approval", request_approval)
        workflow.add_node("execute_solution", execute_solution)
        workflow.add_node("record_outcome", record_outcome)
        workflow.set_entry_point("assess_alert")
        workflow.add_edge("assess_alert", "generate_solutions")
        workflow.add_conditional_edges(
            "generate_solutions",
            should_request_approval,
            {"request_approval": "request_approval", "execute_solution": "execute_solution"}
        )
        workflow.add_edge("request_approval", "record_outcome")
        workflow.add_edge("execute_solution", "record_outcome")
        workflow.add_edge("record_outcome", END)
        return workflow.compile()

    graph = create_workflow()

    @app.post("/alert")
    async def process_alert(alert: AlertInput):
        alert_dict = alert.model_dump()
        alert_dict["id"] = alert_dict.get("id") or "alert-" + datetime.utcnow().strftime("%Y%m%d%H%M%S")
        initial_state = {
            "messages": [],
            "alert": alert_dict,
            "assessment": {},
            "solutions": [],
            "selected_solution": {},
            "approval_status": "pending",
            "execution_result": {},
            "runbook_id": "",
            "topic_id": 0,
            "thread_id": alert_dict["id"],
            "runbook_match": {},
            "handled_locally": False
        }
        try:
            result = await graph.ainvoke(initial_state)
            return {
                "alert_id": alert_dict["id"],
                "status": result.get("approval_status", "pending"),
                "assessment": result.get("assessment", {}),
                "solutions_count": len(result.get("solutions", []))
            }
        except Exception as e:
            logger.error("Graph execution failed: %s", e)
            raise HTTPException(status_code=500, detail=str(e))

    @app.post("/approve")
    async def approve_action(request: ApprovalRequest):
        if request.alert_id not in pending_approvals:
            raise HTTPException(status_code=404, detail="Approval not found or expired")
        pending = pending_approvals[request.alert_id]
        solutions = pending["solutions"]
        if request.solution_index < 1 or request.solution_index > len(solutions):
            raise HTTPException(status_code=400, detail="Invalid solution index")
        selected = solutions[request.solution_index - 1]
        state = {
            "messages": [],
            "alert": pending["alert"],
            "assessment": pending["assessment"],
            "solutions": solutions,
            "selected_solution": selected,
            "approval_status": "approved",
            "execution_result": {},
            "runbook_id": "",
            "topic_id": 0,
            "thread_id": request.alert_id
        }
        result = await execute_solution(state)
        state.update(result)
        await record_outcome(state)
        del pending_approvals[request.alert_id]
        return {
            "success": result["execution_result"].get("success", False),
            "summary": result["execution_result"].get("summary", ""),
            "runbook_id": "runbook-" + request.alert_id[:8]
        }

    @app.post("/ignore")
    async def ignore_action(request: IgnoreRequest):
        if request.alert_id in pending_approvals:
            pending = pending_approvals[request.alert_id]
            await call_knowledge_mcp("record", "POST", {
                "collection": "decisions",
                "data": {
                    "alert_id": request.alert_id,
                    "alert": pending["alert"],
                    "action": "ignored",
                    "ignored_by": request.ignored_by,
                    "timestamp": datetime.utcnow().isoformat()
                }
            })
            del pending_approvals[request.alert_id]
        return {"status": "ignored"}

    @app.get("/pending/{alert_id}")
    async def get_pending(alert_id: str):
        if alert_id not in pending_approvals:
            return None
        return pending_approvals[alert_id]

    @app.get("/status")
    async def get_status():
        return {"pending_count": len(pending_approvals), "auto_executed": 0, "active_runbooks": 0, "learning_queue": 0}

    class QueryRequest(BaseModel):
        prompt: str
        use_claude: bool = False
        context: Optional[dict] = None

    @app.post("/query")
    async def query_llm(request: QueryRequest):
        """
        Direct query endpoint for LLM access.
        Routes to Claude Agent or local LLM based on use_claude flag.
        """
        if request.use_claude:
            result = await call_claude_agent(
                request.prompt,
                context=request.context,
                allowed_tools=["Read", "Glob", "Grep", "WebSearch", "WebFetch"]
            )
        else:
            result = await call_litellm([{"role": "user", "content": request.prompt}])
        return {"response": result}

    @app.post("/claude/run")
    async def run_claude_task(prompt: str, context: Optional[dict] = None):
        """Direct endpoint to run a Claude Agent task."""
        result = await call_claude_agent(prompt, context=context)
        return {"response": result}

    @app.get("/health")
    async def health():
        return {"status": "healthy"}

    def main():
        port = int(os.environ.get("PORT", "8000"))
        logger.info("Starting LangGraph orchestrator on port %d", port)
        uvicorn.run(app, host="0.0.0.0", port=port)

    if __name__ == "__main__":
        main()

  requirements.txt: |
    langgraph>=0.2.0
    langchain-core>=0.3.0
    langchain-openai>=0.2.0
    asyncpg>=0.30.0
    psycopg>=3.2.0
    psycopg-pool>=3.2.0
    redis>=5.2.0
    fastapi>=0.115.0
    uvicorn>=0.34.0
    httpx>=0.28.0
    pydantic>=2.11.0
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: langgraph
  namespace: ai-platform
  labels:
    app: langgraph
    component: orchestrator
spec:
  replicas: 1
  selector:
    matchLabels:
      app: langgraph
  template:
    metadata:
      labels:
        app: langgraph
        component: orchestrator
    spec:
      initContainers:
        - name: install-deps
          image: python:3.11-slim
          command: ['sh', '-c', 'pip install --target=/app/deps -r /code/requirements.txt']
          volumeMounts:
            - name: code
              mountPath: /code
            - name: deps
              mountPath: /app/deps
      containers:
        - name: langgraph
          image: python:3.11-slim
          command: ['sh', '-c', 'cd /app && PYTHONPATH=/app/deps python /code/main.py']
          ports:
            - containerPort: 8000
              name: http
          env:
            - name: PORT
              value: "8000"
            - name: POSTGRES_URL
              value: "postgresql://postgres:postgres@postgres:5432/langgraph"
            - name: REDIS_URL
              value: "redis://redis:6379"
            - name: LITELLM_URL
              value: "http://litellm:4000"
            - name: QDRANT_URL
              value: "http://qdrant:6333"
            - name: CLAUDE_AGENT_URL
              value: "http://claude-agent:8000"
            - name: KNOWLEDGE_MCP_URL
              value: "http://knowledge-mcp:8000"
            - name: INFRASTRUCTURE_MCP_URL
              value: "http://infrastructure-mcp:8000"
            - name: TELEGRAM_SERVICE_URL
              value: "http://telegram-service:8000"
            - name: OLLAMA_URL
              value: "http://ollama:11434"
            - name: EMBEDDING_MODEL
              value: "nomic-embed-text"
            - name: RUNBOOK_MATCH_THRESHOLD
              value: "0.75"
          volumeMounts:
            - name: code
              mountPath: /code
            - name: deps
              mountPath: /app/deps
          resources:
            requests:
              memory: "256Mi"
              cpu: "200m"
            limits:
              memory: "512Mi"
              cpu: "1000m"
          readinessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 60
            periodSeconds: 10
            timeoutSeconds: 5
          livenessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 90
            periodSeconds: 30
            timeoutSeconds: 5
      volumes:
        - name: code
          configMap:
            name: langgraph-code
        - name: deps
          emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: langgraph
  namespace: ai-platform
  labels:
    app: langgraph
    component: orchestrator
spec:
  selector:
    app: langgraph
  ports:
    - port: 8000
      targetPort: 8000
      name: http
