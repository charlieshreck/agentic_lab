apiVersion: v1
kind: ConfigMap
metadata:
  name: langgraph-code
  namespace: ai-platform
  labels:
    app: langgraph
data:
  main.py: |
    #!/usr/bin/env python3
    """LangGraph Orchestrator - Kernow Autonomous Operations (KAO).

    Universal incident broker: receives alerts from any source, normalizes them,
    writes to PostgreSQL, checks FP patterns, assesses severity, creates Claude
    screen sessions, and notifies Afferent PWA.
    """
    import os
    import logging
    import json
    import re
    import uuid as uuid_mod
    import hashlib
    from typing import TypedDict, Annotated, Sequence, Optional, List, Dict, Any
    from datetime import datetime, timedelta
    import httpx
    import asyncio
    from textwrap import dedent

    from langgraph.graph import StateGraph, END
    from langchain_core.messages import BaseMessage

    from fastapi import FastAPI, HTTPException, Request
    from pydantic import BaseModel, Field
    import uvicorn

    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

    # ============================================================================
    # CONFIGURATION
    # ============================================================================

    LITELLM_URL = os.environ.get("LITELLM_URL", "http://litellm:4000")
    QDRANT_URL = os.environ.get("QDRANT_URL", "http://qdrant:6333")
    REDIS_URL = os.environ.get("REDIS_URL", "redis://redis:6379")
    KNOWLEDGE_MCP_URL = os.environ.get("KNOWLEDGE_MCP_URL", "http://knowledge-mcp:8000")
    INFRASTRUCTURE_MCP_URL = os.environ.get("INFRASTRUCTURE_MCP_URL", "http://infrastructure-mcp:8000")
    OBSERVABILITY_MCP_URL = os.environ.get("OBSERVABILITY_MCP_URL", "http://observability-mcp:8000")
    EXTERNAL_MCP_URL = os.environ.get("EXTERNAL_MCP_URL", "http://external-mcp:8000")

    # Incident DB - shared PostgreSQL
    INCIDENT_DB_URL = os.environ.get(
        "INCIDENT_DB_URL",
        "postgresql://langgraph:password@postgresql.ai-platform.svc.cluster.local:5432/incident_management"
    )

    # Afferent PWA webhook
    AFFERENT_WEBHOOK_URL = os.environ.get("AFFERENT_WEBHOOK_URL", "")

    # LLM models via LiteLLM
    GEMINI_MODEL = os.environ.get("GEMINI_MODEL", "gemini/gemini-2.0-flash")
    EMBEDDING_MODEL = os.environ.get("EMBEDDING_MODEL", "embeddings")

    # Runbook matching thresholds
    RUNBOOK_EXACT_THRESHOLD = float(os.environ.get("RUNBOOK_EXACT_THRESHOLD", "0.95"))
    RUNBOOK_SIMILAR_THRESHOLD = float(os.environ.get("RUNBOOK_SIMILAR_THRESHOLD", "0.80"))

    # A2A API token for MCP REST bridge
    A2A_API_TOKEN = os.environ.get("A2A_API_TOKEN", "")

    # GitHub integration
    GITHUB_OWNER = os.environ.get("GITHUB_OWNER", "charlieshreck")
    GITHUB_REPO = os.environ.get("GITHUB_REPO", "agentic_lab")

    # ============================================================================
    # POSTGRESQL CONNECTION (asyncpg)
    # ============================================================================

    import asyncpg

    _pg_pool = None

    async def get_pg_pool():
        """Get or create asyncpg connection pool."""
        global _pg_pool
        if _pg_pool is None:
            try:
                _pg_pool = await asyncpg.create_pool(
                    INCIDENT_DB_URL,
                    min_size=2,
                    max_size=10,
                    command_timeout=15,
                )
                logger.info("PostgreSQL pool created for incident_management")
            except Exception as e:
                logger.error(f"Failed to create PostgreSQL pool: {e}")
                raise
        return _pg_pool

    async def pg_insert_incident(data: dict) -> dict:
        """Insert or update incident in PostgreSQL. Returns the row."""
        pool = await get_pg_pool()
        async with pool.acquire() as conn:
            row = await conn.fetchrow("""
                INSERT INTO incidents (
                    external_id, status, severity, source, alert_name,
                    description, fingerprint, enrichment, raw_payload, labels
                ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                ON CONFLICT (external_id) DO UPDATE SET
                    status = EXCLUDED.status,
                    severity = EXCLUDED.severity,
                    description = EXCLUDED.description,
                    enrichment = EXCLUDED.enrichment,
                    raw_payload = EXCLUDED.raw_payload,
                    labels = EXCLUDED.labels,
                    updated_at = NOW()
                RETURNING id, external_id, status, severity, source,
                          alert_name, description, detected_at
            """,
                data.get("external_id"),
                data.get("status", "detected"),
                data.get("severity", "warning"),
                data.get("source", "unknown"),
                data.get("alert_name", "Unknown"),
                data.get("description", ""),
                data.get("fingerprint"),
                json.dumps(data.get("enrichment", {})),
                json.dumps(data.get("raw_payload", {})),
                json.dumps(data.get("labels", {})),
            )
            return dict(row) if row else {}

    async def pg_update_incident(incident_id: int, **kwargs) -> dict:
        """Update incident fields by numeric ID."""
        pool = await get_pg_pool()
        sets = []
        vals = []
        i = 1
        for key, val in kwargs.items():
            if key in ("status", "severity", "screen_session", "model_used",
                        "resolution_type", "resolution_summary", "runbook_id"):
                sets.append(f"{key} = ${i}")
                vals.append(val)
                i += 1
            elif key in ("enrichment", "lessons_learned"):
                sets.append(f"{key} = ${i}")
                vals.append(json.dumps(val))
                i += 1
            elif key in ("resolved_at", "closed_at", "close_scheduled_at"):
                sets.append(f"{key} = ${i}")
                vals.append(val)
                i += 1
        if not sets:
            return {}
        vals.append(incident_id)
        query = f"UPDATE incidents SET {', '.join(sets)} WHERE id = ${i} RETURNING *"
        async with pool.acquire() as conn:
            row = await conn.fetchrow(query, *vals)
            return dict(row) if row else {}

    async def pg_insert_transition(incident_id: int, from_status: str,
                                    to_status: str, actor: str = "langgraph",
                                    note: str = None):
        """Insert audit trail row."""
        pool = await get_pg_pool()
        async with pool.acquire() as conn:
            await conn.execute("""
                INSERT INTO incident_transitions (incident_id, from_status, to_status, actor, note)
                VALUES ($1, $2, $3, $4, $5)
            """, incident_id, from_status, to_status, actor, note)

    async def pg_check_fp_pattern(alert_name: str, source: str) -> dict:
        """Check if alert matches a known false positive pattern."""
        pool = await get_pg_pool()
        async with pool.acquire() as conn:
            row = await conn.fetchrow("""
                SELECT * FROM fp_patterns
                WHERE alert_pattern = $1 AND source = $2
                  AND (suppress_until IS NULL OR suppress_until > NOW())
            """, alert_name, source)
            if row:
                # Bump occurrence count
                await conn.execute("""
                    UPDATE fp_patterns SET occurrences = occurrences + 1
                    WHERE id = $1
                """, row["id"])
                return dict(row)
            return {}

    async def pg_record_fp_pattern(alert_name: str, source: str, reason: str):
        """Record a new FP pattern (or bump count if exists)."""
        pool = await get_pg_pool()
        async with pool.acquire() as conn:
            await conn.execute("""
                INSERT INTO fp_patterns (alert_pattern, source, reason, suppress_until)
                VALUES ($1, $2, $3, NOW() + INTERVAL '7 days')
                ON CONFLICT (alert_pattern, source) DO UPDATE SET
                    occurrences = fp_patterns.occurrences + 1,
                    reason = EXCLUDED.reason,
                    suppress_until = NOW() + INTERVAL '7 days'
            """, alert_name, source, reason)

    # ============================================================================
    # FASTAPI APP
    # ============================================================================

    app = FastAPI(title="KAO LangGraph Orchestrator")

    # ============================================================================
    # INGEST PAYLOAD & SOURCE NORMALIZERS
    # ============================================================================

    class IngestPayload(BaseModel):
        """Universal ingest payload. Source normalizers convert vendor formats."""
        source: str  # keep, alertmanager, gatus, coroot, pulse, pbs, beszel, manual
        alert_name: str
        severity: str = "warning"
        description: str = ""
        fingerprint: Optional[str] = None
        labels: Optional[dict] = None
        raw_payload: Optional[dict] = None
        class Config:
            extra = "allow"

    def normalize_keep(body: dict) -> dict:
        """Normalize Keep alert/incident webhook payload."""
        # Keep sends both alerts and incidents
        if body.get("incident_id"):
            return {
                "source": "keep",
                "alert_name": body.get("incident_name") or body.get("name") or "Keep Incident",
                "severity": body.get("severity") or "warning",
                "description": body.get("user_summary") or body.get("generated_summary") or body.get("description") or "",
                "fingerprint": f"keep-incident-{body['incident_id']}",
                "labels": {
                    "incident_id": body.get("incident_id"),
                    "event": body.get("event"),
                    "alerts_count": body.get("alerts_count"),
                    "status": body.get("status"),
                },
                "raw_payload": body,
            }
        name = body.get("name") or body.get("alertname") or "Keep Alert"
        return {
            "source": "keep",
            "alert_name": name,
            "severity": body.get("severity") or "warning",
            "description": body.get("description") or body.get("message") or "",
            "fingerprint": body.get("fingerprint") or body.get("id") or body.get("alert_hash"),
            "labels": {
                k: body.get(k) for k in
                ("namespace", "pod", "deployment", "service", "container", "cluster")
                if body.get(k)
            },
            "raw_payload": body,
        }

    def normalize_alertmanager(body: dict) -> list:
        """Normalize AlertManager webhook. Returns list (AM sends arrays)."""
        results = []
        alerts = body.get("alerts", [body])
        for alert in alerts:
            labels = alert.get("labels", {})
            annotations = alert.get("annotations", {})
            name = labels.get("alertname", "AlertManager Alert")
            results.append({
                "source": "alertmanager",
                "alert_name": name,
                "severity": labels.get("severity", "warning"),
                "description": annotations.get("description") or annotations.get("summary") or "",
                "fingerprint": alert.get("fingerprint") or hashlib.md5(
                    json.dumps(labels, sort_keys=True).encode()
                ).hexdigest(),
                "labels": labels,
                "raw_payload": alert,
            })
        return results

    def normalize_gatus(body: dict) -> dict:
        """Normalize Gatus webhook payload."""
        return {
            "source": "gatus",
            "alert_name": body.get("endpoint_name") or body.get("name") or "Gatus Endpoint",
            "severity": "critical" if body.get("success") is False else "info",
            "description": body.get("description") or f"Endpoint {body.get('endpoint_name', '?')} status changed",
            "fingerprint": f"gatus-{body.get('endpoint_name', 'unknown')}",
            "labels": {
                "endpoint_group": body.get("endpoint_group"),
                "endpoint_url": body.get("endpoint_url"),
                "success": body.get("success"),
                "condition_results": body.get("condition_results"),
            },
            "raw_payload": body,
        }

    def normalize_coroot(body: dict) -> dict:
        """Normalize Coroot webhook payload."""
        return {
            "source": "coroot",
            "alert_name": body.get("title") or body.get("name") or "Coroot Anomaly",
            "severity": body.get("severity") or "warning",
            "description": body.get("text") or body.get("message") or "",
            "fingerprint": body.get("id") or f"coroot-{body.get('title', 'unknown')}",
            "labels": {
                "application": body.get("application"),
                "namespace": body.get("namespace"),
                "url": body.get("url"),
            },
            "raw_payload": body,
        }

    def normalize_pulse(body: dict) -> dict:
        """Normalize Pulse agent webhook payload."""
        return {
            "source": "pulse",
            "alert_name": body.get("check_name") or body.get("name") or "Pulse Check",
            "severity": body.get("severity") or "warning",
            "description": body.get("message") or body.get("description") or "",
            "fingerprint": body.get("fingerprint") or f"pulse-{body.get('check_name', 'unknown')}",
            "labels": body.get("labels", {}),
            "raw_payload": body,
        }

    def normalize_pbs(body: dict) -> dict:
        """Normalize Proxmox Backup Server webhook."""
        return {
            "source": "pbs",
            "alert_name": body.get("title") or "PBS Alert",
            "severity": body.get("severity") or "warning",
            "description": body.get("message") or body.get("body") or "",
            "fingerprint": f"pbs-{body.get('title', 'unknown')}",
            "labels": {
                "datastore": body.get("datastore"),
                "hostname": body.get("hostname"),
            },
            "raw_payload": body,
        }

    def normalize_beszel(body: dict) -> dict:
        """Normalize Beszel agent webhook."""
        return {
            "source": "beszel",
            "alert_name": body.get("name") or body.get("title") or "Beszel Alert",
            "severity": body.get("severity") or "warning",
            "description": body.get("message") or body.get("description") or "",
            "fingerprint": body.get("id") or f"beszel-{body.get('name', 'unknown')}",
            "labels": body.get("labels", {}),
            "raw_payload": body,
        }

    SOURCE_NORMALIZERS = {
        "keep": normalize_keep,
        "alertmanager": normalize_alertmanager,
        "gatus": normalize_gatus,
        "coroot": normalize_coroot,
        "pulse": normalize_pulse,
        "pbs": normalize_pbs,
        "beszel": normalize_beszel,
    }

    # ============================================================================
    # AFFERENT NOTIFICATION
    # ============================================================================

    async def notify_afferent_ingest(incident_row: dict, extra: dict = None):
        """POST incident to Afferent /webhook/ingest."""
        if not AFFERENT_WEBHOOK_URL:
            return
        base_url = AFFERENT_WEBHOOK_URL.rsplit("/", 1)[0]  # strip old path
        url = f"{base_url}/webhook/ingest"
        payload = {
            "id": incident_row.get("id"),
            "external_id": incident_row.get("external_id"),
            "status": incident_row.get("status"),
            "severity": incident_row.get("severity"),
            "source": incident_row.get("source"),
            "alert_name": incident_row.get("alert_name"),
            "description": incident_row.get("description"),
        }
        if extra:
            payload.update(extra)
        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                resp = await client.post(url, json=payload)
                if resp.status_code < 300:
                    logger.info(f"Afferent notified: incident {incident_row.get('id')}")
                else:
                    logger.warning(f"Afferent notification failed: HTTP {resp.status_code}")
            except Exception as e:
                logger.error(f"Afferent notification error: {e}")

    async def notify_afferent_update(incident_id: int, status: str, note: str = None):
        """POST status update to Afferent /webhook/incident-update."""
        if not AFFERENT_WEBHOOK_URL:
            return
        base_url = AFFERENT_WEBHOOK_URL.rsplit("/", 1)[0]
        url = f"{base_url}/webhook/incident-update"
        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                await client.post(url, json={
                    "incident_id": incident_id,
                    "status": status,
                    "note": note or "",
                })
            except Exception as e:
                logger.error(f"Afferent update error: {e}")

    # ============================================================================
    # LLM CALLS
    # ============================================================================

    async def call_llm(prompt: str, model: str = None) -> str:
        """Call LLM via LiteLLM proxy."""
        model = model or GEMINI_MODEL
        messages = [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": prompt},
        ]
        async with httpx.AsyncClient(timeout=120.0) as client:
            try:
                resp = await client.post(
                    LITELLM_URL + "/v1/chat/completions",
                    json={"model": model, "messages": messages},
                )
                resp.raise_for_status()
                return resp.json()["choices"][0]["message"]["content"]
            except Exception as e:
                logger.error(f"LLM call failed: {e}")
                return f"Error: {e}"

    async def get_embedding(text: str) -> list:
        """Get embedding vector from LiteLLM."""
        async with httpx.AsyncClient(timeout=30.0) as client:
            try:
                resp = await client.post(
                    LITELLM_URL + "/v1/embeddings",
                    json={"model": EMBEDDING_MODEL, "input": text},
                )
                if resp.status_code == 200:
                    data = resp.json()
                    if data.get("data"):
                        return data["data"][0].get("embedding", [])
                return []
            except Exception as e:
                logger.error(f"Embedding error: {e}")
                return []

    SYSTEM_PROMPT = """You are the KAO (Kernow Autonomous Operations) incident assessment engine.

    Your job: Given an alert/incident, determine severity, assess if it matches known
    false positive patterns, and recommend next steps.

    Network: prod=10.10.0.0/24, agentic=10.20.0.0/24, monit=10.30.0.0/24

    Respond with JSON:
    {
      "severity": "critical|warning|info",
      "confidence": 0.0-1.0,
      "assessment": "brief description",
      "recommended_model": "sonnet|opus",
      "is_false_positive": false,
      "fp_reason": null
    }"""

    # ============================================================================
    # MCP TOOL HELPERS
    # ============================================================================

    async def call_mcp_tool(mcp_url: str, tool_name: str, arguments: dict = None) -> dict:
        """Generic MCP tool caller using JSON-RPC protocol."""
        async with httpx.AsyncClient(timeout=30.0) as client:
            try:
                response = await client.post(
                    mcp_url + "/mcp",
                    json={
                        "jsonrpc": "2.0",
                        "id": 1,
                        "method": "tools/call",
                        "params": {"name": tool_name, "arguments": arguments or {}},
                    },
                )
                if response.status_code == 200:
                    result = response.json()
                    content = result.get("result", {}).get("content", [])
                    if content:
                        text_val = content[0].get("text", "{}")
                        try:
                            return json.loads(text_val)
                        except (json.JSONDecodeError, ValueError):
                            return {"text": text_val}
                return {}
            except Exception as e:
                logger.warning(f"MCP tool call failed ({tool_name}): {e}")
                return {}

    MCP_DOMAIN_URLS = {
        "infrastructure": INFRASTRUCTURE_MCP_URL,
        "observability": OBSERVABILITY_MCP_URL,
        "knowledge": KNOWLEDGE_MCP_URL,
    }

    TOOL_CATALOG = {
        "kubectl_delete_pod": {"mcp": "infrastructure", "required": ["namespace", "pod_name"], "risk": "medium"},
        "kubectl_get_pods": {"mcp": "infrastructure", "required": ["namespace"], "risk": "low"},
        "kubectl_logs": {"mcp": "infrastructure", "required": ["pod_name", "namespace"], "risk": "low"},
        "kubectl_restart_deployment": {"mcp": "infrastructure", "required": ["deployment_name", "namespace"], "risk": "medium"},
        "kubectl_scale_deployment": {"mcp": "infrastructure", "required": ["deployment_name", "namespace", "replicas"], "risk": "high"},
        "kubectl_get_deployments": {"mcp": "infrastructure", "required": ["namespace"], "risk": "low"},
        "kubectl_rollout_status": {"mcp": "infrastructure", "required": ["deployment_name", "namespace"], "risk": "low"},
        "kubectl_get_services": {"mcp": "infrastructure", "required": ["namespace"], "risk": "low"},
        "kubectl_get_events": {"mcp": "infrastructure", "required": ["namespace"], "risk": "low"},
        "argocd_sync_application": {"mcp": "infrastructure", "required": ["app_name"], "risk": "medium"},
        "argocd_get_applications": {"mcp": "infrastructure", "required": [], "risk": "low"},
        "create_silence": {"mcp": "observability", "required": ["matchers", "duration"], "risk": "low"},
        "list_alerts": {"mcp": "observability", "required": [], "risk": "low"},
        "query_metrics_instant": {"mcp": "observability", "required": ["query"], "risk": "low"},
    }

    async def call_mcp_rest(mcp_url: str, tool_name: str, arguments: dict) -> dict:
        """Call MCP tool via REST bridge (/api/call)."""
        headers = {"Content-Type": "application/json"}
        if A2A_API_TOKEN:
            headers["Authorization"] = f"Bearer {A2A_API_TOKEN}"
        async with httpx.AsyncClient(timeout=120.0) as client:
            try:
                response = await client.post(
                    f"{mcp_url}/api/call",
                    json={"tool": tool_name, "arguments": arguments},
                    headers=headers,
                )
                result = response.json()
                if response.status_code == 200 and result.get("status") == "success":
                    return {"success": True, "tool": tool_name, "output": result.get("output")}
                else:
                    return {"success": False, "tool": tool_name, "error": result.get("error", f"HTTP {response.status_code}")}
            except Exception as e:
                logger.error(f"REST bridge call failed: {tool_name} - {e}")
                return {"success": False, "tool": tool_name, "error": str(e)}

    # ============================================================================
    # ENRICHMENT - Coroot metrics, runbook search
    # ============================================================================

    async def enrich_incident(alert_name: str, labels: dict, raw_payload: dict) -> dict:
        """Enrich incident with Coroot metrics and runbook matches."""
        enrichment = {}

        # Search for matching runbooks
        try:
            embedding = await get_embedding(f"{alert_name} {json.dumps(labels)}")
            if embedding:
                async with httpx.AsyncClient(timeout=10.0) as client:
                    resp = await client.post(
                        QDRANT_URL + "/collections/runbooks/points/search",
                        json={"vector": embedding, "limit": 3, "with_payload": True, "score_threshold": 0.5},
                    )
                    if resp.status_code == 200:
                        hits = resp.json().get("result", [])
                        enrichment["runbook_matches"] = [
                            {
                                "id": h.get("id"),
                                "score": h.get("score"),
                                "name": h.get("payload", {}).get("name"),
                                "description": h.get("payload", {}).get("description", "")[:200],
                            }
                            for h in hits
                        ]
        except Exception as e:
            logger.warning(f"Runbook search failed: {e}")

        # Fetch Coroot anomalies if source hints at it
        source_url = raw_payload.get("url", "") or ""
        if "coroot" in source_url or labels.get("namespace"):
            try:
                anomalies = await call_mcp_tool(OBSERVABILITY_MCP_URL, "coroot_get_recent_anomalies",
                                                 {"params": {"hours": 24}})
                if anomalies:
                    enrichment["coroot_anomalies"] = anomalies
            except Exception as e:
                logger.warning(f"Coroot anomaly fetch failed: {e}")

        return enrichment

    # ============================================================================
    # INCIDENT GRAPH STATE
    # ============================================================================

    class IncidentState(TypedDict):
        incident_id: int
        external_id: str
        alert_name: str
        severity: str
        source: str
        description: str
        fingerprint: str
        labels: dict
        raw_payload: dict
        enrichment: dict
        # FP check
        fp_match: dict
        # Assessment
        assessment: dict
        recommended_model: str
        # Screen session
        screen_session: str
        # Outcome
        status: str

    # ============================================================================
    # GRAPH NODES
    # ============================================================================

    async def ingest_incident(state: IncidentState) -> dict:
        """Node 1: Write incident to PostgreSQL, enrich, notify Afferent."""
        alert_name = state["alert_name"]
        labels = state.get("labels", {})
        raw_payload = state.get("raw_payload", {})

        # Generate external_id from fingerprint or hash
        fingerprint = state.get("fingerprint") or hashlib.md5(
            f"{state['source']}:{alert_name}:{json.dumps(labels, sort_keys=True)}".encode()
        ).hexdigest()

        # Enrich with Coroot + runbook search
        enrichment = await enrich_incident(alert_name, labels, raw_payload)

        # Insert into PostgreSQL
        row = await pg_insert_incident({
            "external_id": fingerprint,
            "status": "detected",
            "severity": state.get("severity", "warning"),
            "source": state["source"],
            "alert_name": alert_name,
            "description": state.get("description", ""),
            "fingerprint": fingerprint,
            "enrichment": enrichment,
            "raw_payload": raw_payload,
            "labels": labels,
        })

        incident_id = row.get("id", 0)
        logger.info(f"Incident {incident_id} ingested: {alert_name} [{state['source']}]")

        # Record initial transition
        await pg_insert_transition(incident_id, None, "detected", "langgraph", f"Ingested from {state['source']}")

        # Notify Afferent
        await notify_afferent_ingest(row, extra={
            "enrichment": enrichment,
            "labels": labels,
            "raw_payload": raw_payload,
        })

        return {
            "incident_id": incident_id,
            "external_id": fingerprint,
            "fingerprint": fingerprint,
            "enrichment": enrichment,
            "status": "detected",
        }

    async def check_fp_patterns(state: IncidentState) -> dict:
        """Node 2: Check if alert matches a known false positive pattern."""
        alert_name = state["alert_name"]
        source = state["source"]

        fp = await pg_check_fp_pattern(alert_name, source)
        if fp:
            logger.info(f"FP pattern match: {alert_name} from {source} (occurrences: {fp.get('occurrences')})")
            # Auto-suppress: mark as false_positive
            incident_id = state["incident_id"]
            await pg_update_incident(incident_id, status="false_positive",
                                      resolution_type="false_positive",
                                      resolution_summary=f"Auto-suppressed: {fp.get('reason', 'Known FP pattern')}")
            await pg_insert_transition(incident_id, "detected", "false_positive", "langgraph",
                                        f"Auto-suppressed FP (pattern: {fp.get('id')})")
            await notify_afferent_update(incident_id, "false_positive",
                                          f"Auto-suppressed: {fp.get('reason')}")
            return {"fp_match": fp, "status": "false_positive"}

        return {"fp_match": {}, "status": "detected"}

    async def assess_incident(state: IncidentState) -> dict:
        """Node 3: Use LLM to assess severity and recommend model."""
        # Skip if already suppressed as FP
        if state.get("status") == "false_positive":
            return {"assessment": {"skipped": True}, "recommended_model": "none"}

        alert_name = state["alert_name"]
        description = state.get("description", "")
        severity = state.get("severity", "warning")
        enrichment = state.get("enrichment", {})

        prompt = f"""Assess this incident:
    Alert: {alert_name}
    Severity: {severity}
    Source: {state['source']}
    Description: {description}
    Labels: {json.dumps(state.get('labels', {}))}
    Runbook matches: {json.dumps(enrichment.get('runbook_matches', []))}

    Respond with JSON only."""

        try:
            result = await call_llm(prompt)
            # Try to parse JSON from response
            assessment = {}
            try:
                if "```json" in result:
                    json_str = result.split("```json")[1].split("```")[0]
                    assessment = json.loads(json_str.strip())
                elif result.strip().startswith("{"):
                    assessment = json.loads(result.strip())
                else:
                    assessment = {"assessment": result[:500], "confidence": 0.5, "recommended_model": "sonnet"}
            except json.JSONDecodeError:
                assessment = {"assessment": result[:500], "confidence": 0.5, "recommended_model": "sonnet"}

            recommended_model = assessment.get("recommended_model", "sonnet")
            # Override: critical -> opus
            if severity == "critical" or assessment.get("severity") == "critical":
                recommended_model = "opus"

            # Check if LLM thinks it's a false positive
            if assessment.get("is_false_positive"):
                incident_id = state["incident_id"]
                reason = assessment.get("fp_reason", "LLM assessed as false positive")
                await pg_record_fp_pattern(alert_name, state["source"], reason)
                await pg_update_incident(incident_id, status="false_positive",
                                          resolution_type="false_positive",
                                          resolution_summary=reason)
                await pg_insert_transition(incident_id, "detected", "false_positive", "langgraph",
                                            f"LLM FP assessment: {reason}")
                await notify_afferent_update(incident_id, "false_positive", reason)
                return {"assessment": assessment, "recommended_model": "none", "status": "false_positive"}

            # Update incident to investigating
            incident_id = state["incident_id"]
            await pg_update_incident(incident_id, status="investigating",
                                      enrichment={**state.get("enrichment", {}), "assessment": assessment})
            await pg_insert_transition(incident_id, "detected", "investigating", "langgraph",
                                        f"Assessment: confidence={assessment.get('confidence', 0)}, model={recommended_model}")
            await notify_afferent_update(incident_id, "investigating",
                                          f"Assessment complete. Model: {recommended_model}")

            return {
                "assessment": assessment,
                "recommended_model": recommended_model,
                "status": "investigating",
            }

        except Exception as e:
            logger.error(f"Assessment failed: {e}")
            return {
                "assessment": {"error": str(e)},
                "recommended_model": "sonnet",
                "status": "investigating",
            }

    async def create_screen(state: IncidentState) -> dict:
        """Node 4: Create a Claude screen session for the incident (via Afferent)."""
        # Skip if FP
        if state.get("status") == "false_positive":
            return {"screen_session": "", "status": "false_positive"}

        incident_id = state["incident_id"]
        recommended_model = state.get("recommended_model", "sonnet")

        # Update incident with model selection
        await pg_update_incident(incident_id, status="awaiting_review", model_used=recommended_model)
        await pg_insert_transition(incident_id, "investigating", "awaiting_review", "langgraph",
                                    f"Awaiting human review. Recommended model: {recommended_model}")
        await notify_afferent_update(incident_id, "awaiting_review",
                                      f"Ready for review. Recommended: {recommended_model}")

        return {"status": "awaiting_review", "screen_session": ""}

    async def record_outcome(state: IncidentState) -> dict:
        """Node 5: Record outcome to knowledge-mcp."""
        incident_id = state.get("incident_id", 0)
        status = state.get("status", "detected")

        # Log event to knowledge base
        try:
            await call_mcp_tool(KNOWLEDGE_MCP_URL, "log_event", {
                "event_type": "incident.processed",
                "description": f"Incident {incident_id}: {state.get('alert_name')} -> {status}",
                "source_agent": "langgraph",
                "metadata": {
                    "incident_id": incident_id,
                    "alert_name": state.get("alert_name"),
                    "source": state.get("source"),
                    "severity": state.get("severity"),
                    "status": status,
                    "recommended_model": state.get("recommended_model"),
                    "fp_match": bool(state.get("fp_match")),
                },
            })
        except Exception as e:
            logger.warning(f"Failed to log event: {e}")

        return {}

    # ============================================================================
    # GRAPH DEFINITION
    # ============================================================================

    def should_continue_after_fp(state: IncidentState) -> str:
        """After FP check, either stop (FP) or continue to assessment."""
        if state.get("status") == "false_positive":
            return "record_outcome"
        return "assess_incident"

    def should_continue_after_assess(state: IncidentState) -> str:
        """After assessment, either stop (FP) or create screen."""
        if state.get("status") == "false_positive":
            return "record_outcome"
        return "create_screen"

    def create_workflow():
        workflow = StateGraph(IncidentState)

        workflow.add_node("ingest_incident", ingest_incident)
        workflow.add_node("check_fp_patterns", check_fp_patterns)
        workflow.add_node("assess_incident", assess_incident)
        workflow.add_node("create_screen", create_screen)
        workflow.add_node("record_outcome", record_outcome)

        workflow.set_entry_point("ingest_incident")
        workflow.add_edge("ingest_incident", "check_fp_patterns")
        workflow.add_conditional_edges(
            "check_fp_patterns",
            should_continue_after_fp,
            {"assess_incident": "assess_incident", "record_outcome": "record_outcome"},
        )
        workflow.add_conditional_edges(
            "assess_incident",
            should_continue_after_assess,
            {"create_screen": "create_screen", "record_outcome": "record_outcome"},
        )
        workflow.add_edge("create_screen", "record_outcome")
        workflow.add_edge("record_outcome", END)

        return workflow.compile()

    graph = create_workflow()

    # ============================================================================
    # INGEST ENDPOINT
    # ============================================================================

    # Deduplicate recent ingests
    _recent_ingests: dict = {}

    @app.post("/ingest", status_code=202)
    async def ingest(request: Request):
        """Universal ingest endpoint. Accepts any alert source.

        Query param ?source=keep|alertmanager|gatus|coroot|pulse|pbs|beszel|manual
        Body: raw vendor payload (or IngestPayload for source=manual)
        """
        body = await request.json()
        source = request.query_params.get("source", "").lower()

        # Auto-detect source if not specified
        if not source:
            if body.get("incident_id") or body.get("fingerprint"):
                source = "keep"
            elif body.get("alerts") and isinstance(body.get("alerts"), list):
                source = "alertmanager"
            elif body.get("endpoint_name"):
                source = "gatus"
            elif body.get("source"):
                source = body["source"]
            else:
                source = "manual"

        logger.info(f"Ingest: source={source}, keys={list(body.keys())[:10]}")

        # Normalize
        normalizer = SOURCE_NORMALIZERS.get(source)
        if normalizer:
            normalized = normalizer(body)
        else:
            # Manual / unknown: expect IngestPayload format
            normalized = {
                "source": source or "manual",
                "alert_name": body.get("alert_name") or body.get("name") or "Unknown",
                "severity": body.get("severity", "warning"),
                "description": body.get("description", ""),
                "fingerprint": body.get("fingerprint"),
                "labels": body.get("labels", {}),
                "raw_payload": body,
            }

        # AlertManager returns a list
        if isinstance(normalized, list):
            results = []
            for item in normalized:
                result = await _process_single_ingest(item)
                results.append(result)
            return {"status": "accepted", "count": len(results), "incidents": results}
        else:
            result = await _process_single_ingest(normalized)
            return {"status": "accepted", "count": 1, "incidents": [result]}

    async def _process_single_ingest(normalized: dict) -> dict:
        """Process a single normalized ingest payload through the graph."""
        fingerprint = normalized.get("fingerprint") or hashlib.md5(
            f"{normalized['source']}:{normalized['alert_name']}".encode()
        ).hexdigest()

        # Deduplicate: skip if processed within last 60 seconds
        now = datetime.utcnow()
        last_seen = _recent_ingests.get(fingerprint)
        if last_seen and (now - last_seen).total_seconds() < 60:
            logger.info(f"Deduplicated: {normalized['alert_name']} ({fingerprint[:12]})")
            return {"fingerprint": fingerprint, "status": "deduplicated"}
        _recent_ingests[fingerprint] = now

        # Prune old entries
        if len(_recent_ingests) > 200:
            sorted_items = sorted(_recent_ingests.items(), key=lambda x: x[1])
            _recent_ingests.clear()
            for k, v in sorted_items[-100:]:
                _recent_ingests[k] = v

        # Build initial state
        initial_state = {
            "incident_id": 0,
            "external_id": fingerprint,
            "alert_name": normalized.get("alert_name", "Unknown"),
            "severity": normalized.get("severity", "warning"),
            "source": normalized.get("source", "unknown"),
            "description": normalized.get("description", ""),
            "fingerprint": fingerprint,
            "labels": normalized.get("labels", {}),
            "raw_payload": normalized.get("raw_payload", {}),
            "enrichment": {},
            "fp_match": {},
            "assessment": {},
            "recommended_model": "",
            "screen_session": "",
            "status": "detected",
        }

        # Process through graph in background
        asyncio.create_task(_run_graph(initial_state))

        return {
            "fingerprint": fingerprint,
            "alert_name": normalized.get("alert_name"),
            "source": normalized.get("source"),
            "status": "accepted",
        }

    async def _run_graph(initial_state: dict):
        """Run graph in background."""
        try:
            result = await graph.ainvoke(initial_state)
            logger.info(
                f"Incident {result.get('incident_id')} processed: "
                f"{result.get('alert_name')} -> {result.get('status')}"
            )
        except Exception as e:
            logger.error(f"Graph execution failed for {initial_state.get('alert_name')}: {e}")

    # ============================================================================
    # RUNBOOK ENDPOINTS (preserved from existing)
    # ============================================================================

    # -- Runbook Pydantic models --

    class ExtractField(BaseModel):
        field: str
        expected: Optional[Any] = None
        store_as: str

    class LookForPattern(BaseModel):
        pattern: str
        confirms: Optional[str] = None
        indicates: Optional[str] = None

    class DiagnosisCheck(BaseModel):
        name: str
        command: str
        extract: Optional[List[ExtractField]] = None
        look_for: Optional[List[LookForPattern]] = None
        store_as: Optional[str] = None
        timeout: int = 30

    class ConfidenceRule(BaseModel):
        condition: str
        confidence: float = Field(ge=0, le=1)
        diagnosis: str
        note: Optional[str] = None

    class GatherCommand(BaseModel):
        name: str
        command: str
        store_as: str
        on_failure: str = "continue"
        timeout: int = 30

    class Precondition(BaseModel):
        check: str
        reason: str

    class FixStep(BaseModel):
        name: str
        action: str
        command: Optional[str] = None
        formula: Optional[str] = None
        store_as: Optional[str] = None
        rollback: Optional[str] = None
        timeout: int = 60
        on_failure: str = "abort"
        max_value: Optional[str] = None

    class ValidationCheck(BaseModel):
        name: str
        command: str
        expected: Optional[str] = None
        should_not_contain: Optional[str] = None
        wait: int = 0
        retries: int = 1

    class DecisionRules(BaseModel):
        escalate_if: List[str] = Field(default_factory=list)
        handle_locally_if: List[str] = Field(default_factory=list)

    class TriggerConfig(BaseModel):
        alert_patterns: List[str] = Field(default_factory=list)
        keywords: List[str] = Field(default_factory=list)
        severity: List[str] = Field(default_factory=lambda: ["warning", "critical"])

    class DiagnosisConfig(BaseModel):
        description: str = ""
        checks: List[DiagnosisCheck] = Field(default_factory=list)
        confidence_rules: List[ConfidenceRule] = Field(default_factory=list)

    class InfoGatherConfig(BaseModel):
        description: str = ""
        commands: List[GatherCommand] = Field(default_factory=list)

    class FixConfig(BaseModel):
        description: str = ""
        preconditions: List[Precondition] = Field(default_factory=list)
        steps: List[FixStep] = Field(default_factory=list)
        validation: List[ValidationCheck] = Field(default_factory=list)

    class RunbookMetadata(BaseModel):
        created_by: str = "system"
        created_at: str = Field(default_factory=lambda: datetime.utcnow().isoformat())
        updated_at: Optional[str] = None
        success_count: int = 0
        failure_count: int = 0
        false_positive_count: int = 0
        escalation_count: int = 0
        avg_resolution_time: Optional[float] = None
        auto_approve_eligible: bool = False
        tags: List[str] = Field(default_factory=list)
        related_runbooks: List[str] = Field(default_factory=list)

    class Runbook(BaseModel):
        id: str = Field(default_factory=lambda: f"rb-{uuid_mod.uuid4().hex[:8]}")
        name: str
        version: int = 1
        description: str = ""
        triggers: TriggerConfig = Field(default_factory=TriggerConfig)
        diagnosis: DiagnosisConfig = Field(default_factory=DiagnosisConfig)
        information_gathering: InfoGatherConfig = Field(default_factory=InfoGatherConfig)
        decision: DecisionRules = Field(default_factory=DecisionRules)
        fix: FixConfig = Field(default_factory=FixConfig)
        escalation_template: str = ""
        metadata: RunbookMetadata = Field(default_factory=RunbookMetadata)

    class SimpleRunbookRequest(BaseModel):
        name: str
        description: str
        keywords: List[str]
        diagnosis_command: str = "echo 'Check manually'"
        fix_commands: List[str] = Field(default_factory=list)
        validation_command: str = "echo 'OK'"
        escalate_if: List[str] = Field(default_factory=lambda: ["confidence < 0.7"])

    async def store_runbook_qdrant(runbook: Runbook) -> bool:
        """Store runbook in Qdrant."""
        text = f"{runbook.name} {runbook.description} {' '.join(runbook.triggers.keywords)}"
        embedding = await get_embedding(text)
        if not embedding:
            return False
        point_id = str(uuid_mod.uuid5(uuid_mod.NAMESPACE_DNS, runbook.id))
        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                resp = await client.put(
                    QDRANT_URL + "/collections/runbooks/points",
                    json={"points": [{"id": point_id, "vector": embedding, "payload": runbook.model_dump()}]},
                )
                return resp.status_code == 200
            except Exception as e:
                logger.error(f"Runbook store error: {e}")
                return False

    @app.post("/runbook/store")
    async def store_runbook_api(runbook_data: dict):
        try:
            runbook = Runbook(**runbook_data)
            if await store_runbook_qdrant(runbook):
                return {"status": "stored", "runbook_id": runbook.id, "name": runbook.name}
            raise HTTPException(status_code=500, detail="Failed to store runbook")
        except Exception as e:
            raise HTTPException(status_code=400, detail=str(e))

    @app.post("/runbook/create-simple")
    async def create_simple_runbook(request: SimpleRunbookRequest):
        try:
            runbook = Runbook(
                name=request.name,
                description=request.description,
                triggers=TriggerConfig(keywords=request.keywords),
                diagnosis=DiagnosisConfig(
                    checks=[DiagnosisCheck(name="Verify issue", command=request.diagnosis_command)],
                ),
                decision=DecisionRules(
                    escalate_if=request.escalate_if,
                    handle_locally_if=["confidence >= 0.8"],
                ),
                fix=FixConfig(
                    steps=[FixStep(name=f"Step {i+1}", action="command", command=cmd) for i, cmd in enumerate(request.fix_commands)]
                    if request.fix_commands
                    else [FixStep(name="Manual fix", action="command", command="echo 'Apply fix manually'")],
                    validation=[ValidationCheck(name="Verify fix", command=request.validation_command)],
                ),
                metadata=RunbookMetadata(created_by="claude-code", auto_approve_eligible=False),
            )
            if await store_runbook_qdrant(runbook):
                return {"status": "created", "runbook_id": runbook.id, "name": runbook.name}
            raise HTTPException(status_code=500, detail="Failed to store runbook")
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))

    @app.get("/runbooks")
    async def list_runbooks(limit: int = 20):
        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                resp = await client.post(
                    QDRANT_URL + "/collections/runbooks/points/scroll",
                    json={"limit": limit, "with_payload": True},
                )
                if resp.status_code == 200:
                    points = resp.json().get("result", {}).get("points", [])
                    return {
                        "runbooks": [
                            {
                                "id": p.get("id"),
                                "name": p.get("payload", {}).get("name"),
                                "description": p.get("payload", {}).get("description", "")[:200],
                            }
                            for p in points
                        ],
                        "count": len(points),
                    }
                return {"runbooks": [], "count": 0}
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))

    @app.get("/runbook/{runbook_id}")
    async def get_runbook(runbook_id: str):
        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                resp = await client.get(QDRANT_URL + f"/collections/runbooks/points/{runbook_id}")
                if resp.status_code == 200:
                    return resp.json().get("result", {}).get("payload", {})
                return None
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))

    @app.delete("/runbook/{runbook_id}")
    async def delete_runbook(runbook_id: str):
        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                resp = await client.post(
                    QDRANT_URL + "/collections/runbooks/points/delete",
                    json={"points": [runbook_id]},
                )
                return {"status": "deleted" if resp.status_code == 200 else "failed"}
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))

    # ============================================================================
    # STATUS & HEALTH
    # ============================================================================

    @app.get("/health")
    async def health():
        return {"status": "healthy", "version": "5.0-kao"}

    @app.get("/status")
    async def get_status():
        try:
            pool = await get_pg_pool()
            async with pool.acquire() as conn:
                counts = await conn.fetchrow("""
                    SELECT
                        COUNT(*) FILTER (WHERE status NOT IN ('resolved', 'closed', 'false_positive')) AS active,
                        COUNT(*) FILTER (WHERE status = 'false_positive') AS fp_suppressed,
                        COUNT(*) FILTER (WHERE status IN ('resolved', 'closed')) AS resolved,
                        COUNT(*) AS total
                    FROM incidents
                    WHERE detected_at > NOW() - INTERVAL '24 hours'
                """)
                return {
                    "active_incidents": counts["active"],
                    "fp_suppressed_24h": counts["fp_suppressed"],
                    "resolved_24h": counts["resolved"],
                    "total_24h": counts["total"],
                }
        except Exception as e:
            return {"error": str(e)}

    # ============================================================================
    # STARTUP
    # ============================================================================

    @app.on_event("startup")
    async def startup():
        try:
            await get_pg_pool()
            logger.info("PostgreSQL pool ready")
        except Exception as e:
            logger.error(f"PostgreSQL pool failed on startup: {e}")

    def main():
        port = int(os.environ.get("PORT", "8000"))
        logger.info("Starting KAO LangGraph Orchestrator v5.0 on port %d", port)
        uvicorn.run(app, host="0.0.0.0", port=port)

    if __name__ == "__main__":
        main()

  requirements.txt: |
    langgraph>=0.2.0
    langchain-core>=0.3.0
    asyncpg>=0.30.0
    redis>=5.2.0
    fastapi>=0.115.0
    uvicorn>=0.34.0
    httpx>=0.28.0
    pydantic>=2.11.0
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: langgraph
  namespace: ai-platform
  labels:
    app: langgraph
    component: orchestrator
spec:
  replicas: 1
  selector:
    matchLabels:
      app: langgraph
  template:
    metadata:
      labels:
        app: langgraph
        component: orchestrator
    spec:
      initContainers:
        - name: install-deps
          image: python:3.11-slim
          command: ['sh', '-c', 'pip install --target=/app/deps -r /code/requirements.txt']
          volumeMounts:
            - name: code
              mountPath: /code
            - name: deps
              mountPath: /app/deps
      containers:
        - name: langgraph
          image: python:3.11-slim
          command: ['sh', '-c', 'cd /app && PYTHONPATH=/app/deps python /code/main.py']
          ports:
            - containerPort: 8000
              name: http
          env:
            - name: PORT
              value: "8000"
            - name: PYTHONPATH
              value: "/app/deps"
            # Core services
            - name: REDIS_URL
              value: "redis://redis:6379"
            - name: LITELLM_URL
              value: "http://litellm:4000"
            - name: QDRANT_URL
              value: "http://qdrant:6333"
            # Incident DB (PostgreSQL)
            - name: INCIDENT_DB_URL
              valueFrom:
                secretKeyRef:
                  name: incident-db-credentials
                  key: DATABASE_URL
            # Afferent PWA webhook (new endpoints)
            - name: AFFERENT_WEBHOOK_URL
              value: "http://10.10.0.22:3456/webhook/ingest"
            # LLM models
            - name: GEMINI_MODEL
              value: "gemini/gemini-2.0-flash"
            - name: EMBEDDING_MODEL
              value: "embeddings"
            # Runbook thresholds
            - name: RUNBOOK_EXACT_THRESHOLD
              value: "0.95"
            - name: RUNBOOK_SIMILAR_THRESHOLD
              value: "0.80"
            # GitHub
            - name: GITHUB_OWNER
              value: "charlieshreck"
            - name: GITHUB_REPO
              value: "agentic_lab"
            # MCP URLs
            - name: OBSERVABILITY_MCP_URL
              value: "http://observability-mcp:8000"
            - name: EXTERNAL_MCP_URL
              value: "http://external-mcp:8000"
            - name: KNOWLEDGE_MCP_URL
              value: "http://knowledge-mcp:8000"
            - name: INFRASTRUCTURE_MCP_URL
              value: "http://infrastructure-mcp:8000"
            # A2A API token for MCP REST bridge authentication
            - name: A2A_API_TOKEN
              valueFrom:
                secretKeyRef:
                  name: a2a-api-token
                  key: TOKEN
          envFrom:
            - configMapRef:
                name: mcp-servers-config
          volumeMounts:
            - name: code
              mountPath: /code
            - name: deps
              mountPath: /app/deps
          resources:
            requests:
              memory: "256Mi"
              cpu: "200m"
            limits:
              memory: "512Mi"
              cpu: "1000m"
          readinessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 60
            periodSeconds: 10
            timeoutSeconds: 5
          livenessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 90
            periodSeconds: 30
            timeoutSeconds: 5
      volumes:
        - name: code
          configMap:
            name: langgraph-code
        - name: deps
          emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: langgraph
  namespace: ai-platform
  labels:
    app: langgraph
    component: orchestrator
spec:
  selector:
    app: langgraph
  ports:
    - port: 8000
      targetPort: 8000
      name: http
