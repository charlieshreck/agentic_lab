---
apiVersion: v1
kind: ConfigMap
metadata:
  name: error-hunter-code
  namespace: ai-platform
  labels:
    app: error-hunter
data:
  main.py: |
    #!/usr/bin/env python3
    """Error Hunter - Proactive Estate Patrol for Kernow Homelab.

    Scheduled LangGraph workflow that sweeps the entire estate during off-hours,
    finds issues before alerts catch them, dispatches fixes to Claude Code screens,
    writes alert rules, and generates runbooks via collaborate.py.

    Phase 1: Core sweep engine with comprehensive checks, dedup, classification,
    and PostgreSQL persistence. Screen dispatch stubs for Phase 2.
    """
    import os
    import json
    import logging
    import hashlib
    import uuid as uuid_mod
    import asyncio
    import re
    from typing import TypedDict, Optional, List, Dict, Any
    from datetime import datetime, timedelta, timezone

    from langgraph.graph import StateGraph, END
    from fastapi import FastAPI, HTTPException, Request
    from pydantic import BaseModel
    import httpx
    import uvicorn

    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger("error-hunter")

    # ========================================================================
    # CONFIGURATION
    # ========================================================================

    LITELLM_URL = os.environ.get("LITELLM_URL", "http://litellm:4000")
    KNOWLEDGE_MCP_URL = os.environ.get("KNOWLEDGE_MCP_URL", "http://knowledge-mcp:8000")
    INFRASTRUCTURE_MCP_URL = os.environ.get("INFRASTRUCTURE_MCP_URL", "http://infrastructure-mcp:8000")
    OBSERVABILITY_MCP_URL = os.environ.get("OBSERVABILITY_MCP_URL", "http://observability-mcp:8000")
    HOME_MCP_URL = os.environ.get("HOME_MCP_URL", "http://home-mcp:8000")
    MEDIA_MCP_URL = os.environ.get("MEDIA_MCP_URL", "http://media-mcp:8000")
    EXTERNAL_MCP_URL = os.environ.get("EXTERNAL_MCP_URL", "http://external-mcp:8000")

    # PostgreSQL (shared with KAO incident_management DB)
    INCIDENT_DB_URL = os.environ.get(
        "INCIDENT_DB_URL",
        "postgresql://langgraph:password@postgresql.ai-platform.svc.cluster.local:5432/incident_management"
    )

    # Kernow Hub / Synapse for screen dispatch (Phase 2)
    AFFERENT_URL = os.environ.get("AFFERENT_URL", "http://10.10.0.22:3456")
    AFFERENT_AUTH_TOKEN = os.environ.get("AFFERENT_AUTH_TOKEN", "")
    SYNAPSE_URL = os.environ.get("SYNAPSE_URL", "http://10.10.0.22:3460")

    # A2A token for MCP REST bridge
    A2A_API_TOKEN = os.environ.get("A2A_API_TOKEN", "")

    # GitHub token for Renovate PR management
    GITHUB_TOKEN = os.environ.get("GITHUB_TOKEN", "")

    # Repos to check for Renovate updates
    RENOVATE_REPOS = [
        ("charlieshreck", "prod_homelab"),
        ("charlieshreck", "agentic_lab"),
        ("charlieshreck", "monit_homelab"),
    ]

    # Cluster names for infrastructure-mcp kubectl tools (must match KUBECONFIGS keys)
    CLUSTERS = {
        "production": "prod",
        "agentic": "agentic",
        "monitoring": "monit",
    }

    # Namespaces to skip per cluster — prevents cross-cluster duplicate findings
    # (e.g. ai-platform namespace exists on prod for ArgoCD but workloads are on agentic)
    CLUSTER_SKIP_NAMESPACES = {
        "production": {"ai-platform"},
        "monitoring": {"ai-platform"},
        "agentic": set(),
    }

    # Routine CronJob prefixes — failures classified as log_only, not fix_now
    ROUTINE_CRONJOB_PREFIXES = (
        "configarr-", "huntarr-start-", "huntarr-stop-", "mount-canary-writer-",
        "matter-hub-health-", "renovate-", "silverbullet-outline-sync-",
        "alert-forwarder-", "entity-enrichment-", "graph-enrichment-",
        "graph-sync-", "neo4j-backup-", "postgresql-backup-", "qdrant-backup-",
        "redis-backup-", "pattern-detector-", "runbook-indexer-",
        "knowledge-reconcile-", "error-hunter-sweep-", "network-entity-discovery-",
    )

    # Media apps using Recreate strategy (RWO Mayastor PVCs).
    # Coroot fires transient critical alerts during normal rollouts (~25s gap).
    # These are redundant — pod/deployment checks catch real failures.
    # See: agentic_lab/runbooks/media/coroot-recreate-strategy-alerts.md
    MEDIA_RECREATE_APPS = frozenset({
        "cleanuparr", "configarr", "huntarr", "maintainerr", "notifiarr",
        "overseerr", "prowlarr", "radarr", "recomendarr", "sabnzbd",
        "sonarr", "tautulli", "transmission",
    })

    # Max concurrent screens per sweep
    # Minimum actionable findings to justify creating a screen session
    MIN_FINDINGS_FOR_SCREEN = 1

    # Log error patterns to search for
    LOG_ERROR_PATTERNS = [
        r"(?i)OOMKilled",
        r"(?i)out of memory",
        r"(?i)connection refused",
        r"(?i)connection reset",
        r"(?i)connection timed out",
        r"(?i)dial tcp.*: i/o timeout",
        r"(?i)no such host",
        r"(?i)name resolution failure",
        r"(?i)certificate.*expired",
        r"(?i)x509.*certificate",
        r"(?i)permission denied",
        r"(?i)unauthorized",
        r"(?i)403 forbidden",
        r"(?i)500 internal server error",
        r"(?i)502 bad gateway",
        r"(?i)503 service unavailable",
        r"(?i)panic:",
        r"(?i)fatal error",
        r"(?i)segmentation fault",
        r"(?i)killed",
        r"(?i)exec format error",
        r"(?i)image.*not found",
        r"(?i)pull.*access denied",
        r"(?i)back-off restarting",
        r"(?i)readiness probe failed",
        r"(?i)liveness probe failed",
        r"(?i)CrashLoopBackOff",
        r"(?i)ErrImagePull",
        r"(?i)ImagePullBackOff",
        r"(?i)mount.*failed",
        r"(?i)nfs.*stale",
        r"(?i)disk pressure",
        r"(?i)evicted",
    ]
    _compiled_patterns = [re.compile(p) for p in LOG_ERROR_PATTERNS]

    # MCP tool validation matrix: each MCP gets a canary tool call to verify
    # it actually works (not just health endpoint)
    MCP_VALIDATION = {
        "infrastructure": {
            "url_key": "INFRASTRUCTURE_MCP_URL",
            "canary_tools": [
                # kubectl uses flat kwargs — response is markdown, no structured key
                ("kubectl_get_namespaces", {"cluster": "admin@agentic-platform"}, None),
                # proxmox_list_nodes is faster than list_vms (single API call vs all hosts)
                ("proxmox_list_nodes", {"params": {"host": "ruapehu"}}, None),
                ("cloudflare_list_zones", {"params": {}}, None),
                ("get_system_status", {}, None),
                ("truenas_list_pools", {"params": {}}, None),
            ],
        },
        "observability": {
            "url_key": "OBSERVABILITY_MCP_URL",
            "canary_tools": [
                ("gatus_get_failing_endpoints", {}, None),
                ("coroot_get_recent_anomalies", {"hours": 1}, None),
                ("get_scrape_targets", {}, None),
                ("grafana_list_dashboards", {}, None),
            ],
        },
        "home": {
            "url_key": "HOME_MCP_URL",
            "canary_tools": [
                ("list_entities", {}, None),
                ("unifi_list_clients", {}, None),
                ("adguard_get_stats", {}, None),
                ("tasmota_list_devices", {}, None),
            ],
        },
        "media": {
            "url_key": "MEDIA_MCP_URL",
            "canary_tools": [
                ("plex_get_server_status", {}, None),
                ("sonarr_list_series", {}, None),
                ("radarr_list_movies", {}, None),
                ("tautulli_get_activity", {}, None),
            ],
        },
        "knowledge": {
            "url_key": "KNOWLEDGE_MCP_URL",
            "canary_tools": [
                ("list_collections", {}, None),
                ("search_runbooks", {"query": "test"}, None),
            ],
        },
        "external": {
            "url_key": "EXTERNAL_MCP_URL",
            "canary_tools": [
                ("websearch_search", {"query": "test", "num_results": 1}, None),
                ("github_list_repos", {"owner": "charlieshreck", "per_page": 1}, None),
            ],
        },
    }

    # ========================================================================
    # MCP TOOL CALLING (same pattern as alerting-pipeline)
    # ========================================================================

    async def call_mcp_tool(mcp_url: str, tool_name: str, params: dict = None,
                            timeout: float = 30.0) -> dict:
        """Call a tool on a domain MCP server via the REST bridge."""
        headers = {"Content-Type": "application/json"}
        if A2A_API_TOKEN:
            headers["Authorization"] = f"Bearer {A2A_API_TOKEN}"
        async with httpx.AsyncClient(timeout=timeout) as client:
            try:
                resp = await client.post(
                    f"{mcp_url}/api/call",
                    json={"tool": tool_name, "arguments": params or {}},
                    headers=headers,
                )
                if resp.status_code == 200:
                    result = resp.json()
                    if result.get("status") == "success":
                        output = result.get("output")
                        if isinstance(output, dict):
                            return output
                        if isinstance(output, str):
                            try:
                                return json.loads(output)
                            except (json.JSONDecodeError, ValueError):
                                return {"text": output}
                        if isinstance(output, list):
                            return {"items": output}
                        return {"result": output}
                    else:
                        return {"_error": True, "message": result.get("error", "Unknown error"), "status_code": resp.status_code}
                else:
                    return {"_error": True, "message": f"HTTP {resp.status_code}: {resp.text[:200]}", "status_code": resp.status_code}
            except httpx.TimeoutException:
                return {"_error": True, "message": f"Timeout calling {tool_name}", "status_code": 0}
            except Exception as e:
                return {"_error": True, "message": str(e), "status_code": 0}

    async def call_mcp_tool_safe(mcp_url: str, tool_name: str, params: dict = None,
                                  timeout: float = 30.0) -> dict:
        """Like call_mcp_tool but returns {} on error (backward compat)."""
        result = await call_mcp_tool(mcp_url, tool_name, params, timeout)
        if result.get("_error"):
            logger.warning(f"MCP {tool_name} failed: {result.get('message', 'unknown')}")
            return {}
        return result

    # ========================================================================
    # LLM TOOL-CALLING (ReAct engine foundation)
    # ========================================================================

    MAX_REACT_STEPS = 10
    REACT_TIMEOUT = 300  # 5 minutes

    async def call_llm_with_tools(messages: list, tools: list = None,
                                   model: str = "anthropic/claude-haiku-4-5-20251001") -> dict:
        """Call LiteLLM with optional tool definitions and parse the response.

        Returns one of:
        - {"done": True, "summary": "..."}  — LLM says remediation is complete
        - {"tool_call": {"name": ..., "arguments": ...}, "reasoning": "..."}
        - {"error": "..."}
        """
        payload = {
            "model": model,
            "messages": messages,
            "max_tokens": 2048,
            "temperature": 0,
        }
        if tools:
            payload["tools"] = tools
            payload["tool_choice"] = "auto"

        try:
            async with httpx.AsyncClient(timeout=60.0) as client:
                resp = await client.post(
                    f"{LITELLM_URL}/v1/chat/completions",
                    json=payload,
                    headers={"Content-Type": "application/json"},
                )
                if resp.status_code != 200:
                    return {"error": f"LLM returned HTTP {resp.status_code}: {resp.text[:300]}"}

                data = resp.json()
                choice = data.get("choices", [{}])[0]
                message = choice.get("message", {})

                # Check for tool calls
                tool_calls = message.get("tool_calls", [])
                if tool_calls:
                    tc = tool_calls[0]
                    fn = tc.get("function", {})
                    try:
                        args = json.loads(fn.get("arguments", "{}"))
                    except json.JSONDecodeError:
                        args = {}
                    return {
                        "tool_call": {"name": fn.get("name", ""), "arguments": args},
                        "reasoning": message.get("content", "") or "",
                        "tool_call_id": tc.get("id", ""),
                    }

                # No tool call — check if LLM is signaling completion
                content = message.get("content", "")
                if content:
                    # Check for explicit DONE marker or summary
                    lower = content.lower()
                    if "remediation complete" in lower or "##done##" in lower or "no further action" in lower:
                        return {"done": True, "summary": content}
                    # If no tool call and no done signal, treat as reasoning-only step
                    return {"done": True, "summary": content}

                return {"error": "Empty LLM response"}

        except httpx.TimeoutException:
            return {"error": "LLM call timed out"}
        except Exception as e:
            return {"error": f"LLM call failed: {e}"}

    # ========================================================================
    # MCP TOOL CATALOG (curated safe tools for ReAct loop)
    # ========================================================================

    MCP_TOOL_CATALOG = {
        # Infrastructure — reads + safe restarts
        "kubectl_get_pods": INFRASTRUCTURE_MCP_URL,
        "kubectl_logs": INFRASTRUCTURE_MCP_URL,
        "kubectl_get_deployments": INFRASTRUCTURE_MCP_URL,
        "kubectl_get_services": INFRASTRUCTURE_MCP_URL,
        "kubectl_get_events": INFRASTRUCTURE_MCP_URL,
        "kubectl_get_statefulsets": INFRASTRUCTURE_MCP_URL,
        "kubectl_get_configmaps": INFRASTRUCTURE_MCP_URL,
        "kubectl_get_pvcs": INFRASTRUCTURE_MCP_URL,
        "kubectl_get_nodes": INFRASTRUCTURE_MCP_URL,
        "kubectl_restart_deployment": INFRASTRUCTURE_MCP_URL,
        "kubectl_delete_pod": INFRASTRUCTURE_MCP_URL,
        "kubectl_describe": INFRASTRUCTURE_MCP_URL,
        "kubectl_rollout_status": INFRASTRUCTURE_MCP_URL,
        "argocd_get_applications": INFRASTRUCTURE_MCP_URL,
        "argocd_sync_application": INFRASTRUCTURE_MCP_URL,
        "truenas_list_shares": INFRASTRUCTURE_MCP_URL,
        "truenas_get_alerts": INFRASTRUCTURE_MCP_URL,
        "cloudflare_list_tunnels": INFRASTRUCTURE_MCP_URL,
        "cloudflare_get_tunnel_status": INFRASTRUCTURE_MCP_URL,
        # Observability — reads
        "query_metrics": OBSERVABILITY_MCP_URL,
        "query_metrics_instant": OBSERVABILITY_MCP_URL,
        "coroot_get_service_metrics": OBSERVABILITY_MCP_URL,
        "coroot_get_recent_anomalies": OBSERVABILITY_MCP_URL,
        "list_alerts": OBSERVABILITY_MCP_URL,
        "gatus_get_failing_endpoints": OBSERVABILITY_MCP_URL,
        # Home — reads
        "adguard_get_filtering_status": HOME_MCP_URL,
        "get_service_status_summary": HOME_MCP_URL,
        "check_service_health": HOME_MCP_URL,
        # Media — reads + safe actions
        "plex_get_server_status": MEDIA_MCP_URL,
        "plex_get_active_sessions": MEDIA_MCP_URL,
        "sonarr_get_queue": MEDIA_MCP_URL,
        "sonarr_remove_queue_item": MEDIA_MCP_URL,
        "radarr_get_queue": MEDIA_MCP_URL,
        "radarr_remove_queue_item": MEDIA_MCP_URL,
        "transmission_list_torrents": MEDIA_MCP_URL,
        "transmission_remove_torrent": MEDIA_MCP_URL,
        "transmission_pause_torrent": MEDIA_MCP_URL,
        "cleanuparr_health": MEDIA_MCP_URL,
        "cleanuparr_list_jobs": MEDIA_MCP_URL,
        "cleanuparr_trigger_job": MEDIA_MCP_URL,
        "cleanuparr_get_status": MEDIA_MCP_URL,
        "sabnzbd_get_queue": MEDIA_MCP_URL,
        "sabnzbd_get_history": MEDIA_MCP_URL,
        "huntarr_get_status": MEDIA_MCP_URL,
    }

    def route_tool_to_mcp(tool_name: str) -> str:
        """Look up which MCP server handles a given tool."""
        url = MCP_TOOL_CATALOG.get(tool_name)
        if not url:
            raise ValueError(f"Tool '{tool_name}' not in MCP_TOOL_CATALOG")
        return url

    # OpenAI-format tool definitions for the LLM
    MCP_TOOL_DEFINITIONS = [
        # Infrastructure
        {"type": "function", "function": {"name": "kubectl_get_pods", "description": "List pods in a namespace", "parameters": {"type": "object", "properties": {"namespace": {"type": "string", "description": "Kubernetes namespace"}, "cluster": {"type": "string", "description": "Cluster name: prod, agentic, or monit"}}, "required": ["namespace", "cluster"]}}},
        {"type": "function", "function": {"name": "kubectl_logs", "description": "Get logs from a pod", "parameters": {"type": "object", "properties": {"pod_name": {"type": "string"}, "namespace": {"type": "string"}, "cluster": {"type": "string"}, "tail_lines": {"type": "integer", "description": "Number of lines to tail"}}, "required": ["pod_name", "namespace", "cluster"]}}},
        {"type": "function", "function": {"name": "kubectl_get_deployments", "description": "List deployments in a namespace", "parameters": {"type": "object", "properties": {"namespace": {"type": "string"}, "cluster": {"type": "string"}}, "required": ["namespace", "cluster"]}}},
        {"type": "function", "function": {"name": "kubectl_get_events", "description": "Get events in a namespace", "parameters": {"type": "object", "properties": {"namespace": {"type": "string"}, "cluster": {"type": "string"}}, "required": ["namespace", "cluster"]}}},
        {"type": "function", "function": {"name": "kubectl_describe", "description": "Describe a Kubernetes resource", "parameters": {"type": "object", "properties": {"resource_type": {"type": "string", "description": "Resource type (pod, deployment, service, etc.)"}, "name": {"type": "string"}, "namespace": {"type": "string"}, "cluster": {"type": "string"}}, "required": ["resource_type", "name", "namespace", "cluster"]}}},
        {"type": "function", "function": {"name": "kubectl_restart_deployment", "description": "Restart a deployment via rollout restart", "parameters": {"type": "object", "properties": {"deployment_name": {"type": "string"}, "namespace": {"type": "string"}, "cluster": {"type": "string"}}, "required": ["deployment_name", "namespace", "cluster"]}}},
        {"type": "function", "function": {"name": "kubectl_delete_pod", "description": "Delete a pod to trigger restart", "parameters": {"type": "object", "properties": {"pod_name": {"type": "string"}, "namespace": {"type": "string"}, "cluster": {"type": "string"}}, "required": ["pod_name", "namespace", "cluster"]}}},
        {"type": "function", "function": {"name": "kubectl_rollout_status", "description": "Check rollout status of a deployment", "parameters": {"type": "object", "properties": {"deployment_name": {"type": "string"}, "namespace": {"type": "string"}, "cluster": {"type": "string"}}, "required": ["deployment_name", "namespace", "cluster"]}}},
        {"type": "function", "function": {"name": "kubectl_get_services", "description": "List services in a namespace", "parameters": {"type": "object", "properties": {"namespace": {"type": "string"}, "cluster": {"type": "string"}}, "required": ["namespace", "cluster"]}}},
        {"type": "function", "function": {"name": "kubectl_get_statefulsets", "description": "List statefulsets in a namespace", "parameters": {"type": "object", "properties": {"namespace": {"type": "string"}, "cluster": {"type": "string"}}, "required": ["namespace", "cluster"]}}},
        {"type": "function", "function": {"name": "kubectl_get_configmaps", "description": "List configmaps in a namespace", "parameters": {"type": "object", "properties": {"namespace": {"type": "string"}, "cluster": {"type": "string"}}, "required": ["namespace", "cluster"]}}},
        {"type": "function", "function": {"name": "kubectl_get_pvcs", "description": "List persistent volume claims in a namespace", "parameters": {"type": "object", "properties": {"namespace": {"type": "string"}, "cluster": {"type": "string"}}, "required": ["namespace", "cluster"]}}},
        {"type": "function", "function": {"name": "kubectl_get_nodes", "description": "List nodes in a cluster", "parameters": {"type": "object", "properties": {"cluster": {"type": "string"}}, "required": ["cluster"]}}},
        {"type": "function", "function": {"name": "argocd_get_applications", "description": "List all ArgoCD applications and their sync status", "parameters": {"type": "object", "properties": {}}}},
        {"type": "function", "function": {"name": "argocd_sync_application", "description": "Trigger ArgoCD sync for an application", "parameters": {"type": "object", "properties": {"app_name": {"type": "string", "description": "ArgoCD application name"}}, "required": ["app_name"]}}},
        {"type": "function", "function": {"name": "truenas_list_shares", "description": "List NFS/SMB shares on TrueNAS", "parameters": {"type": "object", "properties": {"instance": {"type": "string", "description": "TrueNAS instance: hdd or media"}}, "required": ["instance"]}}},
        {"type": "function", "function": {"name": "truenas_get_alerts", "description": "Get active alerts from TrueNAS", "parameters": {"type": "object", "properties": {"instance": {"type": "string", "description": "TrueNAS instance: hdd or media"}}, "required": ["instance"]}}},
        {"type": "function", "function": {"name": "cloudflare_list_tunnels", "description": "List Cloudflare tunnels", "parameters": {"type": "object", "properties": {}}}},
        {"type": "function", "function": {"name": "cloudflare_get_tunnel_status", "description": "Get status of a Cloudflare tunnel", "parameters": {"type": "object", "properties": {"tunnel_id": {"type": "string"}}, "required": ["tunnel_id"]}}},
        # Observability
        {"type": "function", "function": {"name": "query_metrics", "description": "Query VictoriaMetrics with PromQL over a time range", "parameters": {"type": "object", "properties": {"query": {"type": "string", "description": "PromQL query"}, "start": {"type": "string", "description": "Start time (e.g. '1h' for 1 hour ago)"}, "step": {"type": "string", "description": "Step interval (e.g. '1m')"}}, "required": ["query"]}}},
        {"type": "function", "function": {"name": "query_metrics_instant", "description": "Query VictoriaMetrics with PromQL (instant)", "parameters": {"type": "object", "properties": {"query": {"type": "string", "description": "PromQL query"}}, "required": ["query"]}}},
        {"type": "function", "function": {"name": "coroot_get_service_metrics", "description": "Get service metrics from Coroot", "parameters": {"type": "object", "properties": {"service_name": {"type": "string"}}, "required": ["service_name"]}}},
        {"type": "function", "function": {"name": "coroot_get_recent_anomalies", "description": "Get recent anomalies from Coroot", "parameters": {"type": "object", "properties": {}}}},
        {"type": "function", "function": {"name": "list_alerts", "description": "List active alerts from AlertManager", "parameters": {"type": "object", "properties": {}}}},
        {"type": "function", "function": {"name": "gatus_get_failing_endpoints", "description": "Get failing Gatus endpoints", "parameters": {"type": "object", "properties": {}}}},
        # Home
        {"type": "function", "function": {"name": "adguard_get_filtering_status", "description": "Get AdGuard Home filtering status", "parameters": {"type": "object", "properties": {}}}},
        {"type": "function", "function": {"name": "get_service_status_summary", "description": "Get Homepage service status summary", "parameters": {"type": "object", "properties": {}}}},
        {"type": "function", "function": {"name": "check_service_health", "description": "Check health of a specific service", "parameters": {"type": "object", "properties": {"service_name": {"type": "string"}}, "required": ["service_name"]}}},
        # Media
        {"type": "function", "function": {"name": "plex_get_server_status", "description": "Get Plex server status", "parameters": {"type": "object", "properties": {}}}},
        {"type": "function", "function": {"name": "plex_get_active_sessions", "description": "Get active Plex sessions", "parameters": {"type": "object", "properties": {}}}},
        {"type": "function", "function": {"name": "sonarr_get_queue", "description": "Get Sonarr download queue with queue IDs", "parameters": {"type": "object", "properties": {}}}},
        {"type": "function", "function": {"name": "sonarr_remove_queue_item", "description": "Remove an item from Sonarr queue", "parameters": {"type": "object", "properties": {"queue_id": {"type": "integer", "description": "Queue item ID"}, "remove_from_client": {"type": "boolean", "description": "Also remove from download client"}, "blocklist": {"type": "boolean", "description": "Add to blocklist"}}, "required": ["queue_id"]}}},
        {"type": "function", "function": {"name": "radarr_get_queue", "description": "Get Radarr download queue with queue IDs", "parameters": {"type": "object", "properties": {}}}},
        {"type": "function", "function": {"name": "radarr_remove_queue_item", "description": "Remove an item from Radarr queue", "parameters": {"type": "object", "properties": {"queue_id": {"type": "integer", "description": "Queue item ID"}, "remove_from_client": {"type": "boolean", "description": "Also remove from download client"}, "blocklist": {"type": "boolean", "description": "Add to blocklist"}}, "required": ["queue_id"]}}},
        {"type": "function", "function": {"name": "transmission_list_torrents", "description": "List all torrents in Transmission", "parameters": {"type": "object", "properties": {}}}},
        {"type": "function", "function": {"name": "transmission_remove_torrent", "description": "Remove a torrent from Transmission", "parameters": {"type": "object", "properties": {"torrent_id": {"type": "integer", "description": "Torrent ID"}, "delete_data": {"type": "boolean", "description": "Also delete downloaded data"}}, "required": ["torrent_id"]}}},
        {"type": "function", "function": {"name": "transmission_pause_torrent", "description": "Pause a torrent in Transmission", "parameters": {"type": "object", "properties": {"torrent_id": {"type": "integer", "description": "Torrent ID"}}, "required": ["torrent_id"]}}},
        {"type": "function", "function": {"name": "cleanuparr_health", "description": "Check Cleanuparr health status", "parameters": {"type": "object", "properties": {}}}},
        {"type": "function", "function": {"name": "cleanuparr_list_jobs", "description": "List all Cleanuparr cleanup jobs", "parameters": {"type": "object", "properties": {}}}},
        {"type": "function", "function": {"name": "cleanuparr_trigger_job", "description": "Trigger a Cleanuparr job", "parameters": {"type": "object", "properties": {"job_type": {"type": "string", "description": "Job type: QueueCleaner, DownloadCleaner, MalwareBlocker, BlacklistSynchronizer"}}, "required": ["job_type"]}}},
        {"type": "function", "function": {"name": "cleanuparr_get_status", "description": "Get Cleanuparr system status", "parameters": {"type": "object", "properties": {}}}},
        {"type": "function", "function": {"name": "sabnzbd_get_queue", "description": "Get SABnzbd download queue", "parameters": {"type": "object", "properties": {}}}},
        {"type": "function", "function": {"name": "sabnzbd_get_history", "description": "Get SABnzbd download history", "parameters": {"type": "object", "properties": {"limit": {"type": "integer", "description": "Number of history items"}}}}},
        {"type": "function", "function": {"name": "huntarr_get_status", "description": "Get Huntarr status", "parameters": {"type": "object", "properties": {}}}},
    ]

    # ========================================================================
    # REACT SYSTEM PROMPT + FINDING PROMPT
    # ========================================================================

    def build_react_system_prompt() -> dict:
        """Build the system message for the ReAct remediation loop."""
        return {"role": "system", "content": """You are an infrastructure remediation agent for the Kernow homelab.

    Your job: investigate and fix the issue described, using the available MCP tools.

    Architecture:
    - 3 Kubernetes clusters: prod (10.10.0.0/24), agentic (10.20.0.0/24), monit (10.10.0.0/24)
    - GitOps managed by ArgoCD on prod cluster (manages all 3 clusters)
    - MCP servers run in agentic cluster (ai-platform namespace)
    - OPNsense firewall at 10.10.0.1 (AdGuard DNS, Caddy reverse proxy)
    - TrueNAS-HDD at 10.10.0.103 (NFS storage)

    Rules:
    1. Start by investigating — use read tools (kubectl_get_pods, kubectl_logs, kubectl_get_events, etc.) before taking action
    2. Only restart/delete pods after confirming the issue via investigation
    3. For ArgoCD drift, try argocd_sync_application first
    4. Never make destructive changes — no PVC deletes, no data loss operations
    5. After fixing, verify the fix with another read operation
    6. Keep tool calls focused — don't scan the entire cluster if you know the namespace

    When you're done (fixed or determined unfixable), respond with a clear summary starting with "Remediation complete:" or "##DONE##".
    Include what you found, what you did, and the current state."""}

    def build_finding_prompt(finding: dict) -> str:
        """Build the user message describing the finding to investigate."""
        parts = [
            f"## Finding: {finding.get('check_name', 'unknown')}",
            f"**Severity**: {finding.get('severity', 'unknown')}",
            f"**Cluster**: {finding.get('cluster', 'unknown')}",
            f"**Resource**: {finding.get('resource', 'unknown')}",
            f"**Description**: {finding.get('description', 'No description')}",
        ]
        suggestion = finding.get("suggestion", "")
        if suggestion:
            parts.append(f"**Suggested action**: {suggestion}")
        extra = finding.get("extra", {})
        if extra:
            parts.append(f"**Context**: {json.dumps(extra, default=str)[:500]}")
        parts.append("")
        parts.append("Investigate this issue using the available tools and fix it if possible.")
        return "\n".join(parts)

    async def llm_remediate(finding: dict) -> dict:
        """Tier 2: LLM-guided remediation via ReAct loop.

        LLM reasons about the finding, selects MCP tools, error-hunter
        executes them, returns results, LLM reasons again.
        Max steps and timeout prevent runaway loops.
        """
        messages = [build_react_system_prompt()]
        messages.append({"role": "user", "content": build_finding_prompt(finding)})

        steps = []
        start_time = asyncio.get_event_loop().time()

        for step_num in range(MAX_REACT_STEPS):
            # Check timeout
            elapsed = asyncio.get_event_loop().time() - start_time
            if elapsed > REACT_TIMEOUT:
                logger.warning(f"ReAct timeout after {step_num} steps ({elapsed:.0f}s)")
                return {"success": False, "steps": steps, "summary": f"Timeout after {step_num} steps"}

            response = await call_llm_with_tools(messages, tools=MCP_TOOL_DEFINITIONS)

            if response.get("error"):
                logger.error(f"ReAct step {step_num} LLM error: {response['error']}")
                return {"success": False, "steps": steps, "summary": f"LLM error: {response['error']}"}

            if response.get("done"):
                logger.info(f"ReAct completed in {step_num} steps: {response['summary'][:100]}")
                return {"success": True, "steps": steps, "summary": response["summary"]}

            if response.get("tool_call"):
                tool_name = response["tool_call"]["name"]
                tool_args = response["tool_call"]["arguments"]

                # Validate tool is in catalog
                if tool_name not in MCP_TOOL_CATALOG:
                    logger.warning(f"ReAct requested unknown tool: {tool_name}")
                    messages.append({"role": "assistant", "content": response.get("reasoning", ""),
                                     "tool_calls": [{"id": response.get("tool_call_id", f"call_{step_num}"),
                                                     "type": "function",
                                                     "function": {"name": tool_name, "arguments": json.dumps(tool_args)}}]})
                    messages.append({"role": "tool", "tool_call_id": response.get("tool_call_id", f"call_{step_num}"),
                                     "content": f"Error: tool '{tool_name}' is not available. Available tools: {', '.join(sorted(MCP_TOOL_CATALOG.keys()))}"})
                    continue

                # Execute tool via MCP
                mcp_url = route_tool_to_mcp(tool_name)
                logger.info(f"ReAct step {step_num}: calling {tool_name}({json.dumps(tool_args)[:200]})")
                result = await call_mcp_tool(mcp_url, tool_name, tool_args, timeout=30.0)

                steps.append({"step": step_num, "tool": tool_name, "args": tool_args,
                              "result": str(result)[:1000], "error": result.get("_error", False)})

                # Feed result back to LLM using tool message format
                tool_call_id = response.get("tool_call_id", f"call_{step_num}")
                messages.append({"role": "assistant", "content": response.get("reasoning", ""),
                                 "tool_calls": [{"id": tool_call_id, "type": "function",
                                                 "function": {"name": tool_name, "arguments": json.dumps(tool_args)}}]})
                result_str = json.dumps(result, indent=2, default=str)[:2000]
                messages.append({"role": "tool", "tool_call_id": tool_call_id,
                                 "content": result_str})

        logger.warning(f"ReAct exhausted {MAX_REACT_STEPS} steps")
        return {"success": False, "steps": steps, "summary": f"Max {MAX_REACT_STEPS} steps reached"}

    # ========================================================================
    # POSTGRESQL HELPERS
    # ========================================================================

    _pg_pool = None

    async def get_pg_pool():
        global _pg_pool
        if _pg_pool is None:
            import asyncpg
            _pg_pool = await asyncpg.create_pool(INCIDENT_DB_URL, min_size=1, max_size=5)
        return _pg_pool

    async def pg_execute(query: str, *args):
        pool = await get_pg_pool()
        async with pool.acquire() as conn:
            return await conn.execute(query, *args)

    async def pg_fetchrow(query: str, *args):
        pool = await get_pg_pool()
        async with pool.acquire() as conn:
            return await conn.fetchrow(query, *args)

    async def pg_fetch(query: str, *args):
        pool = await get_pg_pool()
        async with pool.acquire() as conn:
            return await conn.fetch(query, *args)

    async def init_db():
        """Ensure schema extensions exist (idempotent)."""
        # Add incident_id bridge column to error_hunter_findings
        await pg_execute("""
            ALTER TABLE error_hunter_findings
            ADD COLUMN IF NOT EXISTS incident_id INTEGER
        """)

        # Create remediations log table
        await pg_execute("""
            CREATE TABLE IF NOT EXISTS error_hunter_remediations (
                id SERIAL PRIMARY KEY,
                finding_id INTEGER,
                incident_id INTEGER,
                tier INTEGER NOT NULL,
                success BOOLEAN NOT NULL DEFAULT false,
                verified BOOLEAN DEFAULT false,
                steps JSONB DEFAULT '[]',
                summary TEXT,
                started_at TIMESTAMPTZ DEFAULT NOW(),
                completed_at TIMESTAMPTZ
            )
        """)

        # Create unified estate work queue view
        await pg_execute("""
            CREATE OR REPLACE VIEW estate_work_queue AS
            SELECT item_type, id, name, severity, priority, status, description,
                   detected_at, last_updated, resolved_at, incident_id, metadata
            FROM (
                SELECT
                    'finding' as item_type, id, check_name as name, severity,
                    classification as priority, status, description,
                    first_seen as detected_at, last_seen as last_updated,
                    NULL::timestamptz as resolved_at, incident_id,
                    evidence::text as metadata,
                    CASE severity WHEN 'critical' THEN 1 WHEN 'warning' THEN 2 ELSE 3 END as severity_order
                FROM error_hunter_findings
                WHERE status NOT IN ('resolved', 'ignored', 'superseded', 'escalated')
                UNION ALL
                SELECT
                    'incident' as item_type, id, alert_name as name, severity,
                    CASE WHEN severity = 'critical' THEN 'fix_now' ELSE 'create_rule' END as priority,
                    status, description, detected_at, updated_at as last_updated,
                    resolved_at, NULL::integer as incident_id,
                    enrichment::text as metadata,
                    CASE severity WHEN 'critical' THEN 1 WHEN 'warning' THEN 2 ELSE 3 END as severity_order
                FROM incidents
                WHERE status NOT IN ('resolved', 'false_positive', 'closed', 'escalated')
            ) combined
            ORDER BY severity_order, detected_at ASC
        """)

        logger.info("init_db: schema extensions applied")

    async def dedup_incident_to_finding(incident_id: int, alert_name: str) -> Optional[int]:
        """Check if a KAO incident matches an existing error-hunter finding."""
        try:
            rows = await pg_fetch("""
                SELECT id FROM error_hunter_findings
                WHERE check_name ILIKE $1 AND status IN ('new', 'dispatched', 'investigating')
                LIMIT 1
            """, f"%{alert_name}%")
            if rows:
                finding_id = rows[0]["id"]
                await pg_execute(
                    "UPDATE error_hunter_findings SET incident_id = $1 WHERE id = $2",
                    incident_id, finding_id
                )
                logger.info(f"Linked incident {incident_id} to finding {finding_id}")
                return finding_id
        except Exception as e:
            logger.warning(f"Dedup check failed: {e}")
        return None

    # ========================================================================
    # PROGRESS REPORTING
    # ========================================================================

    async def report_progress(sweep_id: str, stage: str, detail: str = "", percent: int = 0):
        """Post sweep progress to Kernow Hub for live dashboard updates."""
        try:
            async with httpx.AsyncClient(timeout=5.0) as client:
                await client.post(
                    f"{AFFERENT_URL}/webhook/patrol-progress",
                    json={"sweep_id": sweep_id, "stage": stage, "detail": detail, "percent": percent},
                )
        except Exception:
            pass  # Non-critical — don't fail the sweep

    # ========================================================================
    # FINDING HELPERS
    # ========================================================================

    def make_fingerprint(check_name: str, cluster: str, resource: str) -> str:
        """Stable fingerprint for deduplication."""
        raw = f"{check_name}:{cluster or 'global'}:{resource or 'none'}"
        return hashlib.sha256(raw.encode()).hexdigest()[:16]

    def make_finding(
        check_category: str,
        check_name: str,
        cluster: str,
        resource: str,
        severity: str,
        description: str,
        evidence: dict = None,
        suggested_fix: str = "",
        alert_target: str = "alerting-pipeline",
        alert_rule_sketch: dict = None,
    ) -> dict:
        return {
            "check_category": check_category,
            "check_name": check_name,
            "cluster": cluster,
            "resource": resource,
            "severity": severity,
            "description": description,
            "evidence": evidence or {},
            "fingerprint": make_fingerprint(check_name, cluster, resource),
            "suggested_fix": suggested_fix,
            "alert_target": alert_target,
            "alert_rule_sketch": alert_rule_sketch or {},
        }

    def safe_int(val, default=0) -> int:
        if isinstance(val, int):
            return val
        if isinstance(val, str):
            try:
                return int(val)
            except ValueError:
                return default
        return default

    def extract_list(result: dict, *keys) -> list:
        """Extract a list from an MCP result, trying multiple key names."""
        for key in keys:
            val = result.get(key)
            if isinstance(val, list):
                return val
        # Maybe the result itself wraps items
        if result.get("items") and isinstance(result["items"], list):
            return result["items"]
        if result.get("result") and isinstance(result["result"], list):
            return result["result"]
        return []

    # ========================================================================
    # CHECK: KUBERNETES (all 3 clusters)
    # ========================================================================

    async def check_kubernetes(cluster_name: str) -> list:
        """Comprehensive Kubernetes health check for a single cluster."""
        findings = []
        infra = INFRASTRUCTURE_MCP_URL
        ctx = CLUSTERS.get(cluster_name, cluster_name)

        # Get all namespaces (skip system + cross-cluster namespaces)
        skip_ns = {"kube-system", "kube-public", "kube-node-lease"} | CLUSTER_SKIP_NAMESPACES.get(cluster_name, set())
        ns_result = await call_mcp_tool_safe(infra, "kubectl_get_namespaces", {"cluster": ctx})
        namespaces = []
        if ns_result:
            for ns in extract_list(ns_result, "namespaces"):
                name = ns.get("name") if isinstance(ns, dict) else str(ns)
                if not name or not isinstance(name, str):
                    continue
                if name not in skip_ns:
                    namespaces.append(name)
        if not namespaces:
            namespaces = ["default"]

        for ns in namespaces:
            # -- PODS: check ALL non-healthy states --
            try:
                pods_result = await call_mcp_tool_safe(infra, "kubectl_get_pods", {
                    "namespace": ns, "cluster": ctx,
                })
                for pod in extract_list(pods_result, "pods"):
                    if not isinstance(pod, dict):
                        continue
                    pod_name = pod.get("name", "unknown")
                    status = pod.get("status", pod.get("phase", ""))
                    restarts = safe_int(pod.get("restarts", pod.get("restart_count", 0)))
                    ready = pod.get("ready", "")

                    # Skip ALL CronJob/Job pods — failures are caught by the job_failed check.
                    # This prevents double-reporting AND noise from old completed/failed Job pods.
                    is_cronjob_pod = any(pod_name.startswith(p) for p in ROUTINE_CRONJOB_PREFIXES)
                    # Also detect Job pods by naming pattern: <name>-<timestamp>-<random>
                    # CronJob pods have a numeric segment (schedule timestamp) before the random suffix
                    is_job_pod = bool(re.match(r'.+-\d{8,}-[a-z0-9]{5}$', pod_name))
                    # Init jobs (e.g. outline-db-init, incident-db-init) are one-shot — skip entirely
                    is_init_job = "-db-init" in pod_name or "-seed" in pod_name or pod_name.endswith("-init")
                    if is_cronjob_pod or is_init_job:
                        continue
                    if is_job_pod and status not in ("Running",):
                        continue

                    # Unhealthy pod states
                    unhealthy = (
                        "CrashLoopBackOff", "Error", "OOMKilled", "ImagePullBackOff",
                        "ErrImagePull", "CreateContainerConfigError", "InvalidImageName",
                        "Pending", "Terminating", "Unknown", "Init:Error",
                        "Init:CrashLoopBackOff", "PodInitializing",
                    )
                    if status and status not in ("Running", "Succeeded", "Completed"):
                        sev = "critical" if status in ("CrashLoopBackOff", "Error", "OOMKilled", "ImagePullBackOff") else "warning"
                        findings.append(make_finding(
                            "kubernetes", "pod_not_running", cluster_name,
                            f"{ns}/{pod_name}", sev,
                            f"Pod {pod_name} in {ns} is {status} (restarts: {restarts})",
                            {"pod": pod_name, "namespace": ns, "status": status, "restarts": restarts, "ready": ready},
                            f"kubectl -n {ns} describe pod {pod_name} && kubectl -n {ns} logs {pod_name} --tail=50",
                            "prometheus",
                        ))

                    # Running but not ready (probes failing)
                    elif status == "Running" and ready and "/" in str(ready):
                        parts = str(ready).split("/")
                        if len(parts) == 2:
                            current, total = safe_int(parts[0]), safe_int(parts[1])
                            if total > 0 and current < total:
                                findings.append(make_finding(
                                    "kubernetes", "pod_not_ready", cluster_name,
                                    f"{ns}/{pod_name}", "warning",
                                    f"Pod {pod_name} in {ns} is Running but not ready ({ready})",
                                    {"pod": pod_name, "namespace": ns, "ready": ready, "restarts": restarts},
                                    f"Check readiness probe: kubectl -n {ns} describe pod {pod_name}",
                                    "prometheus",
                                ))

                    # High restart count — threshold raised to 50 to ignore long-running pods
                    # with accumulated restarts. Rate-based alerting is better (Prometheus handles that).
                    if restarts >= 50:
                        findings.append(make_finding(
                            "kubernetes", "pod_restart_trending", cluster_name,
                            f"{ns}/{pod_name}", "warning",
                            f"Pod {pod_name} in {ns} has {restarts} restarts",
                            {"pod": pod_name, "namespace": ns, "restarts": restarts, "status": status},
                            "Check pod events and previous logs: kubectl -n {ns} logs {pod_name} --previous",
                            "prometheus",
                            {"type": "prometheus", "alert_name": "HomelabPodRestartsTrending",
                             "expr": f'increase(kube_pod_container_status_restarts_total{{namespace="{ns}"}}[6h]) > 5',
                             "for": "30m", "severity": "warning"},
                        ))
            except Exception as e:
                logger.error(f"Pod check error {cluster_name}/{ns}: {e}")

            # -- DEPLOYMENTS: unavailable + progressing stuck --
            try:
                dep_result = await call_mcp_tool_safe(infra, "kubectl_get_deployments", {
                    "namespace": ns, "cluster": ctx,
                })
                for dep in extract_list(dep_result, "deployments"):
                    if not isinstance(dep, dict):
                        continue
                    dep_name = dep.get("name", "unknown")
                    desired = safe_int(dep.get("replicas", dep.get("desired", 0)))
                    available = safe_int(dep.get("available", dep.get("available_replicas", 0)))
                    ready_r = safe_int(dep.get("ready", dep.get("ready_replicas", 0)))
                    updated = safe_int(dep.get("updated", dep.get("updated_replicas", 0)))

                    if desired > 0 and available < desired:
                        findings.append(make_finding(
                            "kubernetes", "deployment_unavailable", cluster_name,
                            f"{ns}/{dep_name}", "warning",
                            f"Deployment {dep_name} in {ns}: {available}/{desired} available, {ready_r} ready, {updated} updated",
                            {"deployment": dep_name, "namespace": ns, "desired": desired,
                             "available": available, "ready": ready_r, "updated": updated},
                            f"kubectl -n {ns} describe deploy {dep_name} && kubectl -n {ns} get rs -l app={dep_name}",
                            "prometheus",
                        ))

                    # Progressing but not completing (rollout stuck)
                    if desired > 0 and updated < desired and available == desired:
                        findings.append(make_finding(
                            "kubernetes", "deployment_rollout_stuck", cluster_name,
                            f"{ns}/{dep_name}", "warning",
                            f"Deployment {dep_name} in {ns} appears stuck mid-rollout: {updated}/{desired} updated",
                            {"deployment": dep_name, "namespace": ns, "updated": updated, "desired": desired},
                            f"kubectl -n {ns} rollout status deploy {dep_name}",
                            "alerting-pipeline",
                        ))
            except Exception as e:
                logger.error(f"Deployment check error {cluster_name}/{ns}: {e}")

            # -- STATEFULSETS --
            try:
                sts_result = await call_mcp_tool_safe(infra, "kubectl_get_statefulsets", {
                    "namespace": ns, "cluster": ctx,
                })
                for sts in extract_list(sts_result, "statefulsets"):
                    if not isinstance(sts, dict):
                        continue
                    sts_name = sts.get("name", "unknown")
                    desired = safe_int(sts.get("replicas", 0))
                    ready_r = safe_int(sts.get("ready", sts.get("ready_replicas", 0)))
                    if desired > 0 and ready_r < desired:
                        findings.append(make_finding(
                            "kubernetes", "statefulset_not_ready", cluster_name,
                            f"{ns}/{sts_name}", "warning",
                            f"StatefulSet {sts_name} in {ns}: {ready_r}/{desired} ready",
                            {"statefulset": sts_name, "namespace": ns, "desired": desired, "ready": ready_r},
                            f"kubectl -n {ns} describe sts {sts_name}",
                            "prometheus",
                        ))
            except Exception as e:
                logger.error(f"StatefulSet check error {cluster_name}/{ns}: {e}")

            # -- FAILED JOBS --
            try:
                jobs_result = await call_mcp_tool_safe(infra, "kubectl_get_jobs", {
                    "namespace": ns, "cluster": ctx,
                })
                for job in extract_list(jobs_result, "jobs"):
                    if not isinstance(job, dict):
                        continue
                    job_name = job.get("name", "unknown")
                    failed = safe_int(job.get("failed", 0))
                    if failed <= 0:
                        continue
                    # Skip routine CronJob failures — these are normal retries
                    if any(job_name.startswith(p) for p in ROUTINE_CRONJOB_PREFIXES):
                        continue
                    findings.append(make_finding(
                        "kubernetes", "job_failed", cluster_name,
                        f"{ns}/{job_name}", "warning",
                        f"Job {job_name} in {ns} has {failed} failure(s)",
                        {"job": job_name, "namespace": ns, "failed": failed},
                        f"kubectl -n {ns} logs job/{job_name}",
                        "prometheus",
                    ))
            except Exception as e:
                logger.error(f"Job check error {cluster_name}/{ns}: {e}")

            # -- WARNING EVENTS --
            try:
                events_result = await call_mcp_tool_safe(infra, "kubectl_get_events", {
                    "namespace": ns, "cluster": ctx,
                })
                events = extract_list(events_result, "events")
                warning_events = [e for e in events if isinstance(e, dict) and e.get("type", "").lower() == "warning"]
                if len(warning_events) > 10:
                    reasons = {}
                    for evt in warning_events:
                        reason = evt.get("reason", "Unknown")
                        reasons[reason] = reasons.get(reason, 0) + 1
                    top_reasons = sorted(reasons.items(), key=lambda x: x[1], reverse=True)[:5]
                    findings.append(make_finding(
                        "kubernetes", "warning_events_accumulating", cluster_name,
                        f"{ns}/events", "warning",
                        f"{len(warning_events)} warning events in {ns}. Top: {', '.join(f'{r}({c})' for r, c in top_reasons)}",
                        {"namespace": ns, "count": len(warning_events), "top_reasons": dict(top_reasons)},
                        "Review events for persistent issues",
                        "alerting-pipeline",
                    ))
            except Exception as e:
                logger.error(f"Events check error {cluster_name}/{ns}: {e}")

        # -- ARGOCD (only from production, manages all 3 clusters) --
        if cluster_name == "production":
            try:
                apps_result = await call_mcp_tool_safe(infra, "argocd_get_applications")
                for app in extract_list(apps_result, "applications"):
                    if not isinstance(app, dict):
                        continue
                    app_name = app.get("name", "unknown")
                    sync = app.get("sync_status", app.get("syncStatus", ""))
                    health = app.get("health_status", app.get("healthStatus", ""))

                    if sync == "OutOfSync":
                        findings.append(make_finding(
                            "kubernetes", "argocd_out_of_sync", "production",
                            f"argocd/{app_name}", "warning",
                            f"ArgoCD app '{app_name}' is OutOfSync (health: {health})",
                            {"app": app_name, "sync": sync, "health": health},
                            "Check ArgoCD UI or run: argocd app sync " + app_name,
                            "alerting-pipeline",
                        ))

                    if health in ("Degraded", "Missing", "Progressing"):
                        sev = "critical" if health == "Degraded" else "warning"
                        findings.append(make_finding(
                            "kubernetes", "argocd_unhealthy", "production",
                            f"argocd/{app_name}", sev,
                            f"ArgoCD app '{app_name}' health: {health} (sync: {sync})",
                            {"app": app_name, "sync": sync, "health": health},
                            "Investigate: argocd app get " + app_name,
                            "alerting-pipeline",
                        ))
            except Exception as e:
                logger.error(f"ArgoCD check error: {e}")

        # -- NODES --
        try:
            nodes_result = await call_mcp_tool_safe(infra, "kubectl_get_nodes", {"cluster": ctx})
            for node in extract_list(nodes_result, "nodes"):
                if not isinstance(node, dict):
                    continue
                node_name = node.get("name", "unknown")
                status = node.get("status", "")
                if status and status != "Ready":
                    findings.append(make_finding(
                        "kubernetes", "node_not_ready", cluster_name,
                        f"node/{node_name}", "critical",
                        f"Node {node_name} in {cluster_name} is {status}",
                        {"node": node_name, "status": status},
                        f"talosctl -n {node_name} health",
                        "alerting-pipeline",
                    ))
        except Exception as e:
            logger.error(f"Node check error {cluster_name}: {e}")

        return findings

    # ========================================================================
    # CHECK: LOG TRAWLING (scan pod logs for errors)
    # ========================================================================

    async def check_logs(cluster_name: str) -> list:
        """Trawl pod logs across all namespaces looking for error patterns."""
        findings = []
        infra = INFRASTRUCTURE_MCP_URL
        ctx = CLUSTERS.get(cluster_name, cluster_name)

        # Get namespaces (skip system + cross-cluster)
        skip_ns = {"kube-system", "kube-public", "kube-node-lease"} | CLUSTER_SKIP_NAMESPACES.get(cluster_name, set())
        ns_result = await call_mcp_tool_safe(infra, "kubectl_get_namespaces", {"cluster": ctx})
        namespaces = []
        for ns in extract_list(ns_result, "namespaces"):
            name = ns.get("name") if isinstance(ns, dict) else str(ns)
            if not name or not isinstance(name, str):
                continue
            if name not in skip_ns:
                namespaces.append(name)
        if not namespaces:
            return findings

        for ns in namespaces:
            # Get pods in this namespace
            pods_result = await call_mcp_tool_safe(infra, "kubectl_get_pods", {
                "namespace": ns, "cluster": ctx,
            })
            pods = extract_list(pods_result, "pods")

            for pod in pods:
                if not isinstance(pod, dict):
                    continue
                pod_name = pod.get("name", "unknown")
                status = pod.get("status", "")

                # Skip completed/succeeded pods
                if status in ("Succeeded", "Completed"):
                    continue

                # Get last 100 lines of logs
                try:
                    log_result = await call_mcp_tool(infra, "kubectl_logs", {
                        "pod_name": pod_name,
                        "namespace": ns,
                        "cluster": ctx,
                        "tail_lines": 100,
                    }, timeout=15.0)

                    if log_result.get("_error"):
                        continue

                    log_text = log_result.get("text", log_result.get("logs", log_result.get("output", "")))
                    if not isinstance(log_text, str) or not log_text.strip():
                        continue

                    # Scan for error patterns
                    matched_patterns = set()
                    error_lines = []
                    for line in log_text.split("\n"):
                        for i, pattern in enumerate(_compiled_patterns):
                            if pattern.search(line):
                                pattern_name = LOG_ERROR_PATTERNS[i]
                                if pattern_name not in matched_patterns:
                                    matched_patterns.add(pattern_name)
                                    error_lines.append(line.strip()[:200])

                    if matched_patterns:
                        # Determine severity from pattern types
                        critical_patterns = {"OOMKilled", "panic:", "fatal error", "segmentation fault"}
                        has_critical = any(p in str(matched_patterns) for p in critical_patterns)
                        sev = "critical" if has_critical else "warning"

                        findings.append(make_finding(
                            "logs", "log_error_pattern", cluster_name,
                            f"{ns}/{pod_name}", sev,
                            f"Pod {pod_name} in {ns}: {len(matched_patterns)} error pattern(s) found: {', '.join(list(matched_patterns)[:5])}",
                            {"pod": pod_name, "namespace": ns,
                             "patterns": list(matched_patterns),
                             "sample_lines": error_lines[:5]},
                            f"kubectl -n {ns} logs {pod_name} --tail=200",
                            "alerting-pipeline",
                        ))
                except Exception as e:
                    # Log fetch errors are expected for some pods
                    pass

        return findings

    # ========================================================================
    # CHECK: MCP TOOL VALIDATION (maintenance hunter)
    # ========================================================================

    async def check_mcp_health() -> list:
        """Validate each MCP server by calling canary tools.

        Not just 'is the HTTP endpoint up' but 'can it actually call its
        backend APIs'. This catches auth failures, connectivity issues,
        stale configs, and API changes.
        """
        findings = []

        mcp_urls = {
            "infrastructure": INFRASTRUCTURE_MCP_URL,
            "observability": OBSERVABILITY_MCP_URL,
            "home": HOME_MCP_URL,
            "media": MEDIA_MCP_URL,
            "knowledge": KNOWLEDGE_MCP_URL,
            "external": EXTERNAL_MCP_URL,
        }

        for mcp_name, config in MCP_VALIDATION.items():
            mcp_url = mcp_urls.get(mcp_name, "")
            if not mcp_url:
                continue

            # First check HTTP health endpoint
            try:
                async with httpx.AsyncClient(timeout=10.0) as client:
                    resp = await client.get(f"{mcp_url}/health")
                    if resp.status_code >= 500:
                        findings.append(make_finding(
                            "mcp_health", "mcp_http_down", "agentic",
                            f"mcp/{mcp_name}", "critical",
                            f"MCP {mcp_name} health endpoint returned HTTP {resp.status_code}",
                            {"mcp": mcp_name, "url": mcp_url, "status_code": resp.status_code},
                            f"kubectl -n ai-platform logs deploy/{mcp_name}-mcp --tail=50",
                            "alerting-pipeline",
                        ))
                        continue  # Skip tool validation if HTTP is down
            except Exception as e:
                findings.append(make_finding(
                    "mcp_health", "mcp_unreachable", "agentic",
                    f"mcp/{mcp_name}", "critical",
                    f"MCP {mcp_name} unreachable: {str(e)[:100]}",
                    {"mcp": mcp_name, "url": mcp_url, "error": str(e)},
                    f"kubectl -n ai-platform get pods -l app={mcp_name}-mcp",
                    "alerting-pipeline",
                ))
                continue

            # Now validate each canary tool
            for tool_name, tool_params, expected_key in config["canary_tools"]:
                try:
                    result = await call_mcp_tool(mcp_url, tool_name, tool_params, timeout=20.0)

                    if result.get("_error"):
                        error_msg = result.get("message", "Unknown error")
                        findings.append(make_finding(
                            "mcp_health", "mcp_tool_broken", "agentic",
                            f"mcp/{mcp_name}/{tool_name}", "warning",
                            f"MCP {mcp_name} tool '{tool_name}' failed: {error_msg[:150]}",
                            {"mcp": mcp_name, "tool": tool_name, "error": error_msg,
                             "params": tool_params},
                            f"Check {mcp_name}-mcp logs and backend API connectivity",
                            "alerting-pipeline",
                        ))
                    elif expected_key and not result.get(expected_key):
                        # Tool returned success but unexpected/empty response
                        findings.append(make_finding(
                            "mcp_health", "mcp_tool_empty_response", "agentic",
                            f"mcp/{mcp_name}/{tool_name}", "info",
                            f"MCP {mcp_name} tool '{tool_name}' returned empty '{expected_key}' (may be normal)",
                            {"mcp": mcp_name, "tool": tool_name, "expected_key": expected_key,
                             "actual_keys": list(result.keys())[:10]},
                            "Verify the backend service has data",
                            "alerting-pipeline",
                        ))
                except Exception as e:
                    findings.append(make_finding(
                        "mcp_health", "mcp_tool_exception", "agentic",
                        f"mcp/{mcp_name}/{tool_name}", "warning",
                        f"MCP {mcp_name} tool '{tool_name}' exception: {str(e)[:100]}",
                        {"mcp": mcp_name, "tool": tool_name, "error": str(e)},
                        "Check MCP server pod health and restart if needed",
                        "alerting-pipeline",
                    ))

        return findings

    # ========================================================================
    # CHECK: INFRASTRUCTURE
    # ========================================================================

    async def check_infrastructure() -> list:
        """Check Proxmox, TrueNAS, OPNsense, Cloudflare health."""
        findings = []
        infra = INFRASTRUCTURE_MCP_URL

        # -- Proxmox VMs + LXCs (per-host parallel to avoid all-hosts timeout) --
        PROXMOX_HOSTS = ["ruapehu", "pihanga", "hikurangi"]

        async def _check_proxmox_host(host: str) -> list:
            host_findings = []
            # VMs
            try:
                vms_result = await call_mcp_tool_safe(
                    infra, "proxmox_list_vms", {"params": {"host": host, "response_format": "json"}},
                    timeout=45.0)
                if not vms_result:
                    logger.warning(f"Proxmox {host}: no VM data returned (timeout or error)")
                for vm in extract_list(vms_result, "vms"):
                    if not isinstance(vm, dict):
                        continue
                    vm_name = vm.get("name", "unknown")
                    vm_id = vm.get("vmid", "?")
                    status = vm.get("status", "")
                    if status and status != "running":
                        host_findings.append(make_finding(
                            "infrastructure", "proxmox_vm_not_running", "global",
                            f"proxmox/{host}/vm-{vm_id}-{vm_name}", "warning",
                            f"VM '{vm_name}' (ID: {vm_id}) on {host} status: {status}",
                            {"vm_name": vm_name, "vmid": vm_id, "status": status, "host": host},
                            f"qm start {vm_id}", "alerting-pipeline",
                        ))
            except Exception as e:
                logger.error(f"Proxmox VM check error on {host}: {e}")
            # LXCs
            try:
                ct_result = await call_mcp_tool_safe(
                    infra, "proxmox_list_containers", {"params": {"host": host, "response_format": "json"}},
                    timeout=45.0)
                if not ct_result:
                    logger.warning(f"Proxmox {host}: no container data returned (timeout or error)")
                for ct in extract_list(ct_result, "containers"):
                    if not isinstance(ct, dict):
                        continue
                    ct_name = ct.get("name", "unknown")
                    ct_id = ct.get("vmid", "?")
                    status = ct.get("status", "")
                    if status and status != "running":
                        host_findings.append(make_finding(
                            "infrastructure", "proxmox_ct_not_running", "global",
                            f"proxmox/{host}/ct-{ct_id}-{ct_name}", "warning",
                            f"LXC '{ct_name}' (ID: {ct_id}) on {host} status: {status}",
                            {"ct_name": ct_name, "vmid": ct_id, "status": status, "host": host},
                            "", "alerting-pipeline",
                        ))
            except Exception as e:
                logger.error(f"Proxmox LXC check error on {host}: {e}")
            return host_findings

        host_results = await asyncio.gather(
            *[_check_proxmox_host(h) for h in PROXMOX_HOSTS],
            return_exceptions=True,
        )
        for result in host_results:
            if isinstance(result, list):
                findings.extend(result)
            elif isinstance(result, Exception):
                logger.error(f"Proxmox host check failed: {result}")

        # -- TrueNAS pools --
        try:
            pools_result = await call_mcp_tool_safe(infra, "truenas_list_pools")
            for pool in extract_list(pools_result, "pools"):
                if not isinstance(pool, dict):
                    continue
                pool_name = pool.get("name", "unknown")
                status = str(pool.get("status", pool.get("healthy", ""))).upper()
                if status and status not in ("ONLINE", "HEALTHY", ""):
                    findings.append(make_finding(
                        "infrastructure", "truenas_pool_degraded", "global",
                        f"truenas/{pool_name}", "critical",
                        f"ZFS pool '{pool_name}' status: {status}",
                        {"pool": pool_name, "status": status},
                        f"zpool status {pool_name}", "alerting-pipeline",
                    ))
        except Exception as e:
            logger.error(f"TrueNAS pool check error: {e}")

        # -- TrueNAS alerts (all levels including INFO for app updates) --
        try:
            alerts_result = await call_mcp_tool_safe(infra, "truenas_get_all_alerts")
            for alert in extract_list(alerts_result, "alerts"):
                if not isinstance(alert, dict):
                    continue
                level = str(alert.get("level", "INFO")).upper()
                dismissed = alert.get("dismissed", False)
                if dismissed:
                    continue
                msg = alert.get("formatted", alert.get("message", "Unknown"))
                if level in ("CRITICAL", "ERROR"):
                    sev = "critical"
                elif level == "WARNING":
                    sev = "warning"
                else:
                    sev = "info"
                findings.append(make_finding(
                    "infrastructure", "truenas_alert", "global",
                    f"truenas/alert-{alert.get('id', 'x')}", sev,
                    f"TrueNAS alert ({level}): {str(msg)[:200]}",
                    alert, "", "alerting-pipeline",
                ))
        except Exception as e:
            logger.error(f"TrueNAS alerts check error: {e}")

        # -- TrueNAS disk usage --
        try:
            disk_result = await call_mcp_tool_safe(infra, "truenas_get_disk_usage")
            if disk_result:
                for ds in extract_list(disk_result, "datasets", "result"):
                    if not isinstance(ds, dict):
                        continue
                    name = ds.get("name", "unknown")
                    used_pct = ds.get("used_percent", ds.get("usage_percent", 0))
                    if isinstance(used_pct, str):
                        used_pct = float(used_pct.rstrip("%")) if used_pct.rstrip("%").replace(".", "").isdigit() else 0
                    if used_pct > 85:
                        findings.append(make_finding(
                            "infrastructure", "truenas_disk_high", "global",
                            f"truenas/disk-{name}", "warning" if used_pct < 95 else "critical",
                            f"TrueNAS dataset '{name}' at {used_pct:.0f}% capacity",
                            {"dataset": name, "used_percent": used_pct},
                            "Review and clean up data", "alerting-pipeline",
                        ))
        except Exception as e:
            logger.error(f"TrueNAS disk check error: {e}")

        # -- OPNsense gateway --
        try:
            gw_result = await call_mcp_tool_safe(infra, "get_gateway_status")
            for gw in extract_list(gw_result, "gateways"):
                if not isinstance(gw, dict):
                    continue
                gw_name = gw.get("name", "unknown")
                status = str(gw.get("status", gw.get("status_translated", "")))
                loss = gw.get("loss", "0%")
                if "down" in status.lower():
                    findings.append(make_finding(
                        "infrastructure", "gateway_down", "global",
                        f"opnsense/gw-{gw_name}", "critical",
                        f"Gateway '{gw_name}' is down (loss: {loss})",
                        {"gateway": gw_name, "status": status, "loss": loss},
                        "", "alerting-pipeline",
                    ))
        except Exception as e:
            logger.error(f"Gateway check error: {e}")

        # -- OPNsense services --
        try:
            svc_result = await call_mcp_tool_safe(infra, "get_services")
            for svc in extract_list(svc_result, "services"):
                if not isinstance(svc, dict):
                    continue
                svc_name = svc.get("name", "unknown")
                running = svc.get("running", True)
                if not running:
                    findings.append(make_finding(
                        "infrastructure", "opnsense_service_down", "global",
                        f"opnsense/svc-{svc_name}", "warning",
                        f"OPNsense service '{svc_name}' is not running",
                        {"service": svc_name}, "", "alerting-pipeline",
                    ))
        except Exception as e:
            logger.error(f"OPNsense service check error: {e}")

        # -- Cloudflare tunnels --
        try:
            cf_result = await call_mcp_tool_safe(infra, "cloudflare_list_tunnels")
            for tunnel in extract_list(cf_result, "tunnels"):
                if not isinstance(tunnel, dict):
                    continue
                t_name = tunnel.get("name", "unknown")
                t_status = str(tunnel.get("status", ""))
                if t_status.lower() not in ("healthy", "active", ""):
                    findings.append(make_finding(
                        "infrastructure", "cloudflare_tunnel_unhealthy", "global",
                        f"cloudflare/tunnel-{t_name}", "critical",
                        f"Cloudflare tunnel '{t_name}' status: {t_status}",
                        {"tunnel": t_name, "status": t_status},
                        "", "alerting-pipeline",
                    ))
        except Exception as e:
            logger.error(f"Cloudflare tunnel check error: {e}")

        return findings

    # ========================================================================
    # CHECK: OBSERVABILITY
    # ========================================================================

    async def check_observability() -> list:
        """Check Coroot anomalies, scrape targets, Gatus failures."""
        findings = []
        obs = OBSERVABILITY_MCP_URL

        # -- Coroot anomalies --
        try:
            anomalies_result = await call_mcp_tool_safe(obs, "coroot_get_recent_anomalies", {
                "hours": 24
            })
            for anom in extract_list(anomalies_result, "anomalies"):
                if not isinstance(anom, dict):
                    continue
                app_name = anom.get("application", anom.get("app", "unknown"))
                check = anom.get("check", anom.get("type", "unknown"))
                sev = str(anom.get("severity", "warning")).lower()
                msg = anom.get("message", f"{check} anomaly on {app_name}")

                # Skip transient Coroot anomalies for media Recreate-strategy apps.
                # Format: "app@namespace" or just "app" — extract the base name.
                base_app = app_name.split("@")[0] if "@" in app_name else app_name
                if base_app in MEDIA_RECREATE_APPS:
                    logger.debug(f"Skipping transient Coroot media anomaly: {app_name}/{check}")
                    continue

                findings.append(make_finding(
                    "metrics", "coroot_anomaly", "global",
                    f"coroot/{app_name}/{check}",
                    sev if sev in ("critical", "warning", "info") else "warning",
                    f"Coroot anomaly: {str(msg)[:200]}",
                    anom, "", "alerting-pipeline",
                ))
        except Exception as e:
            logger.error(f"Coroot anomaly check error: {e}")

        # -- Scrape targets down --
        try:
            targets_result = await call_mcp_tool_safe(obs, "get_scrape_targets")
            targets = extract_list(targets_result, "targets", "activeTargets")
            for target in targets:
                if not isinstance(target, dict):
                    continue
                if str(target.get("health", "")).lower() == "down":
                    job = target.get("labels", {}).get("job", target.get("job", "unknown"))
                    instance = target.get("labels", {}).get("instance", target.get("scrapeUrl", "unknown"))
                    findings.append(make_finding(
                        "metrics", "scrape_target_down", "monitoring",
                        f"prometheus/{job}/{instance}", "warning",
                        f"Scrape target down: job={job} instance={instance} error={target.get('lastError', '')[:100]}",
                        {"job": job, "instance": instance, "error": target.get("lastError", "")},
                        "Check if the target service is running", "prometheus",
                        {"type": "prometheus", "alert_name": "HomelabScrapeTargetDown",
                         "expr": f'up{{job="{job}"}} == 0', "for": "10m", "severity": "warning"},
                    ))
        except Exception as e:
            logger.error(f"Scrape target check error: {e}")

        # -- Gatus: ALL failing endpoints (not just summary) --
        try:
            gatus_result = await call_mcp_tool_safe(obs, "gatus_get_failing_endpoints")
            for ep in extract_list(gatus_result, "endpoints"):
                if not isinstance(ep, dict):
                    continue
                ep_name = ep.get("name", "unknown")
                ep_group = ep.get("group", "")
                ep_url = ep.get("url", "")
                findings.append(make_finding(
                    "network", "gatus_endpoint_failing", "global",
                    f"gatus/{ep_group}/{ep_name}", "warning",
                    f"Gatus endpoint failing: {ep_group}/{ep_name} ({ep_url})",
                    ep, "", "gatus",
                ))
        except Exception as e:
            logger.error(f"Gatus check error: {e}")

        # -- Gatus: check all endpoint statuses for degraded/flapping --
        try:
            all_status = await call_mcp_tool_safe(obs, "gatus_get_endpoint_status")
            for ep in extract_list(all_status, "endpoints", "statuses"):
                if not isinstance(ep, dict):
                    continue
                ep_name = ep.get("name", "unknown")
                ep_group = ep.get("group", "")
                results = ep.get("results", [])
                if isinstance(results, list) and len(results) >= 5:
                    # Check for flapping (alternating success/failure)
                    recent = results[-10:]
                    successes = sum(1 for r in recent if isinstance(r, dict) and r.get("success"))
                    failures = len(recent) - successes
                    if failures >= 3 and successes >= 2:
                        findings.append(make_finding(
                            "network", "gatus_endpoint_flapping", "global",
                            f"gatus/{ep_group}/{ep_name}/flap", "warning",
                            f"Gatus endpoint flapping: {ep_group}/{ep_name} ({failures}/{len(recent)} failures in recent checks)",
                            {"name": ep_name, "group": ep_group, "successes": successes, "failures": failures},
                            "Investigate intermittent connectivity", "gatus",
                        ))
        except Exception as e:
            logger.error(f"Gatus status check error: {e}")

        return findings

    # ========================================================================
    # CHECK: RENOVATE / DEPENDENCY UPDATES
    # ========================================================================

    async def check_renovate() -> list:
        """Check for pending Renovate PRs and auto-merge mature ones."""
        findings = []
        ext = EXTERNAL_MCP_URL

        if not GITHUB_TOKEN:
            logger.warning("GITHUB_TOKEN not set, skipping Renovate check")
            return findings

        for owner, repo in RENOVATE_REPOS:
            try:
                prs_result = await call_mcp_tool_safe(ext, "github_list_prs", {
                    "owner": owner, "repo": repo, "state": "open", "per_page": 20,
                })
                # Parse the markdown response for Renovate PRs
                prs_text = ""
                if isinstance(prs_result, dict):
                    prs_text = prs_result.get("result", "")
                elif isinstance(prs_result, str):
                    prs_text = prs_result

                # Extract PR info from markdown lines like:
                # - #7 **chore(deps): Update ...** (renovate/... -> main)
                for line in prs_text.split("\n"):
                    if "chore(deps):" not in line and "renovate" not in line.lower():
                        continue
                    # Extract PR number
                    pr_match = re.search(r"#(\d+)", line)
                    if not pr_match:
                        continue
                    pr_num = int(pr_match.group(1))

                    # Extract title
                    title_match = re.search(r"\*\*(.+?)\*\*", line)
                    pr_title = title_match.group(1) if title_match else f"PR #{pr_num}"

                    # Skip config migration PRs
                    if "migrate" in pr_title.lower():
                        continue

                    # Get PR details to check age
                    pr_detail = await call_mcp_tool_safe(ext, "github_get_pr", {
                        "owner": owner, "repo": repo, "pr_number": pr_num,
                    })
                    pr_text = ""
                    if isinstance(pr_detail, dict):
                        pr_text = pr_detail.get("result", "")

                    is_major = "major" in line.lower() or "major" in pr_text.lower()

                    # Record finding
                    findings.append(make_finding(
                        "dependencies", "renovate_pr_pending", "global",
                        f"renovate/{owner}/{repo}/pr-{pr_num}",
                        "warning" if is_major else "info",
                        f"Renovate PR #{pr_num} on {repo}: {pr_title}",
                        {"owner": owner, "repo": repo, "pr_number": pr_num,
                         "title": pr_title, "is_major": is_major},
                        "", "",
                    ))

                    # Auto-merge non-major (safe) updates; skip major (risky)
                    if not is_major:
                        merged = await _try_merge_renovate_pr(owner, repo, pr_num, pr_title)
                        if merged:
                            logger.info(f"Auto-merged Renovate PR #{pr_num} on {repo}: {pr_title}")
                            # Update finding to reflect merge
                            findings[-1]["evidence"]["auto_merged"] = True
                            findings[-1]["description"] = f"[AUTO-MERGED] {findings[-1]['description']}"

            except Exception as e:
                logger.error(f"Renovate check error for {owner}/{repo}: {e}")

        return findings

    async def _try_merge_renovate_pr(owner: str, repo: str, pr_num: int, title: str) -> bool:
        """Merge a Renovate PR via GitHub API. Returns True if merged."""
        if not GITHUB_TOKEN:
            return False
        try:
            async with httpx.AsyncClient(timeout=15.0) as client:
                resp = await client.put(
                    f"https://api.github.com/repos/{owner}/{repo}/pulls/{pr_num}/merge",
                    json={
                        "commit_title": f"{title}\n\nAuto-merged by Error Hunter",
                        "merge_method": "squash",
                    },
                    headers={
                        "Authorization": f"Bearer {GITHUB_TOKEN}",
                        "Accept": "application/vnd.github+json",
                        "X-GitHub-Api-Version": "2022-11-28",
                    },
                )
                if resp.status_code == 200:
                    return True
                else:
                    logger.warning(f"Failed to merge PR #{pr_num} on {repo}: {resp.status_code} {resp.text[:200]}")
                    return False
        except Exception as e:
            logger.error(f"Error merging PR #{pr_num} on {repo}: {e}")
            return False

    # ========================================================================
    # CHECK: BESZEL AGENT MONITORING
    # ========================================================================

    async def check_beszel() -> list:
        """Check Beszel system monitoring — agent status and version drift."""
        findings = []
        beszel_url = os.environ.get("BESZEL_URL", "")
        beszel_user = os.environ.get("BESZEL_USERNAME", "")
        beszel_pass = os.environ.get("BESZEL_PASSWORD", "")

        if not all([beszel_url, beszel_user, beszel_pass]):
            logger.warning("Beszel credentials not configured, skipping check")
            return findings

        try:
            async with httpx.AsyncClient(timeout=15.0, verify=False) as client:
                # Authenticate (PocketBase user auth)
                auth_resp = await client.post(
                    f"{beszel_url}/api/collections/users/auth-with-password",
                    json={"identity": beszel_user, "password": beszel_pass},
                )
                if auth_resp.status_code != 200:
                    findings.append(make_finding(
                        "observability", "beszel_auth_failed", "global",
                        "beszel/hub", "warning",
                        f"Beszel hub auth failed: HTTP {auth_resp.status_code}",
                    ))
                    return findings

                token = auth_resp.json().get("token", "")
                headers = {"Authorization": token}

                # Get hub version from key endpoint
                hub_version = ""
                try:
                    key_resp = await client.get(f"{beszel_url}/api/beszel/getkey", headers=headers)
                    if key_resp.status_code == 200:
                        hub_version = key_resp.json().get("v", "")
                except Exception:
                    pass

                # Get all systems
                sys_resp = await client.get(
                    f"{beszel_url}/api/collections/systems/records?perPage=50",
                    headers=headers,
                )
                systems = sys_resp.json().get("items", [])

                for system in systems:
                    name = system.get("name", "unknown")
                    status = system.get("status", "unknown")
                    info = system.get("info", {})
                    agent_version = info.get("v", "")
                    host = system.get("host", "")

                    # Check for down/pending agents
                    if status != "up":
                        findings.append(make_finding(
                            "observability", "beszel_agent_down", "global",
                            f"beszel/{name}", "warning",
                            f"Beszel agent '{name}' status: {status}",
                            {"host": host, "status": status},
                            "Check agent connectivity and SSH key",
                        ))

                    # Check for version drift (report only — agents on different networks, no SSH)
                    if hub_version and agent_version and agent_version != hub_version:
                        findings.append(make_finding(
                            "observability", "beszel_agent_version_drift", "global",
                            f"beszel/{name}/version", "info",
                            f"Beszel agent '{name}' at {host} is {agent_version} (hub: {hub_version})",
                            {"agent_version": agent_version, "hub_version": hub_version, "host": host},
                            f"SSH to {host} and update agent manually",
                        ))

        except Exception as e:
            logger.error(f"Beszel check error: {e}")

        return findings

    # ========================================================================
    # CHECK: DNS & REVERSE PROXY CONSISTENCY
    # ========================================================================

    async def check_dns_routing() -> list:
        """Check DNS routing consistency — AdGuard rewrites, Caddy backends, Unbound overrides."""
        findings = []
        home = HOME_MCP_URL
        infra = INFRASTRUCTURE_MCP_URL

        try:
            # Fetch all three DNS/proxy layers in parallel
            ag_task = call_mcp_tool_safe(home, "adguard_list_rewrites")
            caddy_task = call_mcp_tool_safe(infra, "list_caddy_reverse_proxies")
            unbound_task = call_mcp_tool_safe(infra, "get_unbound_overrides")
            ag_result, caddy_result, unbound_result = await asyncio.gather(
                ag_task, caddy_task, unbound_task, return_exceptions=True,
            )

            # --- AdGuard duplicate detection ---
            rewrites = []
            if isinstance(ag_result, dict):
                rewrites = ag_result.get("items", ag_result.get("result", []))
                if isinstance(rewrites, str):
                    rewrites = []
            domain_counts: dict[str, int] = {}
            for rw in rewrites:
                domain = rw.get("domain", "") if isinstance(rw, dict) else ""
                if domain:
                    domain_counts[domain] = domain_counts.get(domain, 0) + 1
            for domain, count in domain_counts.items():
                if count > 1:
                    findings.append(make_finding(
                        "network", "dns_duplicate_rewrite", "global",
                        f"adguard/{domain}", "warning",
                        f"AdGuard has {count} duplicate rewrites for '{domain}'",
                        {"domain": domain, "count": count},
                        "Remove duplicate entries in AdGuard DNS rewrites",
                    ))

            # --- Caddy backend reachability ---
            caddy_entries = []
            if isinstance(caddy_result, dict):
                caddy_entries = caddy_result.get("items", caddy_result.get("result", []))
                if isinstance(caddy_entries, str):
                    caddy_entries = []

            adguard_domains = set(domain_counts.keys())
            unbound_domains = set()
            if isinstance(unbound_result, dict):
                overrides = unbound_result.get("items", unbound_result.get("result", []))
                if isinstance(overrides, list):
                    for ov in overrides:
                        if isinstance(ov, dict):
                            unbound_domains.add(ov.get("hostname", ov.get("domain", "")))

            async with httpx.AsyncClient(timeout=5.0, verify=False) as client:
                for entry in caddy_entries:
                    if not isinstance(entry, dict):
                        continue
                    domain = entry.get("domain", entry.get("match", ""))
                    upstream = entry.get("upstream", entry.get("to", ""))
                    if not upstream:
                        continue

                    # TCP reachability test on upstream
                    test_url = upstream if upstream.startswith("http") else f"http://{upstream}"
                    try:
                        resp = await client.get(test_url, follow_redirects=False)
                        # Any response (even 4xx/5xx) means the backend is alive
                    except Exception:
                        findings.append(make_finding(
                            "network", "caddy_backend_unreachable", "global",
                            f"caddy/{domain}", "warning",
                            f"Caddy backend unreachable: {domain} → {upstream}",
                            {"domain": domain, "upstream": upstream},
                            f"Check if service at {upstream} is running",
                        ))

                    # DNS cross-reference
                    clean_domain = domain.replace("*.", "").strip()
                    if clean_domain and clean_domain not in adguard_domains and clean_domain not in unbound_domains:
                        findings.append(make_finding(
                            "network", "dns_missing_route", "global",
                            f"caddy/{clean_domain}/dns", "info",
                            f"Caddy entry '{clean_domain}' has no AdGuard rewrite or Unbound override",
                            {"domain": clean_domain, "upstream": upstream},
                        ))

        except Exception as e:
            logger.error(f"DNS routing check error: {e}")

        return findings

    # ========================================================================
    # CHECK: BACKUP SYSTEMS (Velero, Backrest, PBS)
    # ========================================================================

    async def check_backups() -> list:
        """Check backup system health — Velero, Backrest, PBS."""
        findings = []
        infra = INFRASTRUCTURE_MCP_URL

        # --- Velero (all 3 clusters) ---
        for cluster_name, cluster_key in CLUSTERS.items():
            try:
                pods = await call_mcp_tool_safe(infra, "kubectl_get_pods", {
                    "namespace": "velero", "cluster": cluster_key,
                })
                pod_list = pods.get("result", [])
                if isinstance(pod_list, list):
                    for pod in pod_list:
                        if isinstance(pod, dict) and not pod.get("ready", True):
                            pod_status = pod.get("status", "")
                            # Skip completed Job pods — Succeeded/Completed is normal
                            if pod_status in ("Succeeded", "Completed"):
                                continue
                            findings.append(make_finding(
                                "backups", "velero_pod_unhealthy", cluster_name,
                                f"velero/{pod.get('name', 'unknown')}", "warning",
                                f"Velero pod unhealthy on {cluster_name}: {pod.get('name')} ({pod_status})",
                                {"pod": pod.get("name"), "status": pod_status},
                            ))

                # Check for failed backup jobs
                jobs = await call_mcp_tool_safe(infra, "kubectl_get_jobs", {
                    "namespace": "velero", "cluster": cluster_key,
                })
                job_list = jobs.get("result", [])
                if isinstance(job_list, list):
                    for job in job_list:
                        if isinstance(job, dict):
                            failures = job.get("failed", job.get("failures", 0))
                            if failures and int(failures) > 0:
                                findings.append(make_finding(
                                    "backups", "velero_backup_failed", cluster_name,
                                    f"velero/{job.get('name', 'unknown')}", "warning",
                                    f"Velero backup job failed on {cluster_name}: {job.get('name')}",
                                    {"job": job.get("name"), "failures": failures},
                                ))
            except Exception as e:
                logger.error(f"Velero check error ({cluster_name}): {e}")

        # --- Backrest (monit cluster, backrest namespace) ---
        try:
            backrest_url = "http://10.10.0.30:31800"
            async with httpx.AsyncClient(timeout=10.0, verify=False) as client:
                resp = await client.get(f"{backrest_url}/api/v1/operations")
                if resp.status_code == 200:
                    ops = resp.json()
                    # Check if any plan hasn't run recently
                    plan_last_run: dict[str, str] = {}
                    if isinstance(ops, list):
                        for op in ops:
                            plan_id = op.get("planId", "")
                            ts = op.get("unixTimeStartMs", 0)
                            if plan_id and ts:
                                prev = plan_last_run.get(plan_id, 0)
                                if ts > prev:
                                    plan_last_run[plan_id] = ts

                    now_ms = int(datetime.utcnow().timestamp() * 1000)
                    daily_threshold_ms = 36 * 3600 * 1000  # 36 hours
                    weekly_threshold_ms = 8 * 24 * 3600 * 1000  # 8 days

                    for plan_id, last_ms in plan_last_run.items():
                        is_weekly = "weekly" in plan_id
                        threshold = weekly_threshold_ms if is_weekly else daily_threshold_ms
                        age_ms = now_ms - last_ms
                        if age_ms > threshold:
                            age_hours = age_ms // (3600 * 1000)
                            findings.append(make_finding(
                                "backups", "backrest_plan_overdue", "global",
                                f"backrest/{plan_id}", "warning",
                                f"Backrest plan '{plan_id}' last ran {age_hours}h ago (threshold: {'8d' if is_weekly else '36h'})",
                                {"plan_id": plan_id, "last_run_ms": last_ms, "age_hours": age_hours},
                            ))
                else:
                    findings.append(make_finding(
                        "backups", "backrest_unreachable", "global",
                        "backrest/api", "warning",
                        f"Backrest API returned HTTP {resp.status_code}",
                    ))
        except Exception as e:
            findings.append(make_finding(
                "backups", "backrest_unreachable", "global",
                "backrest/api", "warning",
                f"Backrest API unreachable: {e}",
            ))

        # --- PBS (Proxmox Backup Server) ---
        pbs_host = os.environ.get("PBS_HOST", "")
        pbs_password = os.environ.get("PBS_PASSWORD", "")
        if pbs_host and pbs_password:
            try:
                async with httpx.AsyncClient(timeout=10.0, verify=False) as client:
                    auth_resp = await client.post(
                        f"{pbs_host}/api2/json/access/ticket",
                        data={"username": "root@pam", "password": pbs_password},
                    )
                    if auth_resp.status_code == 200:
                        ticket = auth_resp.json().get("data", {}).get("ticket", "")
                        csrf = auth_resp.json().get("data", {}).get("CSRFPreventionToken", "")
                        cookies = {"PBSAuthCookie": ticket}

                        # List datastores
                        ds_resp = await client.get(
                            f"{pbs_host}/api2/json/admin/datastore",
                            cookies=cookies,
                        )
                        datastores = ds_resp.json().get("data", []) if ds_resp.status_code == 200 else []

                        for ds in datastores:
                            store_name = ds.get("name", ds.get("store", ""))
                            if not store_name:
                                continue
                            snap_resp = await client.get(
                                f"{pbs_host}/api2/json/admin/datastore/{store_name}/snapshots",
                                cookies=cookies,
                            )
                            snapshots = snap_resp.json().get("data", []) if snap_resp.status_code == 200 else []

                            # Find most recent snapshot per backup-type/backup-id
                            group_latest: dict[str, int] = {}
                            for snap in snapshots:
                                group = f"{snap.get('backup-type', '')}/{snap.get('backup-id', '')}"
                                ts = snap.get("backup-time", 0)
                                if ts > group_latest.get(group, 0):
                                    group_latest[group] = ts

                            now_ts = int(datetime.utcnow().timestamp())
                            for group, last_ts in group_latest.items():
                                age_hours = (now_ts - last_ts) // 3600
                                if age_hours > 36:
                                    findings.append(make_finding(
                                        "backups", "pbs_backup_stale", "global",
                                        f"pbs/{store_name}/{group}", "warning",
                                        f"PBS backup '{group}' on '{store_name}' is {age_hours}h old",
                                        {"datastore": store_name, "group": group, "age_hours": age_hours},
                                    ))
                    else:
                        findings.append(make_finding(
                            "backups", "pbs_unreachable", "global",
                            "pbs/auth", "warning",
                            f"PBS auth failed: HTTP {auth_resp.status_code}",
                        ))
            except Exception as e:
                # PBS proxy briefly returns ENOTCONN (~2-5 min) as backup jobs start at 02:00 UTC
                # Suppress false-positive during 02:00-02:15 window to avoid noise
                utc_now = datetime.now(timezone.utc)
                if utc_now.hour == 2 and utc_now.minute < 15:
                    logger.info(f"PBS API unreachable during backup startup window (02:00-02:15 UTC): {e}")
                else:
                    findings.append(make_finding(
                        "backups", "pbs_unreachable", "global",
                        "pbs/api", "warning",
                        f"PBS API unreachable: {e}",
                    ))

        return findings

    # ========================================================================
    # CHECK: NETWORK HEALTH (ntopng, UniFi, Omada)
    # ========================================================================

    async def check_network_health() -> list:
        """Check network infrastructure — ntopng alerts, UniFi health, Omada switch/LAG."""
        findings = []
        obs = OBSERVABILITY_MCP_URL
        home = HOME_MCP_URL
        infra = INFRASTRUCTURE_MCP_URL

        # --- ntopng alerts ---
        try:
            result = await call_mcp_tool_safe(obs, "ntopng_get_alerts")
            alerts = []
            if isinstance(result, dict):
                alerts = result.get("items", result.get("alerts", result.get("result", [])))
            if isinstance(alerts, list):
                for alert in alerts[:20]:
                    if not isinstance(alert, dict):
                        continue
                    sev = alert.get("severity", alert.get("alert_severity", ""))
                    sev_str = "warning"
                    if isinstance(sev, (int, float)):
                        sev_str = "critical" if sev <= 2 else "warning"
                    elif isinstance(sev, str) and "error" in sev.lower():
                        sev_str = "critical"
                    msg = alert.get("msg", alert.get("alert_json", {}).get("msg", str(alert)[:200]))
                    findings.append(make_finding(
                        "network", "ntopng_alert", "global",
                        f"ntopng/{alert.get('type', 'alert')}/{alert.get('entity_val', 'unknown')}", sev_str,
                        f"ntopng alert: {msg}",
                        {"raw": {k: v for k, v in alert.items() if k != "alert_json"}},
                    ))
        except Exception as e:
            logger.error(f"ntopng alert check error: {e}")

        # --- UniFi subsystem health ---
        try:
            result = await call_mcp_tool_safe(home, "unifi_get_health")
            health_items = []
            if isinstance(result, dict):
                health_items = result.get("items", result.get("result", []))
            if isinstance(health_items, list):
                for item in health_items:
                    if not isinstance(item, dict):
                        continue
                    subsystem = item.get("subsystem", "unknown")
                    status = item.get("status", "ok")
                    if status != "ok":
                        findings.append(make_finding(
                            "network", "unifi_subsystem_degraded", "global",
                            f"unifi/{subsystem}", "warning",
                            f"UniFi subsystem '{subsystem}' status: {status}",
                            {"subsystem": subsystem, "status": status},
                        ))
        except Exception as e:
            logger.error(f"UniFi health check error: {e}")

        # --- UniFi alarms ---
        try:
            result = await call_mcp_tool_safe(home, "unifi_get_alarms")
            alarms = []
            if isinstance(result, dict):
                alarms = result.get("items", result.get("result", []))
            if isinstance(alarms, list):
                for alarm in alarms[:10]:
                    if not isinstance(alarm, dict):
                        continue
                    if alarm.get("archived", False):
                        continue
                    findings.append(make_finding(
                        "network", "unifi_alarm", "global",
                        f"unifi/alarm/{alarm.get('_id', 'unknown')}", "warning",
                        f"UniFi alarm: {alarm.get('msg', alarm.get('key', 'unknown'))}",
                        {"alarm_id": alarm.get("_id"), "msg": alarm.get("msg", "")},
                    ))
        except Exception as e:
            logger.error(f"UniFi alarm check error: {e}")

        # --- Omada switch + LAG health ---
        try:
            result = await call_mcp_tool_safe(infra, "omada_get_devices")
            devices = []
            if isinstance(result, dict):
                devices = result.get("items", result.get("result", []))
            if isinstance(devices, list):
                for device in devices:
                    if not isinstance(device, dict):
                        continue
                    status = device.get("status", device.get("statusCategory", ""))
                    name = device.get("name", device.get("model", "unknown"))
                    if isinstance(status, int):
                        if status != 14:  # 14 = connected
                            findings.append(make_finding(
                                "network", "omada_switch_offline", "global",
                                f"omada/{name}", "critical",
                                f"Omada device '{name}' offline (status: {status})",
                                {"name": name, "status": status, "mac": device.get("mac", "")},
                            ))
                    elif isinstance(status, str) and status.lower() not in ("connected", "online"):
                        findings.append(make_finding(
                            "network", "omada_switch_offline", "global",
                            f"omada/{name}", "critical",
                            f"Omada device '{name}' status: {status}",
                            {"name": name, "status": status},
                        ))
        except Exception as e:
            logger.error(f"Omada device check error: {e}")

        try:
            result = await call_mcp_tool_safe(infra, "omada_get_lags")
            lags = []
            if isinstance(result, dict):
                lags = result.get("items", result.get("result", []))
            if isinstance(lags, list):
                for lag in lags:
                    if not isinstance(lag, dict):
                        continue
                    lag_name = lag.get("name", lag.get("id", "unknown"))
                    ports = lag.get("ports", lag.get("lagPorts", []))
                    active = lag.get("activePortCount", lag.get("active_ports", len(ports)))
                    total = lag.get("portCount", lag.get("total_ports", len(ports)))
                    if total and active < total:
                        findings.append(make_finding(
                            "network", "omada_lag_degraded", "global",
                            f"omada/lag/{lag_name}", "warning",
                            f"Omada LAG '{lag_name}' degraded: {active}/{total} ports active",
                            {"lag": lag_name, "active": active, "total": total},
                        ))
        except Exception as e:
            logger.error(f"Omada LAG check error: {e}")

        return findings

    # ========================================================================
    # CHECK: NFS STORAGE & TRUENAS SNAPSHOTS
    # ========================================================================

    async def check_nfs_storage() -> list:
        """Check NFS share health, snapshot freshness, and mount-canary status."""
        findings = []
        infra = INFRASTRUCTURE_MCP_URL

        # Expected NFS shares per TrueNAS instance
        expected_shares = {
            "hdd": ["Pleximetry", "pbs", "victoria-metrics", "victoria-logs"],
        }

        for instance, expected in expected_shares.items():
            try:
                result = await call_mcp_tool_safe(infra, "truenas_list_shares", {"params": {"instance": instance}})
                share_names = set()
                shares = result.get("items", result.get("result", []))
                if isinstance(shares, list):
                    for share in shares:
                        if isinstance(share, dict):
                            path = share.get("path", share.get("paths", [""])[0] if isinstance(share.get("paths"), list) else "")
                            name = path.split("/")[-1] if "/" in path else share.get("name", "")
                            share_names.add(name.lower())
                elif isinstance(shares, str):
                    for name in expected:
                        if name.lower() in shares.lower():
                            share_names.add(name.lower())

                for name in expected:
                    if name.lower() not in share_names and share_names:
                        findings.append(make_finding(
                            "storage", "nfs_share_missing", "global",
                            f"truenas-{instance}/nfs/{name}", "critical",
                            f"Expected NFS share '{name}' not found on TrueNAS-{instance.upper()}",
                            {"instance": instance, "share": name, "found_shares": list(share_names)},
                        ))
            except Exception as e:
                logger.error(f"NFS share check error ({instance}): {e}")

        # --- Snapshot freshness ---
        for instance in ["hdd", "media"]:
            try:
                result = await call_mcp_tool_safe(infra, "truenas_list_snapshots", {"params": {"instance": instance}})
                snapshots = result.get("items", result.get("result", []))
                if isinstance(snapshots, list) and snapshots:
                    # Group by dataset, find latest
                    dataset_latest: dict[str, str] = {}
                    for snap in snapshots:
                        if not isinstance(snap, dict):
                            continue
                        ds = snap.get("dataset", snap.get("filesystem", ""))
                        ts = snap.get("created", snap.get("createtxg", ""))
                        name = snap.get("name", snap.get("snapshot_name", ""))
                        if ds and name:
                            prev = dataset_latest.get(ds, "")
                            if name > prev:
                                dataset_latest[ds] = name
            except Exception as e:
                logger.error(f"Snapshot check error ({instance}): {e}")

        # --- Mount-canary CronJob status ---
        try:
            jobs = await call_mcp_tool_safe(infra, "kubectl_get_jobs", {
                "namespace": "ai-platform", "cluster": "agentic",
            })
            job_list = jobs.get("result", [])
            if isinstance(job_list, list):
                canary_jobs = [j for j in job_list if isinstance(j, dict) and "mount-canary" in j.get("name", "")]
                for job in canary_jobs[-3:]:
                    failures = job.get("failed", job.get("failures", 0))
                    if failures and int(failures) > 0:
                        findings.append(make_finding(
                            "storage", "mount_canary_failed", "agentic",
                            f"ai-platform/{job.get('name', 'mount-canary')}", "warning",
                            f"Mount-canary job failed: {job.get('name')} — NFS connectivity issue",
                            {"job": job.get("name"), "failures": failures},
                            "Check NFS mount from agentic cluster to TrueNAS-HDD",
                        ))
        except Exception as e:
            logger.error(f"Mount-canary check error: {e}")

        return findings

    # ========================================================================
    # CHECK: HOMEPAGE SERVICE URLs & WIDGETS
    # ========================================================================

    async def check_homepage() -> list:
        """Check Homepage service URLs and widget health via home-mcp."""
        findings = []
        home = HOME_MCP_URL

        try:
            result = await call_mcp_tool_safe(home, "get_service_status_summary", timeout=60.0)
            if not result:
                return findings

            services = result.get("services", result.get("result", []))
            total = result.get("total", 0)
            unhealthy_count = result.get("unhealthy", 0)

            # Track failures by group for category-level alerting
            group_failures: dict[str, list] = {}

            if isinstance(services, list):
                for svc in services:
                    if not isinstance(svc, dict):
                        continue
                    status = svc.get("status", "unknown")
                    name = svc.get("name", "unknown")
                    href = svc.get("href", svc.get("url", ""))
                    group = svc.get("group", svc.get("category", "ungrouped"))

                    if status in ("unhealthy", "error", "unreachable", "timeout"):
                        findings.append(make_finding(
                            "services", "homepage_service_unreachable", "global",
                            f"homepage/{group}/{name}", "warning",
                            f"Homepage service unreachable: {name} ({href})",
                            {"name": name, "href": href, "group": group, "status": status},
                        ))
                        group_failures.setdefault(group, []).append(name)

            # Category-level degradation (3+ services in same group failing)
            for group, failed_services in group_failures.items():
                if len(failed_services) >= 3:
                    findings.append(make_finding(
                        "services", "homepage_category_degraded", "global",
                        f"homepage/{group}", "warning",
                        f"Homepage group '{group}' degraded: {len(failed_services)} services unreachable ({', '.join(failed_services[:5])})",
                        {"group": group, "failed_services": failed_services},
                    ))

        except Exception as e:
            logger.error(f"Homepage check error: {e}")

        return findings

    # ========================================================================
    # CHECK: MEDIA PIPELINE (Plex, Arr Suite, Download Clients)
    # ========================================================================

    async def check_media_pipeline() -> list:
        """Check media pipeline health — Plex, Sonarr/Radarr queues, Prowlarr indexers, downloads."""
        findings = []
        media = MEDIA_MCP_URL

        # --- Plex server status ---
        try:
            result = await call_mcp_tool_safe(media, "plex_get_server_status")
            if result.get("_error") or not result:
                findings.append(make_finding(
                    "media", "plex_unreachable", "global",
                    "plex/server", "critical",
                    "Plex Media Server unreachable",
                    {"error": result.get("message", "no response")},
                ))
        except Exception as e:
            logger.error(f"Plex status check error: {e}")

        # --- Sonarr/Radarr queues ---
        media_key = {"sonarr": "series", "radarr": "movie"}
        for app_name, tool_name in [("sonarr", "sonarr_get_queue"), ("radarr", "radarr_get_queue")]:
            try:
                result = await call_mcp_tool_safe(media, tool_name)
                records = result.get("records", result.get("items", result.get("result", [])))
                if isinstance(records, list):
                    orphaned_items = []
                    for item in records:
                        if not isinstance(item, dict):
                            continue
                        status = item.get("status", "").lower()
                        title = item.get("title", "unknown")
                        tracked_status = item.get("trackedDownloadStatus", "")
                        queue_id = item.get("id")
                        match_key = media_key.get(app_name, "series")
                        has_match = item.get(match_key) is not None

                        # Orphaned check FIRST: completed but no series/movie match
                        if status == "completed" and not has_match:
                            orphaned_items.append({"title": title, "queue_id": queue_id,
                                                   "download_client": item.get("downloadClient", "")})
                        # Then check generic stuck items (warning/failed status)
                        elif status in ("warning", "failed") or tracked_status in ("warning", "error"):
                            findings.append(make_finding(
                                "media", "arr_queue_stuck", "global",
                                f"{app_name}/queue/{title[:50]}", "warning",
                                f"{app_name.capitalize()} queue item stuck: {title} (status: {status})",
                                {"app": app_name, "title": title, "status": status,
                                 "tracked_status": tracked_status, "queue_id": queue_id},
                            ))

                    if orphaned_items:
                        findings.append(make_finding(
                            "media", "arr_queue_orphaned", "global",
                            f"{app_name}/queue-orphaned", "warning",
                            f"{len(orphaned_items)} orphaned item(s) in {app_name.capitalize()} queue — "
                            f"completed but no {match_key} match. Files sitting in download temp folder.",
                            {"app": app_name, "count": len(orphaned_items),
                             "items": [{"title": i["title"][:60], "queue_id": i["queue_id"],
                                        "client": i["download_client"]} for i in orphaned_items[:15]]},
                            f"Remove orphaned items from {app_name} queue and clean up temp download files",
                        ))
            except Exception as e:
                logger.error(f"{app_name} queue check error: {e}")

        # --- Prowlarr indexers ---
        try:
            result = await call_mcp_tool_safe(media, "prowlarr_list_indexers")
            indexers = result.get("items", result.get("result", []))
            if isinstance(indexers, list):
                for idx in indexers:
                    if not isinstance(idx, dict):
                        continue
                    name = idx.get("name", "unknown")
                    enable = idx.get("enable", True)
                    status_obj = idx.get("status", {})
                    if isinstance(status_obj, dict):
                        is_healthy = status_obj.get("isHealthy", True)
                        if not is_healthy and enable:
                            findings.append(make_finding(
                                "media", "prowlarr_indexer_failed", "global",
                                f"prowlarr/{name}", "warning",
                                f"Prowlarr indexer '{name}' unhealthy",
                                {"indexer": name, "status": status_obj},
                            ))
                    elif isinstance(status_obj, str) and "fail" in status_obj.lower():
                        findings.append(make_finding(
                            "media", "prowlarr_indexer_failed", "global",
                            f"prowlarr/{name}", "warning",
                            f"Prowlarr indexer '{name}' status: {status_obj}",
                            {"indexer": name, "status": status_obj},
                        ))
        except Exception as e:
            logger.error(f"Prowlarr indexer check error: {e}")

        # --- Download clients (Transmission + SABnzbd) ---
        try:
            result = await call_mcp_tool_safe(media, "transmission_list_torrents")
            torrents = result.get("items", result.get("torrents", result.get("result", [])))
            if isinstance(torrents, list):
                stale_completed = []
                for torrent in torrents:
                    if not isinstance(torrent, dict):
                        continue
                    status = torrent.get("status", 0)
                    error_val = torrent.get("error", 0)
                    name = torrent.get("name", "unknown")
                    progress = torrent.get("progress", torrent.get("percentDone", 0))
                    # Normalize progress (API may return 0-1 or 0-100)
                    if isinstance(progress, (int, float)) and progress <= 1:
                        progress = progress * 100

                    # Transmission error
                    if error_val and int(error_val) > 0:
                        findings.append(make_finding(
                            "media", "download_stalled", "global",
                            f"transmission/{name[:50]}", "warning",
                            f"Transmission download error: {name} (error: {torrent.get('errorString', error_val)})",
                            {"name": name, "error": error_val, "errorString": torrent.get("errorString", "")},
                        ))
                    # Completed but still in client (stopped or seeding) — stale download
                    elif progress >= 100:
                        stale_completed.append(name)

                # Report stale completed torrents as a batch finding
                if stale_completed:
                    findings.append(make_finding(
                        "media", "download_cleanup_needed", "global",
                        "transmission/stale-completed", "warning",
                        f"{len(stale_completed)} completed torrent(s) still in Transmission — not cleaned up by Sonarr/Radarr/Cleanuparr",
                        {"count": len(stale_completed), "torrents": [n[:60] for n in stale_completed[:10]]},
                    ))
        except Exception as e:
            logger.error(f"Transmission check error: {e}")

        # --- SABnzbd active queue ---
        try:
            result = await call_mcp_tool_safe(media, "sabnzbd_get_queue")
            slots = result.get("slots", result.get("items", result.get("result", [])))
            if isinstance(slots, list):
                for slot in slots:
                    if not isinstance(slot, dict):
                        continue
                    status = slot.get("status", "").lower()
                    name = slot.get("filename", slot.get("name", "unknown"))
                    if status in ("failed", "paused"):
                        findings.append(make_finding(
                            "media", "download_stalled", "global",
                            f"sabnzbd/{name[:50]}", "warning",
                            f"SABnzbd download {status}: {name}",
                            {"name": name, "status": status},
                        ))
        except Exception as e:
            logger.error(f"SABnzbd queue check error: {e}")

        # --- SABnzbd history failures ---
        try:
            result = await call_mcp_tool_safe(media, "sabnzbd_get_history", {"limit": 30})
            history = result.get("result", result.get("slots", []))
            if isinstance(history, list):
                failed_items = []
                for item in history:
                    if not isinstance(item, dict):
                        continue
                    status = item.get("status", "").lower()
                    name = item.get("name", "unknown")
                    if status == "failed":
                        failed_items.append(name)
                if failed_items:
                    findings.append(make_finding(
                        "media", "sabnzbd_history_failures", "global",
                        "sabnzbd/history-failures", "warning",
                        f"{len(failed_items)} failed download(s) in SABnzbd history",
                        {"count": len(failed_items), "items": [n[:60] for n in failed_items[:10]]},
                    ))
        except Exception as e:
            logger.error(f"SABnzbd history check error: {e}")

        # --- Cleanuparr health + job status ---
        try:
            result = await call_mcp_tool_safe(media, "cleanuparr_health")
            if result.get("_error"):
                findings.append(make_finding(
                    "media", "media_tool_unhealthy", "global",
                    "cleanuparr/health", "warning",
                    "Cleanuparr unreachable or unhealthy",
                    {"error": str(result.get("message", ""))},
                ))
            else:
                # Check if any connected client is unhealthy
                for client_id, client_info in result.items():
                    if isinstance(client_info, dict) and not client_info.get("isHealthy", True):
                        cname = client_info.get("clientName", client_id)
                        findings.append(make_finding(
                            "media", "media_tool_unhealthy", "global",
                            f"cleanuparr/{cname}", "warning",
                            f"Cleanuparr client '{cname}' unhealthy: {client_info.get('errorMessage', 'unknown')}",
                            {"client": cname, "error": client_info.get("errorMessage", "")},
                        ))
            # Check if cleanuparr has any jobs configured
            jobs_result = await call_mcp_tool_safe(media, "cleanuparr_list_jobs")
            jobs = jobs_result.get("result", jobs_result.get("items", []))
            if isinstance(jobs, list) and len(jobs) == 0:
                findings.append(make_finding(
                    "media", "media_tool_misconfigured", "global",
                    "cleanuparr/no-jobs", "warning",
                    "Cleanuparr has no cleanup jobs configured — stale downloads will accumulate",
                    {"detail": "No queue_cleaner, seeding_cleaner, or orphan_cleaner jobs found"},
                ))
        except Exception as e:
            logger.error(f"Cleanuparr check error: {e}")

        # --- Huntarr health (scheduled 00:00-08:00 UTC only) ---
        try:
            utc_hour = datetime.now(timezone.utc).hour
            # Huntarr runs 00:00-08:00 UTC via CronJob scale schedule
            if 0 <= utc_hour < 8:
                result = await call_mcp_tool_safe(media, "huntarr_get_status")
                if result.get("error") or result.get("_error"):
                    findings.append(make_finding(
                        "media", "media_tool_unhealthy", "global",
                        "huntarr/health", "warning",
                        f"Huntarr unreachable during scheduled uptime (00-08 UTC): {str(result.get('error', result.get('message', '')))[:100]}",
                        {"error": str(result.get("error", result.get("message", "")))[:200],
                         "note": "Huntarr should be running 00:00-08:00 UTC"},
                    ))
        except Exception as e:
            logger.error(f"Huntarr check error: {e}")

        return findings

    # ========================================================================
    # CHECK: HOME AUTOMATION (HA, Tasmota, AdGuard Filtering)
    # ========================================================================

    async def check_home_automation() -> list:
        """Check Home Assistant, Tasmota devices, and AdGuard filter freshness."""
        findings = []
        home = HOME_MCP_URL

        # --- Home Assistant entity availability ---
        try:
            result = await call_mcp_tool_safe(home, "get_home_overview")
            if isinstance(result, dict):
                # Count unavailable entities
                unavailable = []
                entities = result.get("entities", result.get("items", result.get("result", [])))
                if isinstance(entities, list):
                    for entity in entities:
                        if isinstance(entity, dict):
                            state = entity.get("state", "")
                            if state == "unavailable":
                                unavailable.append(entity.get("entity_id", entity.get("name", "unknown")))
                elif isinstance(result, dict) and "unavailable" in result:
                    unavailable_count = result.get("unavailable", 0)
                    if isinstance(unavailable_count, int) and unavailable_count > 3:
                        findings.append(make_finding(
                            "home", "ha_entities_unavailable", "global",
                            "homeassistant/entities", "warning",
                            f"Home Assistant: {unavailable_count} entities unavailable",
                            {"count": unavailable_count},
                        ))
                        return findings

                if len(unavailable) > 3:
                    findings.append(make_finding(
                        "home", "ha_entities_unavailable", "global",
                        "homeassistant/entities", "warning",
                        f"Home Assistant: {len(unavailable)} entities unavailable ({', '.join(unavailable[:5])}...)",
                        {"count": len(unavailable), "entities": unavailable[:20]},
                        "Check network connectivity to smart home devices",
                    ))
        except Exception as e:
            logger.error(f"Home Assistant check error: {e}")

        # --- Tasmota device status ---
        try:
            result = await call_mcp_tool_safe(home, "tasmota_status_all", timeout=30.0)
            devices = result.get("items", result.get("devices", result.get("result", [])))
            if isinstance(devices, list):
                for device in devices:
                    if not isinstance(device, dict):
                        continue
                    name = device.get("name", device.get("ip", "unknown"))
                    online = device.get("online", device.get("reachable", True))
                    if online is False or (isinstance(online, str) and online.lower() == "false"):
                        findings.append(make_finding(
                            "home", "tasmota_device_offline", "global",
                            f"tasmota/{name}", "warning",
                            f"Tasmota device '{name}' offline",
                            {"name": name, "ip": device.get("ip", "")},
                        ))
        except Exception as e:
            logger.error(f"Tasmota check error: {e}")

        # --- AdGuard filter list freshness ---
        try:
            result = await call_mcp_tool_safe(home, "adguard_get_filtering_status")
            filters_data = result.get("filters", result.get("result", []))
            if isinstance(filters_data, list):
                now = datetime.utcnow()
                for f in filters_data:
                    if not isinstance(f, dict):
                        continue
                    name = f.get("name", "unknown")
                    enabled = f.get("enabled", True)
                    last_updated = f.get("last_updated", "")
                    if not enabled or not last_updated:
                        continue
                    try:
                        updated_dt = datetime.fromisoformat(last_updated.replace("Z", "+00:00").replace("+00:00", ""))
                        age_days = (now - updated_dt).days
                        if age_days > 7:
                            findings.append(make_finding(
                                "home", "adguard_filters_stale", "global",
                                f"adguard/filter/{name[:40]}", "warning",
                                f"AdGuard filter '{name}' is {age_days} days stale",
                                {"filter": name, "last_updated": last_updated, "age_days": age_days},
                                "Update AdGuard filter lists in Settings → Filters",
                            ))
                    except (ValueError, TypeError):
                        pass
        except Exception as e:
            logger.error(f"AdGuard filter check error: {e}")

        return findings

    # ========================================================================
    # CHECK: CONTAINER IMAGE FRESHNESS
    # ========================================================================

    async def check_container_images() -> list:
        """Check for outdated container images across all clusters.

        For :latest tagged images, compare running digest against registry.
        For pinned tags, check if newer versions exist via GitHub API.
        """
        findings = []
        infra = INFRASTRUCTURE_MCP_URL

        # Key deployments to check (namespace, deployment, cluster, ghcr_repo)
        TRACKED_IMAGES = [
            # Agentic cluster — MCP servers (all :latest)
            ("ai-platform", "observability-mcp", "agentic", "ghcr.io/charlieshreck/mcp-observability"),
            ("ai-platform", "infrastructure-mcp", "agentic", "ghcr.io/charlieshreck/mcp-infrastructure"),
            ("ai-platform", "knowledge-mcp", "agentic", "ghcr.io/charlieshreck/mcp-knowledge"),
            ("ai-platform", "home-mcp", "agentic", "ghcr.io/charlieshreck/mcp-home"),
            ("ai-platform", "media-mcp", "agentic", "ghcr.io/charlieshreck/mcp-media"),
            ("ai-platform", "external-mcp", "agentic", "ghcr.io/charlieshreck/mcp-external"),
            # Agentic cluster — platform
            ("ai-platform", "error-hunter", "agentic", "ghcr.io/charlieshreck/error-hunter"),
            ("ai-platform", "langgraph", "agentic", "ghcr.io/charlieshreck/kao-langgraph"),
            # Prod cluster — media apps (pinned tags)
            ("media", "sonarr", "prod", "ghcr.io/linuxserver/sonarr"),
            ("media", "radarr", "prod", "ghcr.io/linuxserver/radarr"),
            ("media", "prowlarr", "prod", "ghcr.io/linuxserver/prowlarr"),
            ("media", "overseerr", "prod", "ghcr.io/linuxserver/overseerr"),
            ("media", "tautulli", "prod", "ghcr.io/linuxserver/tautulli"),
            ("media", "cleanuparr", "prod", "ghcr.io/cleanuparr/cleanuparr"),
        ]

        for namespace, deploy_name, cluster, ghcr_repo in TRACKED_IMAGES:
            try:
                ctx = CLUSTERS.get(cluster, cluster)
                # Get running pod image details via kubectl
                result = await call_mcp_tool_safe(infra, "kubectl_get_pods", {
                    "namespace": namespace, "cluster": ctx,
                })
                pod_text = str(result)

                # Find the image tag currently running
                import re
                # Look for image string matching this deployment
                short_name = ghcr_repo.split("/")[-1]
                image_match = re.search(rf'{re.escape(short_name)}[:\s]+(\S+)', pod_text)
                if not image_match:
                    continue
                running_tag = image_match.group(1).strip()

                # For :latest images, check if pod was recently restarted
                # (indicating it might have pulled a new image)
                is_latest = running_tag == "latest" or ":" not in running_tag or running_tag.endswith(":latest")

                if is_latest:
                    # Check pod age — if older than 7 days, the image may be stale
                    # Look for restart count and age
                    age_match = re.search(rf'{deploy_name}\S*\s+\d+/\d+\s+\w+\s+\d+\s+(\d+[dhms]+)', pod_text)
                    if age_match:
                        age_str = age_match.group(1)
                        days = 0
                        d_match = re.search(r'(\d+)d', age_str)
                        if d_match:
                            days = int(d_match.group(1))
                        if days >= 7:
                            findings.append(make_finding(
                                "platform", "container_image_outdated", cluster,
                                f"{namespace}/{deploy_name}", "info",
                                f"{deploy_name} running :latest image for {days}d — may be stale. Consider restart to pull fresh image.",
                                {"deployment": deploy_name, "namespace": namespace,
                                 "cluster": cluster, "image": ghcr_repo,
                                 "tag": "latest", "age_days": days},
                                "Restart deployment to pull latest image: kubectl rollout restart",
                            ))
                else:
                    # Pinned tag — check GitHub Container Registry for newer tags
                    # Use the external MCP GitHub API to check releases
                    # Extract owner/repo from ghcr.io path
                    parts = ghcr_repo.replace("ghcr.io/", "").split("/")
                    if len(parts) >= 2:
                        owner = parts[0]
                        repo = parts[1]
                        try:
                            releases_result = await call_mcp_tool_safe(
                                EXTERNAL_MCP_URL, "github_get_commits",
                                {"owner": owner, "repo": repo, "per_page": 1}
                            )
                            # If we got a result, check if tag is different
                            if isinstance(releases_result, dict) and not releases_result.get("_error"):
                                # Just flag as info — we found the deployment has a pinned tag
                                # Renovate should handle the actual update
                                pass
                        except Exception:
                            pass  # Non-critical, skip if GitHub API fails

            except Exception as e:
                logger.debug(f"Image check failed for {deploy_name}: {e}")
                continue

        return findings

    # ========================================================================
    # CHECK: OPEN KAO INCIDENTS
    # ========================================================================

    async def check_open_incidents() -> list:
        """Pick up open KAO incidents and convert them to findings for unified processing."""
        findings = []
        try:
            rows = await pg_fetch("""
                SELECT id, alert_name, severity, source, description, status,
                       detected_at, enrichment, labels
                FROM incidents
                WHERE status IN ('detected', 'investigating', 'awaiting_review')
                ORDER BY detected_at ASC
            """)
            for row in rows:
                existing = await dedup_incident_to_finding(row["id"], row["alert_name"])
                if existing:
                    continue  # Already linked to an existing finding

                # Auto-resolve transient Coroot media Recreate-strategy rollout alerts.
                # These fire during normal rollouts (~25s gap) and self-heal immediately.
                # Real failures are caught by pod/deployment checks independently.
                alert_name = row.get("alert_name", "")
                source = row.get("source", "")

                # Auto-resolve pulse-check alerts (source="pulse", alert_name="check").
                # These are generic heartbeat signals fired by pulse-agent with no resource
                # or error detail. Consistently false positive — always self-heal immediately.
                if source == "pulse" and alert_name == "check":
                    await pg_execute(
                        """UPDATE incidents SET status = 'resolved', resolved_at = NOW(),
                           resolution_summary = $2 WHERE id = $1""",
                        row["id"],
                        "Auto-resolved: generic pulse health-check — no resource/error detail, "
                        "consistently false positive (pulse-agent heartbeat, no actionable signal)."
                    )
                    logger.info(f"Auto-resolved pulse-check incident #{row['id']}")
                    continue

                # Auto-resolve Plex VM CPU alerts from Pulse.
                # Plex legitimately spikes to 80-95%+ during transcoding, library scans,
                # and media analysis. These are always transient (minutes) and self-heal.
                # Real Plex failures are caught by plex_unreachable and Coroot checks.
                if source == "pulse" and "cpu" in alert_name.lower() and "plex" in alert_name.lower():
                    await pg_execute(
                        """UPDATE incidents SET status = 'resolved', resolved_at = NOW(),
                           resolution_summary = $2 WHERE id = $1""",
                        row["id"],
                        "Auto-resolved: Plex VM CPU spike from Pulse — expected transient behavior "
                        "during transcoding/library scanning. Plex health monitored separately."
                    )
                    logger.info(f"Auto-resolved Plex CPU pulse incident #{row['id']}: {alert_name}")
                    continue

                if source == "coroot" and "@media" in alert_name:
                    app_name = alert_name.split("@")[0]
                    if app_name in MEDIA_RECREATE_APPS:
                        await pg_execute(
                            """UPDATE incidents SET status = 'resolved', resolved_at = NOW(),
                               resolution_summary = $2 WHERE id = $1""",
                            row["id"],
                            f"Auto-resolved: transient Coroot Recreate-strategy rollout alert for {app_name}@media"
                        )
                        logger.info(f"Auto-resolved transient Coroot media incident #{row['id']}: {alert_name}")
                        continue

                findings.append(make_finding(
                    "kao_incident", row["alert_name"], "global",
                    f"incident/{row['id']}", row.get("severity", "warning"),
                    f"KAO incident #{row['id']}: {(row.get('description') or '')[:200]}",
                    {"incident_id": row["id"], "source": row.get("source", "unknown"),
                     "status": row["status"], "enrichment": row.get("enrichment", {})},
                    "Investigate and remediate via MCP tools",
                ))
        except Exception as e:
            logger.error(f"Failed to check open incidents: {e}")
        return findings

    # ========================================================================
    # CHECK: STALE INCIDENTS
    # ========================================================================

    async def check_stale_incidents() -> list:
        """Find and auto-resolve stale KAO incidents AND stale error-hunter findings.

        Incidents stuck in non-terminal states for >6h get auto-resolved.
        Findings stuck as 'dispatched' for >4h get reset to 'new' so the
        next sweep re-dispatches them. Findings stuck as 'new' for >2h
        (missed dispatch) also get flagged.
        """
        findings = []

        # -- Stale KAO incidents --
        try:
            stale_rows = await pg_fetch("""
                SELECT id, alert_name, status, screen_session, detected_at,
                       source, description,
                       EXTRACT(EPOCH FROM (NOW() - detected_at)) / 3600 AS age_hours
                FROM incidents
                WHERE status IN ('detected', 'investigating', 'mitigating')
                  AND detected_at < NOW() - INTERVAL '6 hours'
                ORDER BY detected_at ASC
            """)

            # Convert stale incidents to findings instead of auto-closing
            for row in stale_rows:
                age_hours = (datetime.utcnow() - row["detected_at"]).total_seconds() / 3600
                findings.append(make_finding(
                    "kao_incident", f"stale_{row['alert_name']}", "global",
                    f"incident/{row['id']}", "warning",
                    f"Stale incident #{row['id']} ({age_hours:.0f}h old): {(row.get('description') or '')[:200]}",
                    {"incident_id": row["id"], "source": row.get("source", "unknown"),
                     "status": row["status"], "age_hours": age_hours},
                    "Auto-remediate or escalate — do not auto-close",
                ))

        except Exception as e:
            logger.error(f"Stale incident check error: {e}")

        # -- Stale dispatched findings (screen died without resolving) --
        try:
            stale_dispatched = await pg_fetch("""
                SELECT id, check_name, screen_session, description,
                       EXTRACT(EPOCH FROM (NOW() - last_seen)) / 3600 AS age_hours
                FROM error_hunter_findings
                WHERE status = 'dispatched'
                  AND last_seen < NOW() - INTERVAL '1 hour'
                ORDER BY last_seen ASC
            """)

            if stale_dispatched:
                reset_ids = [row["id"] for row in stale_dispatched]
                await pg_execute("""
                    UPDATE error_hunter_findings
                    SET status = 'new', screen_session = NULL
                    WHERE id = ANY($1::int[])
                      AND status = 'dispatched'
                """, reset_ids)
                logger.info(f"Reset {len(reset_ids)} stale dispatched findings to 'new': {reset_ids}")
                findings.append(make_finding(
                    "operations", "findings_stale_dispatched", "global",
                    "error-hunter/stale-dispatched", "info",
                    f"Reset {len(reset_ids)} stale dispatched finding(s) to 'new' — screen sessions died without resolving",
                    {"count": len(reset_ids), "ids": reset_ids[:20]},
                    "Findings will be re-dispatched on next sweep",
                ))

        except Exception as e:
            logger.error(f"Stale dispatched findings check error: {e}")

        # -- Orphaned 'new' findings that missed dispatch --
        try:
            orphaned_new = await pg_fetch("""
                SELECT id, check_name, classification, description,
                       EXTRACT(EPOCH FROM (NOW() - first_seen)) / 3600 AS age_hours
                FROM error_hunter_findings
                WHERE status = 'new'
                  AND first_seen < NOW() - INTERVAL '2 hours'
                ORDER BY first_seen ASC
            """)

            if orphaned_new:
                # These will be picked up by the current sweep's dispatch
                # Just log awareness
                names = [row["check_name"] for row in orphaned_new]
                logger.info(f"Found {len(orphaned_new)} orphaned 'new' findings older than 2h: {names}")

        except Exception as e:
            logger.error(f"Orphaned findings check error: {e}")

        return findings

    # ========================================================================
    # CHECK: OPNSENSE UPDATES & SECURITY
    # ========================================================================

    async def check_opnsense_updates() -> list:
        """Check OPNsense firmware updates, package security audit, and plugin updates."""
        findings = []
        infra = INFRASTRUCTURE_MCP_URL

        # -- Firmware updates --
        try:
            fw = await call_mcp_tool_safe(infra, "check_firmware_updates", timeout=30.0)
            if fw:
                # Parse firmware update info
                updates_avail = False
                fw_text = ""
                if isinstance(fw, dict):
                    updates_avail = fw.get("updates_available", fw.get("needs_reboot", False))
                    new_ver = fw.get("new_version", fw.get("upgrade_version", ""))
                    cur_ver = fw.get("current_version", fw.get("product_version", ""))
                    pkg_count = fw.get("updates_count", fw.get("package_updates", 0))
                    fw_text = str(fw)
                    if new_ver and new_ver != cur_ver:
                        updates_avail = True
                    if isinstance(pkg_count, int) and pkg_count > 0:
                        updates_avail = True
                elif isinstance(fw, str):
                    fw_text = fw
                    updates_avail = any(kw in fw.lower() for kw in ["update", "upgrade", "available"])

                if updates_avail:
                    findings.append(make_finding(
                        "infrastructure", "opnsense_firmware_update", "global",
                        "opnsense/firmware", "info",
                        f"OPNsense firmware update available",
                        {"raw": fw_text[:500]},
                        "Apply firmware update via OPNsense UI → System → Firmware",
                    ))
        except Exception as e:
            logger.error(f"OPNsense firmware check error: {e}")

        # -- Package security audit --
        try:
            audit = await call_mcp_tool_safe(infra, "get_pkg_audit", timeout=30.0)
            if audit:
                vulns = []
                if isinstance(audit, dict):
                    vulns = audit.get("vulnerabilities", audit.get("audit", []))
                    if isinstance(vulns, str) and "0 problem" in vulns:
                        vulns = []
                    elif isinstance(vulns, str):
                        # Parse "N problem(s)" format
                        import re
                        m = re.search(r"(\d+)\s+problem", vulns)
                        if m and int(m.group(1)) > 0:
                            findings.append(make_finding(
                                "infrastructure", "opnsense_pkg_vuln", "global",
                                "opnsense/pkg-audit", "warning",
                                f"OPNsense package security: {vulns[:200]}",
                                {"raw": str(audit)[:500]},
                                "Review pkg audit and update affected packages",
                            ))
                        vulns = []
                if isinstance(vulns, list) and len(vulns) > 0:
                    findings.append(make_finding(
                        "infrastructure", "opnsense_pkg_vuln", "global",
                        "opnsense/pkg-audit", "warning",
                        f"OPNsense has {len(vulns)} package vulnerability(ies)",
                        {"count": len(vulns), "vulns": str(vulns)[:500]},
                        "Update vulnerable packages via OPNsense firmware page",
                    ))
        except Exception as e:
            logger.error(f"OPNsense pkg audit check error: {e}")

        # -- Installed plugins --
        try:
            plugins = await call_mcp_tool_safe(infra, "list_installed_plugins", timeout=30.0)
            if plugins:
                plugin_list = extract_list(plugins, "plugins", "result")
                outdated = []
                for p in plugin_list:
                    if not isinstance(p, dict):
                        continue
                    name = p.get("name", "unknown")
                    installed = p.get("installed_version", p.get("version", ""))
                    available = p.get("available_version", p.get("latest", ""))
                    needs_update = p.get("outdated", False)
                    if needs_update or (available and installed and available != installed):
                        outdated.append(name)
                if outdated:
                    findings.append(make_finding(
                        "infrastructure", "opnsense_plugin_update", "global",
                        "opnsense/plugins", "info",
                        f"{len(outdated)} OPNsense plugin(s) have updates: {', '.join(outdated[:5])}",
                        {"plugins": outdated},
                        "Update plugins via OPNsense UI → System → Firmware → Plugins",
                    ))
        except Exception as e:
            logger.error(f"OPNsense plugin check error: {e}")

        return findings

    # ========================================================================
    # CHECK: CLOUDFLARE HEALTH
    # ========================================================================

    async def check_cloudflare_health() -> list:
        """Check Cloudflare tunnel connections and zone status."""
        findings = []
        infra = INFRASTRUCTURE_MCP_URL

        # -- Tunnel connection details --
        try:
            tunnels_result = await call_mcp_tool_safe(infra, "cloudflare_list_tunnels",
                                                       {"params": {"response_format": "json"}}, timeout=30.0)
            tunnels = extract_list(tunnels_result, "tunnels")
            for tunnel in tunnels:
                if not isinstance(tunnel, dict):
                    continue
                t_id = tunnel.get("id", "")
                t_name = tunnel.get("name", "unknown")
                t_status = str(tunnel.get("status", "")).lower()

                # If tunnel has an ID, get detailed connection info
                if t_id and len(t_id) == 36:
                    try:
                        detail = await call_mcp_tool_safe(
                            infra, "cloudflare_get_tunnel_status",
                            {"params": {"tunnel_id": t_id}}, timeout=20.0)
                        if detail and isinstance(detail, dict):
                            conns = detail.get("connections", detail.get("conns", []))
                            if isinstance(conns, list) and len(conns) == 0:
                                findings.append(make_finding(
                                    "infrastructure", "cloudflare_tunnel_no_connections", "global",
                                    f"cloudflare/tunnel-{t_name}/conns", "warning",
                                    f"Cloudflare tunnel '{t_name}' has 0 active connections",
                                    {"tunnel": t_name, "tunnel_id": t_id},
                                    "Check cloudflared pod in prod cluster",
                                ))
                    except Exception as e:
                        logger.warning(f"Tunnel detail check error for {t_name}: {e}")
        except Exception as e:
            logger.error(f"Cloudflare tunnel detail check error: {e}")

        # -- Zone status --
        try:
            zones_result = await call_mcp_tool_safe(infra, "cloudflare_list_zones",
                                                     {"params": {"response_format": "json"}}, timeout=20.0)
            zones = extract_list(zones_result, "zones")
            for zone in zones:
                if not isinstance(zone, dict):
                    continue
                z_name = zone.get("name", "unknown")
                z_status = str(zone.get("status", "")).lower()
                if z_status not in ("active", ""):
                    findings.append(make_finding(
                        "infrastructure", "cloudflare_zone_inactive", "global",
                        f"cloudflare/zone-{z_name}", "warning",
                        f"Cloudflare zone '{z_name}' status: {z_status}",
                        {"zone": z_name, "status": z_status},
                        "Check Cloudflare dashboard for zone issues",
                    ))
        except Exception as e:
            logger.error(f"Cloudflare zone check error: {e}")

        return findings

    # ========================================================================
    # CHECK: PROXMOX CLUSTER HEALTH
    # ========================================================================

    async def check_proxmox_health() -> list:
        """Check Proxmox cluster quorum, recent failed tasks, and node health."""
        findings = []
        infra = INFRASTRUCTURE_MCP_URL

        # -- Cluster status / quorum --
        try:
            cluster = await call_mcp_tool_safe(
                infra, "proxmox_get_cluster_status",
                {"params": {"host": "all", "response_format": "json"}}, timeout=30.0)
            if cluster and isinstance(cluster, dict):
                nodes = extract_list(cluster, "nodes", "result")
                for node in nodes:
                    if not isinstance(node, dict):
                        continue
                    name = node.get("name", "unknown")
                    online = node.get("online", node.get("status", ""))
                    if str(online) in ("0", "false", "offline", "False"):
                        findings.append(make_finding(
                            "infrastructure", "proxmox_node_offline", "global",
                            f"proxmox/node-{name}", "critical",
                            f"Proxmox node '{name}' is offline",
                            {"node": name, "online": online},
                            "Check physical host or VM hosting Proxmox",
                        ))
        except Exception as e:
            logger.error(f"Proxmox cluster status check error: {e}")

        # -- Recent failed tasks --
        PROXMOX_HOSTS = ["ruapehu", "pihanga", "hikurangi"]

        async def _check_tasks(host: str) -> list:
            host_findings = []
            try:
                tasks = await call_mcp_tool_safe(
                    infra, "proxmox_list_tasks",
                    {"params": {"host": host, "response_format": "json"}}, timeout=30.0)
                if not tasks:
                    return host_findings
                task_list = extract_list(tasks, "tasks", "result")
                now_ts = datetime.utcnow().timestamp()
                recent_failures = []
                for t in task_list:
                    if not isinstance(t, dict):
                        continue
                    status = str(t.get("status", "")).lower()
                    if status not in ("error", "failed"):
                        continue
                    # Only last 24h
                    end_time = t.get("endtime", t.get("end_time", 0))
                    if isinstance(end_time, (int, float)) and (now_ts - end_time) < 86400:
                        task_type = t.get("type", t.get("worker_type", "unknown"))
                        vmid = t.get("id", t.get("upid", ""))
                        recent_failures.append(f"{task_type}({vmid})")

                if recent_failures:
                    host_findings.append(make_finding(
                        "infrastructure", "proxmox_task_failed", "global",
                        f"proxmox/{host}/tasks", "warning",
                        f"Proxmox {host}: {len(recent_failures)} failed task(s) in last 24h: {', '.join(recent_failures[:5])}",
                        {"host": host, "failed_tasks": recent_failures[:10]},
                        "Check Proxmox task log for details",
                    ))
            except Exception as e:
                logger.error(f"Proxmox task check error for {host}: {e}")
            return host_findings

        task_results = await asyncio.gather(
            *[_check_tasks(h) for h in PROXMOX_HOSTS],
            return_exceptions=True,
        )
        for result in task_results:
            if isinstance(result, list):
                findings.extend(result)
            elif isinstance(result, Exception):
                logger.error(f"Proxmox task check failed: {result}")

        return findings

    # ========================================================================
    # LANGGRAPH STATE AND NODES
    # ========================================================================

    class SweepState(TypedDict):
        sweep_id: str
        sweep_time: str
        raw_findings: list
        deduplicated_findings: list
        fix_now: list
        create_rule: list
        log_only: list
        screens_created: list
        rules_written: list
        runbooks_drafted: list
        remediation_results: list
        summary: dict
        status: str

    async def sweep_estate(state: SweepState) -> dict:
        """Node 1: Run all checks in parallel across the estate."""
        sweep_id = state["sweep_id"]
        logger.info(f"[{sweep_id}] Starting estate sweep...")
        await report_progress(sweep_id, "Scanning clusters", "3 clusters, 22 checks", 10)

        # Record sweep start
        try:
            await pg_execute(
                "INSERT INTO sweep_runs (sweep_id, status) VALUES ($1, 'running')",
                sweep_id
            )
        except Exception as e:
            logger.warning(f"Failed to record sweep start: {e}")

        # Run all checks concurrently
        tasks = [
            check_kubernetes("production"),
            check_kubernetes("agentic"),
            check_kubernetes("monitoring"),
            check_infrastructure(),
            check_observability(),
            check_logs("production"),
            check_logs("agentic"),
            check_logs("monitoring"),
            check_mcp_health(),
            check_renovate(),
            check_beszel(),
            check_dns_routing(),
            check_backups(),
            check_network_health(),
            check_nfs_storage(),
            check_homepage(),
            check_media_pipeline(),
            check_home_automation(),
            check_container_images(),
            check_open_incidents(),
            check_stale_incidents(),
            check_opnsense_updates(),
            check_cloudflare_health(),
            check_proxmox_health(),
        ]

        results = await asyncio.gather(*tasks, return_exceptions=True)

        all_findings = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"Check task {i} failed: {result}")
            elif isinstance(result, list):
                all_findings.extend(result)

        logger.info(f"[{sweep_id}] Sweep complete: {len(all_findings)} raw findings")
        await report_progress(sweep_id, "Scan complete", f"{len(all_findings)} raw findings", 30)
        return {"raw_findings": all_findings}

    async def deduplicate(state: SweepState) -> dict:
        """Node 2: Remove findings already being handled or previously seen."""
        sweep_id = state["sweep_id"]
        await report_progress(sweep_id, "Deduplicating", "Removing known issues", 40)
        findings = state.get("raw_findings", [])
        deduplicated = []

        for finding in findings:
            fp = finding["fingerprint"]

            try:
                # Check if already being tracked and not resolved
                existing = await pg_fetchrow("""
                    SELECT id, status, screen_session, rule_written
                    FROM error_hunter_findings
                    WHERE fingerprint = $1 AND status NOT IN ('resolved', 'superseded', 'ignored')
                    ORDER BY last_seen DESC LIMIT 1
                """, fp)

                if existing:
                    # Update last_seen
                    await pg_execute(
                        "UPDATE error_hunter_findings SET last_seen = NOW() WHERE id = $1",
                        existing["id"]
                    )
                    # Skip if already dispatched
                    if existing["status"] == "dispatched":
                        continue
                    # Skip if rule already written
                    if existing["rule_written"]:
                        continue

                # Check if KAO has an active incident for this
                active_incident = await pg_fetchrow("""
                    SELECT id FROM incidents
                    WHERE fingerprint = $1 AND status NOT IN ('resolved', 'closed', 'false_positive')
                """, fp)
                if active_incident:
                    continue

                deduplicated.append(finding)

            except Exception as e:
                logger.warning(f"Dedup check error for {fp}: {e}")
                deduplicated.append(finding)

        logger.info(f"[{sweep_id}] Dedup: {len(findings)} -> {len(deduplicated)} findings")
        return {"deduplicated_findings": deduplicated}

    async def classify(state: SweepState) -> dict:
        """Node 3: Classify findings into fix_now / create_rule / log_only.

        Phase 1: severity-based heuristics. Phase 6 will add LLM classification.
        """
        await report_progress(state["sweep_id"], "Classifying", "Severity analysis", 55)
        sweep_id = state["sweep_id"]
        findings = state.get("deduplicated_findings", [])

        fix_now = []
        create_rule = []
        log_only = []

        for finding in findings:
            severity = finding.get("severity", "info")
            category = finding.get("check_category", "")
            check_name = finding.get("check_name", "")

            resource = finding.get("resource", "")
            is_routine_cronjob = any(resource.split("/")[-1].startswith(p) for p in ROUTINE_CRONJOB_PREFIXES)

            # Critical findings always go to fix_now
            if severity == "critical":
                fix_now.append(finding)
            # Routine CronJob failures are expected noise — log only
            elif check_name == "job_failed" and is_routine_cronjob:
                log_only.append(finding)
            # Active failures: pods down, deployments unavailable
            elif check_name in ("pod_not_running", "deployment_unavailable", "job_failed",
                                "statefulset_not_ready", "node_not_ready"):
                fix_now.append(finding)
            # MCP health: broken tools and unreachable servers need fixing
            elif check_name in ("mcp_http_down", "mcp_unreachable", "mcp_tool_broken"):
                fix_now.append(finding)
            # KAO incidents picked up from PostgreSQL
            elif check_name.startswith("kao_incident"):
                severity = finding.get("severity", "warning")
                if severity == "critical":
                    fix_now.append(finding)
                else:
                    create_rule.append(finding)
            # ArgoCD drift needs attention but is less urgent
            elif check_name.startswith("argocd_"):
                create_rule.append(finding)
            # Trending issues need a rule but not immediate fix
            elif check_name in ("pod_restart_trending", "warning_events_accumulating",
                                "pod_not_ready", "deployment_rollout_stuck"):
                create_rule.append(finding)
            # Observability gaps (missing scrape targets, etc.)
            elif check_name in ("scrape_target_down", "gatus_endpoint_failing",
                                "gatus_endpoint_flapping"):
                create_rule.append(finding)
            # Log errors: create rules for patterns found
            elif check_name == "log_error_pattern":
                create_rule.append(finding)
            # MCP empty responses: informational, track trends
            elif check_name == "mcp_tool_empty_response":
                log_only.append(finding)
            # Renovate PRs: auto-merged ones are log_only, pending are create_rule
            elif check_name == "renovate_pr_pending":
                if finding.get("evidence", {}).get("auto_merged"):
                    log_only.append(finding)
                else:
                    create_rule.append(finding)
            # DNS/routing — broken backends need immediate fix
            elif check_name == "caddy_backend_unreachable":
                fix_now.append(finding)
            # Backup failures are urgent
            elif check_name in ("velero_backup_failed", "velero_storage_unavailable",
                                "backrest_plan_overdue", "pbs_backup_stale"):
                fix_now.append(finding)
            # Network — switch offline is urgent
            elif check_name == "omada_switch_offline":
                fix_now.append(finding)
            elif check_name in ("omada_lag_degraded", "unifi_subsystem_degraded",
                                "unifi_alarm", "ntopng_alert"):
                create_rule.append(finding)
            # NFS — missing share or canary failure is urgent
            elif check_name in ("nfs_share_missing", "mount_canary_failed"):
                fix_now.append(finding)
            # Backrest unreachable — restart pod
            elif check_name == "backrest_unreachable":
                fix_now.append(finding)
            # Plex unreachable is urgent
            elif check_name == "plex_unreachable":
                fix_now.append(finding)
            # Media/home/homepage — create rules, not urgent
            elif check_name in ("download_cleanup_needed", "arr_queue_orphaned"):
                fix_now.append(finding)
            elif check_name in ("arr_queue_stuck", "prowlarr_indexer_failed",
                                "download_stalled",
                                "sabnzbd_history_failures", "media_tool_unhealthy",
                                "media_tool_misconfigured", "container_image_outdated",
                                "homepage_service_unreachable",
                                "homepage_category_degraded",
                                "ha_entities_unavailable", "adguard_filters_stale",
                                "tasmota_device_offline"):
                create_rule.append(finding)
            # Proxmox node offline is critical
            elif check_name == "proxmox_node_offline":
                fix_now.append(finding)
            # Cloudflare tunnel no connections is urgent
            elif check_name == "cloudflare_tunnel_no_connections":
                fix_now.append(finding)
            # OPNsense security vulns need attention
            elif check_name == "opnsense_pkg_vuln":
                create_rule.append(finding)
            # Proxmox failed tasks, zone inactive, LAG degrade
            elif check_name in ("proxmox_task_failed", "cloudflare_zone_inactive"):
                create_rule.append(finding)
            # Items that need investigation/rules
            elif check_name in ("velero_pod_unhealthy", "pbs_unreachable",
                                "k8s_pvc_pending", "incident_stale"):
                create_rule.append(finding)
            # Informational items
            elif check_name in ("dns_missing_route", "dns_duplicate_rewrite",
                                "nfs_snapshot_stale",
                                "opnsense_firmware_update",
                                "opnsense_plugin_update"):
                log_only.append(finding)
            # Informational
            elif severity == "info":
                log_only.append(finding)
            # Default: create a rule
            else:
                create_rule.append(finding)

        logger.info(f"[{sweep_id}] Classification: fix_now={len(fix_now)}, create_rule={len(create_rule)}, log_only={len(log_only)}")
        await report_progress(sweep_id, "Classified", f"{len(fix_now)} fix, {len(create_rule)} rules, {len(log_only)} log", 65)
        return {"fix_now": fix_now, "create_rule": create_rule, "log_only": log_only}

    # ========================================================================
    # AUTO-REMEDIATION (attempt fixes before dispatching to Claude screen)
    # ========================================================================

    ADGUARD_URL = os.environ.get("ADGUARD_URL", "http://10.10.0.1:3000")
    ADGUARD_USER = os.environ.get("ADGUARD_USER", "admin")
    ADGUARD_PASS = os.environ.get("ADGUARD_PASS", "")

    async def _remediate_argocd_out_of_sync(finding: dict) -> bool:
        """Sync an out-of-sync ArgoCD application."""
        app_name = finding.get("evidence", {}).get("app", "")
        if not app_name:
            return False
        result = await call_mcp_tool(INFRASTRUCTURE_MCP_URL, "argocd_sync_application", {"app_name": app_name})
        if result.get("_error"):
            logger.warning(f"ArgoCD sync failed for {app_name}: {result.get('message')}")
            return False
        logger.info(f"Auto-remediated: synced ArgoCD app '{app_name}'")
        return True

    async def _remediate_argocd_unhealthy(finding: dict) -> bool:
        """Sync a degraded ArgoCD application (often fixes itself on resync)."""
        app_name = finding.get("evidence", {}).get("app", "")
        if not app_name:
            return False
        result = await call_mcp_tool(INFRASTRUCTURE_MCP_URL, "argocd_sync_application", {"app_name": app_name})
        if result.get("_error"):
            return False
        logger.info(f"Auto-remediated: synced degraded ArgoCD app '{app_name}'")
        return True

    async def _remediate_adguard_filters_stale(finding: dict) -> bool:
        """Refresh AdGuard filter lists."""
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                resp = await client.post(
                    f"{ADGUARD_URL}/control/filtering/refresh",
                    json={"whitelist": False},
                    auth=(ADGUARD_USER, ADGUARD_PASS) if ADGUARD_PASS else None,
                )
                if resp.status_code == 200:
                    logger.info("Auto-remediated: refreshed AdGuard filter lists")
                    return True
                logger.warning(f"AdGuard filter refresh returned {resp.status_code}")
        except Exception as e:
            logger.warning(f"AdGuard filter refresh failed: {e}")
        return False

    async def _remediate_pod_not_running(finding: dict) -> bool:
        """Delete a CrashLoopBackOff/Error pod so K8s recreates it."""
        ev = finding.get("evidence", {})
        pod_name = ev.get("pod", "")
        namespace = ev.get("namespace", "")
        status = ev.get("status", "")
        cluster = finding.get("cluster", "")
        ctx = CLUSTERS.get(cluster, cluster)
        # Only bounce pods in recoverable error states
        if status not in ("CrashLoopBackOff", "Error", "OOMKilled"):
            return False
        if not pod_name or not namespace:
            return False
        result = await call_mcp_tool(INFRASTRUCTURE_MCP_URL, "kubectl_delete_pod", {
            "pod_name": pod_name, "namespace": namespace, "cluster": ctx,
        })
        if result.get("_error"):
            logger.warning(f"Pod delete failed for {namespace}/{pod_name}: {result.get('message')}")
            return False
        logger.info(f"Auto-remediated: bounced pod {namespace}/{pod_name} on {cluster} (was {status})")
        return True

    async def _remediate_mcp_tool_broken(finding: dict) -> bool:
        """Restart the MCP deployment whose tool is timing out."""
        resource = finding.get("resource", "")
        # resource format: "mcp/<domain>/<tool_name>"
        parts = resource.split("/")
        if len(parts) < 2:
            return False
        mcp_domain = parts[1]  # e.g. "observability", "infrastructure"
        deploy_name = f"{mcp_domain}-mcp"
        ctx = CLUSTERS.get("agentic", "agentic")
        result = await call_mcp_tool(INFRASTRUCTURE_MCP_URL, "kubectl_restart_deployment", {
            "deployment_name": deploy_name, "namespace": "ai-platform", "cluster": ctx,
        })
        if result.get("_error"):
            logger.warning(f"MCP restart failed for {deploy_name}: {result.get('message')}")
            return False
        logger.info(f"Auto-remediated: restarted {deploy_name} deployment (tool timeout)")
        return True

    async def _remediate_deployment_unavailable(finding: dict) -> bool:
        """Restart an unavailable deployment."""
        ev = finding.get("evidence", {})
        dep_name = ev.get("deployment", "")
        namespace = ev.get("namespace", "")
        cluster = finding.get("cluster", "")
        ctx = CLUSTERS.get(cluster, cluster)
        if not dep_name or not namespace:
            return False
        result = await call_mcp_tool(INFRASTRUCTURE_MCP_URL, "kubectl_restart_deployment", {
            "deployment_name": dep_name, "namespace": namespace, "cluster": ctx,
        })
        if result.get("_error"):
            logger.warning(f"Deployment restart failed for {namespace}/{dep_name}: {result.get('message')}")
            return False
        logger.info(f"Auto-remediated: restarted deployment {namespace}/{dep_name} on {cluster}")
        return True

    async def _remediate_pbs_backup_stale(finding: dict) -> bool:
        """Trigger a PBS backup by running vzdump on the Proxmox host."""
        resource = finding.get("resource", "")
        # resource format: "pbs/<datastore>/<type>/<vmid>"
        parts = resource.split("/")
        if len(parts) < 4:
            return False
        datastore = parts[1]
        vm_type = parts[2]  # "vm" or "ct"
        vmid = parts[3]
        if not vmid.isdigit():
            return False
        # Determine which Proxmox host owns this VM
        # VMs 400-499 are on Ruapehu, 200 on Pihanga
        # IMPORTANT: Node names are case-sensitive in the Proxmox API
        vmid_int = int(vmid)
        if vmid_int >= 400:
            pve_host = "10.10.0.10"
            pve_node = "Ruapehu"
        else:
            pve_host = "10.10.0.20"
            pve_node = "Pihanga"
        pve_token_id = os.environ.get(f"PVE_{pve_node.upper()}_TOKEN_ID", "")
        pve_token_secret = os.environ.get(f"PVE_{pve_node.upper()}_TOKEN_SECRET", "")
        if not pve_token_id or not pve_token_secret:
            logger.warning(f"No PVE_{pve_node.upper()}_TOKEN_ID/SECRET — cannot trigger backup")
            return False
        # PBS storage in Proxmox is "pbs-pihanga" (PBS runs on Pihanga host)
        pbs_storage = "pbs-pihanga"
        try:
            async with httpx.AsyncClient(timeout=120.0, verify=False) as client:
                resp = await client.post(
                    f"https://{pve_host}:8006/api2/json/nodes/{pve_node}/vzdump",
                    headers={"Authorization": f"PVEAPIToken={pve_token_id}={pve_token_secret}"},
                    data={
                        "vmid": vmid,
                        "storage": pbs_storage,
                        "mode": "snapshot",
                        "compress": "zstd",
                        "notes-template": "auto-backup triggered by error-hunter",
                    },
                )
                if resp.status_code == 200:
                    logger.info(f"Auto-remediated: triggered PBS backup for VM {vmid} on {pve_node}")
                    return True
                else:
                    logger.warning(f"PBS backup trigger failed for VM {vmid}: {resp.status_code} {resp.text[:200]}")
        except Exception as e:
            logger.warning(f"PBS backup trigger error for VM {vmid}: {e}")
        return False

    async def _remediate_backrest_unreachable(finding: dict) -> bool:
        """Restart the Backrest pod if it's unreachable."""
        result = await call_mcp_tool(INFRASTRUCTURE_MCP_URL, "kubectl_restart_deployment", {
            "deployment_name": "backrest", "namespace": "backrest", "cluster": "monit",
        })
        if result.get("_error"):
            # Try statefulset
            result = await call_mcp_tool(INFRASTRUCTURE_MCP_URL, "kubectl_restart_statefulset", {
                "statefulset_name": "backrest", "namespace": "backrest", "cluster": "monit",
            })
            if result.get("_error"):
                logger.warning(f"Backrest restart failed: {result.get('message')}")
                return False
        logger.info("Auto-remediated: restarted Backrest")
        return True

    async def _remediate_download_cleanup(finding: dict) -> bool:
        """Remove fake/stale completed torrents from Transmission, then trigger Cleanuparr."""
        media = MEDIA_MCP_URL
        evidence = finding.get("evidence", {})
        torrent_names = evidence.get("torrents", [])
        removed_count = 0

        try:
            # Get full torrent list to match names to IDs
            result = await call_mcp_tool(media, "transmission_list_torrents", {})
            torrents = result.get("items", result.get("torrents", result.get("result", [])))
            if not isinstance(torrents, list):
                logger.warning("Could not list torrents for cleanup")
                return False

            for torrent in torrents:
                if not isinstance(torrent, dict):
                    continue
                name = torrent.get("name", "")
                tid = torrent.get("id", torrent.get("hashString", ""))
                progress = torrent.get("progress", torrent.get("percentDone", 0))
                if isinstance(progress, (int, float)) and progress <= 1:
                    progress = progress * 100

                # Only remove completed torrents
                if progress < 100:
                    continue

                # Fake downloads: .exe files are never legitimate media
                is_fake = name.lower().endswith(".exe") or name.lower().endswith(".msi")
                # Stale: completed and stopped (status 0) for too long
                is_stale = torrent.get("status", -1) == 0

                if is_fake or is_stale:
                    delete_data = is_fake  # Delete data for fakes, keep for stale
                    rm_result = await call_mcp_tool(media, "transmission_remove_torrent", {
                        "torrent_id": tid, "delete_data": delete_data,
                    })
                    if not rm_result.get("_error"):
                        logger.info(f"Removed {'fake' if is_fake else 'stale'} torrent: {name[:60]}")
                        removed_count += 1
                    else:
                        logger.warning(f"Failed to remove torrent {name[:40]}: {rm_result.get('message', '')}")

            # Trigger Cleanuparr QueueCleaner to pick up anything we missed
            try:
                await call_mcp_tool(media, "cleanuparr_trigger_job", {"job_type": "QueueCleaner"})
                logger.info("Triggered Cleanuparr QueueCleaner after download cleanup")
            except Exception as e:
                logger.warning(f"Cleanuparr trigger failed (non-fatal): {e}")

        except Exception as e:
            logger.error(f"Download cleanup remediation error: {e}")
            return False

        if removed_count > 0:
            logger.info(f"Auto-remediated: removed {removed_count} completed torrents from Transmission")
            return True
        # Even if we removed nothing, trigger was sent — partial success
        return removed_count > 0

    async def _remediate_arr_queue_orphaned(finding: dict) -> bool:
        """Remove orphaned items from Sonarr/Radarr queue (completed but no series/movie match)."""
        media = MEDIA_MCP_URL
        evidence = finding.get("evidence", {})
        app_name = evidence.get("app", "sonarr")
        items = evidence.get("items", [])
        remove_tool = f"{app_name}_remove_queue_item"
        removed_count = 0

        for item in items:
            queue_id = item.get("queue_id")
            title = item.get("title", "unknown")
            if not queue_id:
                continue
            try:
                # Remove from arr queue; also remove from download client since it's orphaned
                result = await call_mcp_tool(media, remove_tool, {
                    "queue_id": queue_id,
                    "remove_from_client": True,
                    "blocklist": True,  # Blocklist to prevent re-download of same bad release
                })
                if not result.get("_error"):
                    logger.info(f"Removed orphaned {app_name} queue item: {title[:50]}")
                    removed_count += 1
                else:
                    logger.warning(f"Failed to remove {app_name} queue item {queue_id}: {result.get('message', '')}")
            except Exception as e:
                logger.warning(f"Error removing {app_name} queue item {queue_id}: {e}")

        if removed_count > 0:
            logger.info(f"Auto-remediated: removed {removed_count} orphaned items from {app_name} queue")
            return True
        return False

    # Mapping of check_name patterns to verification functions
    CHECK_VERIFIERS = {
        "pod_crash_loop": lambda f: check_pod_health_single(f),
        "deployment_unavailable": lambda f: check_deployment_single(f),
        "argocd_drift": lambda f: check_argocd_single(f),
        "nfs_mount_stale": lambda f: check_nfs_single(f),
    }

    async def check_pod_health_single(finding):
        """Re-check a specific pod's health."""
        resource = finding.get("resource", "")
        cluster_name = finding.get("cluster", "agentic")
        try:
            result = await call_mcp_tool(INFRASTRUCTURE_MCP_URL, "kubectl_get_pods", {
                "namespace": resource.split("/")[0] if "/" in resource else "default",
                "cluster": cluster_name
            })
            # Check if the specific pod is still in CrashLoopBackOff
            if isinstance(result, str) and finding.get("resource", "") in result:
                if "CrashLoopBackOff" in result or "Error" in result:
                    return [finding]  # Still broken
            return []  # Fixed
        except Exception:
            return []  # Assume fixed if we can't verify

    async def check_deployment_single(finding):
        """Re-check a specific deployment's availability."""
        resource = finding.get("resource", "")
        cluster_name = finding.get("cluster", "agentic")
        try:
            result = await call_mcp_tool(INFRASTRUCTURE_MCP_URL, "kubectl_get_deployments", {
                "namespace": resource.split("/")[0] if "/" in resource else "default",
                "cluster": cluster_name
            })
            if isinstance(result, str) and "0/" in result:
                return [finding]  # Still unavailable
            return []
        except Exception:
            return []

    async def check_argocd_single(finding):
        """Re-check ArgoCD sync status for a specific app."""
        try:
            result = await call_mcp_tool(INFRASTRUCTURE_MCP_URL, "argocd_get_applications", {})
            if isinstance(result, str):
                app_name = finding.get("resource", "").split("/")[-1]
                if app_name in result and ("OutOfSync" in result or "Degraded" in result):
                    return [finding]
            return []
        except Exception:
            return []

    async def check_nfs_single(finding):
        """Re-check NFS mount status."""
        # NFS checks are cluster-wide, just re-run a targeted check
        return []  # Assume fixed after remediation

    async def verify_remediation(finding: dict) -> bool:
        """Re-run the check that produced this finding to verify the fix worked."""
        await asyncio.sleep(30)  # Allow time for k8s reconciliation
        check_name = finding.get("check_name", "")
        for pattern, checker in CHECK_VERIFIERS.items():
            if pattern in check_name:
                try:
                    new_findings = await checker(finding)
                    return len(new_findings) == 0
                except Exception as e:
                    logger.warning(f"Verification failed for {check_name}: {e}")
                    return True  # Assume success if verification itself fails
        return True  # No verifier = assume success

    # Map finding check_names to their auto-fix functions
    REMEDIATORS = {
        "argocd_out_of_sync": _remediate_argocd_out_of_sync,
        "argocd_unhealthy": _remediate_argocd_unhealthy,
        "adguard_filters_stale": _remediate_adguard_filters_stale,
        "pod_not_running": _remediate_pod_not_running,
        "mcp_tool_broken": _remediate_mcp_tool_broken,
        "deployment_unavailable": _remediate_deployment_unavailable,
        "pbs_backup_stale": _remediate_pbs_backup_stale,
        "backrest_unreachable": _remediate_backrest_unreachable,
        "download_cleanup_needed": _remediate_download_cleanup,
        "arr_queue_orphaned": _remediate_arr_queue_orphaned,
    }

    # Checks that benefit from re-scan after remediation (transient items, e.g. downloads completing)
    RESCANNABLE_CHECKS = {
        "arr_queue_orphaned",   # New downloads finish and become orphaned after first pass
        "download_cleanup_needed",  # New torrents may appear
    }
    MAX_REMEDIATION_PASSES = 3
    REMEDIATION_RESCAN_DELAY = 60  # seconds between passes

    async def _run_remediation_pass(findings: list, remediators_map: dict,
                                     log_only: list, sweep_id: str,
                                     already_refreshed_adguard: bool,
                                     already_restarted_mcps: set) -> tuple:
        """Run one pass of remediation on a list of findings.
        Returns (remaining, remediated_count, already_refreshed_adguard)."""
        remaining = []
        remediated_count = 0

        for finding in findings:
            check_name = finding.get("check_name", "")
            remediator = remediators_map.get(check_name)
            fixed = False
            if remediator:
                if check_name == "adguard_filters_stale":
                    if already_refreshed_adguard:
                        fixed = True
                    else:
                        fixed = await remediator(finding)
                        if fixed:
                            already_refreshed_adguard = True
                elif check_name == "mcp_tool_broken":
                    mcp_domain = finding.get("resource", "").split("/")[1] if "/" in finding.get("resource", "") else ""
                    if mcp_domain in already_restarted_mcps:
                        fixed = True
                    else:
                        fixed = await remediator(finding)
                        if fixed:
                            already_restarted_mcps.add(mcp_domain)
                else:
                    fixed = await remediator(finding)

            if fixed:
                verified = await verify_remediation(finding)
                if verified:
                    logger.info(f"Verified remediation for {finding['check_name']}")
                else:
                    logger.warning(f"Remediation not verified for {finding['check_name']}, will retry")
                finding["remediated"] = True
                finding["description"] = f"[AUTO-FIXED] {finding.get('description', '')}"
                log_only.append(finding)
                remediated_count += 1
            else:
                remaining.append(finding)

        return remaining, remediated_count, already_refreshed_adguard

    async def remediate(state: SweepState) -> dict:
        """Node 3.5: Attempt auto-fixes for known finding types.

        Successfully remediated findings move from fix_now/create_rule to log_only.
        Failed remediations stay in their original bucket for dispatch to Claude.

        For checks in RESCANNABLE_CHECKS, re-runs the relevant checks after a delay
        to catch transient items (e.g. downloads that complete between passes).
        Runs up to MAX_REMEDIATION_PASSES until no new findings appear.
        """
        sweep_id = state["sweep_id"]
        await report_progress(sweep_id, "Remediating", "Auto-fixing known issues", 60)

        fix_now = list(state.get("fix_now", []))
        create_rule = list(state.get("create_rule", []))
        log_only = list(state.get("log_only", []))

        total_remediated = 0
        already_refreshed_adguard = False
        already_restarted_mcps = set()

        # ── Pass 1: remediate all fix_now and create_rule items ──
        remaining_fix_now, count, already_refreshed_adguard = await _run_remediation_pass(
            fix_now, REMEDIATORS, log_only, sweep_id,
            already_refreshed_adguard, already_restarted_mcps)
        total_remediated += count

        remaining_create_rule, count, already_refreshed_adguard = await _run_remediation_pass(
            create_rule, REMEDIATORS, log_only, sweep_id,
            already_refreshed_adguard, already_restarted_mcps)
        total_remediated += count

        logger.info(f"[{sweep_id}] Pass 1: {total_remediated} auto-fixed, "
                    f"{len(remaining_fix_now)} fix_now remaining, {len(remaining_create_rule)} create_rule remaining")

        # ── Passes 2+: re-scan checks that may have new transient items ──
        # Only re-scan if we successfully remediated something with a rescannable check
        remediated_checks = set(f.get("check_name") for f in log_only if f.get("remediated"))
        rescannable = remediated_checks & RESCANNABLE_CHECKS

        for pass_num in range(2, MAX_REMEDIATION_PASSES + 1):
            if not rescannable:
                break

            logger.info(f"[{sweep_id}] Pass {pass_num}: waiting {REMEDIATION_RESCAN_DELAY}s for transient items to settle")
            await report_progress(sweep_id, f"Rescan pass {pass_num}",
                                  f"Waiting {REMEDIATION_RESCAN_DELAY}s for downloads to complete", 62)
            await asyncio.sleep(REMEDIATION_RESCAN_DELAY)

            # Re-run only the affected checks
            new_findings = []
            try:
                if rescannable & {"arr_queue_orphaned", "arr_queue_stuck"}:
                    media_findings = await check_media_pipeline()
                    new_findings.extend([f for f in media_findings
                                        if f.get("check_name") in RESCANNABLE_CHECKS])
                if "download_cleanup_needed" in rescannable:
                    media_findings = await check_media_pipeline()
                    new_findings.extend([f for f in media_findings
                                        if f.get("check_name") == "download_cleanup_needed"])
            except Exception as e:
                logger.warning(f"[{sweep_id}] Rescan error: {e}")
                break

            # Deduplicate: only keep findings we haven't already fixed
            already_fixed_fps = set(f.get("fingerprint") for f in log_only if f.get("remediated"))
            new_findings = [f for f in new_findings if f.get("fingerprint") not in already_fixed_fps]

            if not new_findings:
                logger.info(f"[{sweep_id}] Pass {pass_num}: no new findings — all clear")
                break

            # Classify new findings as fix_now (they have known remediators)
            new_fix = [f for f in new_findings if f.get("check_name") in REMEDIATORS]
            if not new_fix:
                logger.info(f"[{sweep_id}] Pass {pass_num}: {len(new_findings)} found but none remediable")
                remaining_fix_now.extend(new_findings)
                break

            logger.info(f"[{sweep_id}] Pass {pass_num}: found {len(new_fix)} new remediable items")
            await report_progress(sweep_id, f"Remediating pass {pass_num}",
                                  f"{len(new_fix)} new items found", 63)

            new_remaining, count, already_refreshed_adguard = await _run_remediation_pass(
                new_fix, REMEDIATORS, log_only, sweep_id,
                already_refreshed_adguard, already_restarted_mcps)
            total_remediated += count
            remaining_fix_now.extend(new_remaining)

            # Check if we fixed anything new — if not, stop retrying
            if count == 0:
                logger.info(f"[{sweep_id}] Pass {pass_num}: nothing new fixed, stopping retry loop")
                break

            # Update rescannable for next pass
            remediated_checks = set(f.get("check_name") for f in log_only if f.get("remediated"))
            rescannable = remediated_checks & RESCANNABLE_CHECKS

        logger.info(f"[{sweep_id}] Remediation complete: {total_remediated} auto-fixed across all passes, "
                    f"{len(remaining_fix_now)} fix_now remaining, {len(remaining_create_rule)} create_rule remaining")
        await report_progress(sweep_id, "Remediated",
                              f"{total_remediated} auto-fixed, {len(remaining_fix_now)+len(remaining_create_rule)} remaining", 65)

        return {"fix_now": remaining_fix_now, "create_rule": remaining_create_rule, "log_only": log_only}

    def _build_sweep_prompt(sweep_id: str, fix_now: list, create_rule: list, log_only: list) -> str:
        """Build a single prompt for the entire sweep — all findings as a batch queue."""
        total = len(fix_now) + len(create_rule) + len(log_only)
        lines = [
            f"# Error Hunter Sweep: {sweep_id}",
            "",
            f"This sweep found **{total} findings** across the Kernow homelab estate.",
            f"- **{len(fix_now)} fix_now** — investigate, fix, and write a runbook",
            f"- **{len(create_rule)} create_rule** — create a monitoring alert and write a runbook",
            f"- **{len(log_only)} log_only** — informational context only",
            "",
            "## Your Workflow",
            "",
            "Work through every fix_now and create_rule item. For EACH item, do all three:",
            "",
            "### 1. Fix (fix_now items)",
            "- Use MCP tools to investigate the root cause",
            "- Fix the issue following GitOps (edit manifests in the repo, commit, push — NEVER kubectl apply)",
            "- If unfixable, document what you found and why",
            "",
            "### 2. Write Alert Rule (create_rule items, and any fix_now that should have an alert)",
            "- Add a PrometheusRule to `/home/monit_homelab/kubernetes/platform/prometheus-rules/homelab-rules.yaml`",
            "- OR add a Gatus endpoint to the relevant Gatus config",
            "- Alert should catch this issue automatically next time",
            "- Commit and push to the monit_homelab repo",
            "",
            "### 3. Write Runbook (ALL actionable items)",
            "- Create a runbook in `/home/agentic_lab/runbooks/` under the right subdirectory:",
            "  - `alerts/` — for alert-triggered issues (use alert name as filename)",
            "  - `troubleshooting/` — for general troubleshooting",
            "  - `infrastructure/` — for infra patterns",
            "- Follow the existing format (see other .md files in those dirs for examples):",
            "  - Alert/Issue Details table (name, severity, source, clusters)",
            "  - Description",
            "  - Quick Diagnosis steps (with MCP tool examples, not raw kubectl)",
            "  - Common Causes with symptoms/verification/resolution",
            "  - Resolution Steps",
            "  - Prevention",
            "- Commit and push to the agentic_lab repo",
            "",
            "## Repository Paths",
            "",
            "- **Prod cluster manifests**: `/home/prod_homelab/`",
            "- **Agentic cluster manifests**: `/home/agentic_lab/`",
            "- **Monit cluster manifests**: `/home/monit_homelab/`",
            "- **Alert rules**: `/home/monit_homelab/kubernetes/platform/prometheus-rules/homelab-rules.yaml`",
            "- **Runbooks**: `/home/agentic_lab/runbooks/`",
            "- **MCP server source**: `/home/mcp-servers/`",
            "",
            "## Git Workflow",
            "",
            "Always use `git -C <repo-path>` — never cd. Commit to each submodule separately:",
            "```",
            "git -C /home/<submodule> add <files>",
            "git -C /home/<submodule> commit -m 'message'",
            "git -C /home/<submodule> push origin main",
            "```",
            "",
            "## When Done — Report Back",
            "",
            "After you finish ALL items, you MUST resolve each finding in the dashboard.",
            "For each fix_now and create_rule finding, run this curl (replace values):",
            "```bash",
            f'curl -s -X POST {AFFERENT_URL}/webhook/screen-resolve \\',
            '  -H "Content-Type: application/json" \\',
            '  -d \'{"type":"finding","id":FINDING_ID,"summary":"<what you did>","lessons":"<pattern or insight for next time>","runbook_path":"<path if you wrote one>"}\'',
            "```",
            "Fields: **summary** (required), **lessons** (optional — key insight worth remembering),",
            "**runbook_path** (optional — file path if you created/updated a runbook).",
            "These feed the knowledge base so future incidents can find your work.",
            "",
            "The FINDING_ID for each item is listed next to its heading below.",
            "This marks them resolved in the dashboard. Do NOT skip this step.",
            "",
            "When all curl calls are done, give a brief summary of what was fixed, what alerts were created, and what runbooks were written.",
            "",
        ]

        def _format_finding(i: int, f: dict) -> list:
            """Format a single finding as a prompt section."""
            fid = f.get("id", "?")
            out = [
                f"### {i}. [{f.get('severity', 'warning').upper()}] {f.get('check_name', 'unknown')} (FINDING_ID={fid})",
                f"- **Cluster**: {f.get('cluster', 'unknown')}",
                f"- **Resource**: {f.get('resource', 'unknown')}",
                f"- **Description**: {f.get('description', '')}",
            ]
            evidence = f.get("evidence", {})
            if evidence:
                out.append("- **Evidence**:")
                for k, v in evidence.items():
                    out.append(f"  - {k}: {v}")
            suggested = f.get("suggested_fix", "")
            if suggested:
                out.append(f"- **Suggested**: {suggested}")
            out.append("")
            return out

        if fix_now:
            lines.append("---")
            lines.append(f"## Fix Now ({len(fix_now)} items)")
            lines.append("For each: investigate → fix → write alert if missing → write runbook")
            lines.append("")
            for i, f in enumerate(fix_now, 1):
                lines.extend(_format_finding(i, f))

        if create_rule:
            lines.append("---")
            lines.append(f"## Create Rules ({len(create_rule)} items)")
            lines.append("For each: create monitoring alert → write runbook")
            lines.append("")
            for i, f in enumerate(create_rule, 1):
                target = f.get("alert_target", "")
                entry = _format_finding(i, f)
                if target:
                    entry.insert(-1, f"- **Alert target**: {target}")
                lines.extend(entry)

        if log_only:
            lines.append("---")
            lines.append(f"## Log Only ({len(log_only)} items) — context only, no action needed")
            lines.append("")
            for i, f in enumerate(log_only, 1):
                lines.append(f"{i}. **{f.get('check_name', '?')}** on {f.get('cluster', '?')}/{f.get('resource', '?')} — {f.get('description', '')}")
            lines.append("")

        return "\n".join(lines)

    async def dispatch_remediation(state: SweepState) -> dict:
        """Node 4: Remediate remaining findings via Tier 2 LLM-guided ReAct loop.

        After Tier 1 (deterministic REMEDIATORS) ran in the remediate() node,
        this node picks up any remaining fix_now items and runs them through
        the LLM ReAct engine. Results are logged to PostgreSQL.
        """
        sweep_id = state["sweep_id"]
        await report_progress(sweep_id, "LLM Remediation", "Running ReAct loop on remaining issues", 70)

        fix_now = state.get("fix_now", [])
        create_rule = state.get("create_rule", [])
        log_only = state.get("log_only", [])

        remediation_results = []
        llm_fixed_count = 0

        # Run LLM remediation on remaining fix_now items (Tier 1 already tried in remediate())
        for finding in fix_now:
            check_name = finding.get("check_name", "")
            logger.info(f"[{sweep_id}] Tier 2 LLM remediation for: {check_name} on {finding.get('resource', '?')}")

            try:
                result = await llm_remediate(finding)

                if result["success"]:
                    # Verify the fix
                    verified = await verify_remediation(finding)
                    remediation_results.append({
                        "finding": finding.get("fingerprint", ""),
                        "check_name": check_name,
                        "tier": 2,
                        "success": verified,
                        "steps": len(result.get("steps", [])),
                        "summary": result.get("summary", "")[:300],
                    })
                    if verified:
                        llm_fixed_count += 1
                        finding["remediated"] = True
                        finding["description"] = f"[LLM-FIXED] {finding.get('description', '')}"
                        log_only.append(finding)
                    else:
                        logger.warning(f"[{sweep_id}] LLM fix not verified for {check_name}")
                        remediation_results[-1]["verified"] = False
                else:
                    remediation_results.append({
                        "finding": finding.get("fingerprint", ""),
                        "check_name": check_name,
                        "tier": 2,
                        "success": False,
                        "escalate": True,
                        "steps": len(result.get("steps", [])),
                        "summary": result.get("summary", "")[:300],
                    })
                    logger.warning(f"[{sweep_id}] LLM remediation failed for {check_name}: {result.get('summary', '')[:100]}")

            except Exception as e:
                logger.error(f"[{sweep_id}] LLM remediation error for {check_name}: {e}")
                remediation_results.append({
                    "finding": finding.get("fingerprint", ""),
                    "check_name": check_name,
                    "tier": 2,
                    "success": False,
                    "error": str(e)[:200],
                })

        # Remove LLM-fixed findings from fix_now
        remaining_fix_now = [f for f in fix_now if not f.get("remediated")]

        # Mark findings in DB based on results
        for finding in fix_now + create_rule:
            try:
                new_status = "resolved" if finding.get("remediated") else "dispatched"
                await pg_execute(
                    "UPDATE error_hunter_findings SET status = $2 WHERE fingerprint = $1 AND status = 'new'",
                    finding.get("fingerprint", ""), new_status,
                )
            except Exception as e:
                logger.warning(f"Failed to update finding status: {e}")

        # Persist remediation results
        for rr in remediation_results:
            try:
                await pg_execute(
                    """INSERT INTO sweep_remediation_log
                       (sweep_id, fingerprint, check_name, tier, success, steps_count, summary)
                       VALUES ($1, $2, $3, $4, $5, $6, $7)""",
                    sweep_id, rr.get("finding", ""), rr.get("check_name", ""),
                    rr.get("tier", 2), rr.get("success", False),
                    rr.get("steps", 0), rr.get("summary", ""),
                )
            except Exception:
                pass  # Table may not exist yet — non-critical

        logger.info(f"[{sweep_id}] Tier 2 remediation: {llm_fixed_count} LLM-fixed, "
                    f"{len(remaining_fix_now)} still unresolved, {len(create_rule)} create_rule pending")
        await report_progress(sweep_id, "LLM Remediated",
                              f"{llm_fixed_count} LLM-fixed, {len(remaining_fix_now)} unresolved", 80)

        # ── Dispatch remaining items to Claude patrol screen ──
        # If there are still unfixed fix_now or create_rule items, build a prompt
        # and send to tamar's patrol screen for full Claude investigation
        screens_created = []
        actionable_fix = [f for f in remaining_fix_now if not f.get("remediated")]
        actionable_rule = [f for f in create_rule if not f.get("remediated")]

        if actionable_fix or actionable_rule:
            await report_progress(sweep_id, "Dispatching to Claude",
                                  f"{len(actionable_fix)} fix + {len(actionable_rule)} rules", 85)
            try:
                prompt = _build_sweep_prompt(sweep_id, actionable_fix, actionable_rule, log_only)
                # Choose model based on severity — critical gets opus, else sonnet
                has_critical = any(f.get("severity") == "critical" for f in actionable_fix)
                model = "opus" if has_critical else "sonnet"

                async with httpx.AsyncClient(timeout=15.0) as client:
                    resp = await client.post(
                        f"{AFFERENT_URL}/api/screens",
                        json={"prompt": prompt, "model": model},
                        headers={"Content-Type": "application/json"},
                    )
                    resp_data = resp.json()
                    screen_name = resp_data.get("session", "patrol")
                    screens_created.append(screen_name)
                    logger.info(f"[{sweep_id}] Dispatched {len(actionable_fix)+len(actionable_rule)} items "
                                f"to Claude patrol screen ({model})")

                # Mark dispatched findings in DB
                for finding in actionable_fix + actionable_rule:
                    try:
                        await pg_execute(
                            "UPDATE error_hunter_findings SET status = 'dispatched', "
                            "screen_session = $2 WHERE fingerprint = $1 AND status = 'new'",
                            finding.get("fingerprint", ""), screen_name,
                        )
                    except Exception as e:
                        logger.warning(f"Failed to mark dispatched: {e}")

            except Exception as e:
                logger.error(f"[{sweep_id}] Failed to dispatch to Claude screen: {e}")
        else:
            logger.info(f"[{sweep_id}] No actionable items remaining — skipping screen dispatch")

        return {
            "fix_now": remaining_fix_now,
            "create_rule": create_rule,
            "log_only": log_only,
            "screens_created": screens_created,
            "rules_written": [],
            "runbooks_drafted": [],
            "remediation_results": remediation_results,
        }

    async def record_sweep(state: SweepState) -> dict:
        """Node 7: Persist all findings and update sweep run record."""
        sweep_id = state["sweep_id"]
        await report_progress(sweep_id, "Recording results", "Persisting to DB", 90)
        fix_now = state.get("fix_now", [])
        create_rule = state.get("create_rule", [])
        log_only = state.get("log_only", [])

        all_classified = (
            [(f, "fix_now") for f in fix_now]
            + [(f, "create_rule") for f in create_rule]
            + [(f, "log_only") for f in log_only]
        )

        # Persist each finding (upsert: update last_seen + sweep_id if already active)
        # Auto-fixed findings (remediated=True) go straight to 'resolved'
        persisted = 0
        for finding, classification in all_classified:
            initial_status = "resolved" if finding.get("remediated") else "new"
            try:
                await pg_execute("""
                    INSERT INTO error_hunter_findings
                        (sweep_id, fingerprint, check_category, check_name, cluster, resource,
                         severity, classification, description, evidence, status, resolved_at)
                    VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11,
                            CASE WHEN $11 = 'resolved' THEN NOW() ELSE NULL END)
                    ON CONFLICT (fingerprint) WHERE status NOT IN ('resolved', 'ignored', 'superseded')
                    DO UPDATE SET last_seen = NOW(), sweep_id = $1, severity = $7, description = $9,
                                  status = CASE WHEN $11 = 'resolved' THEN 'resolved' ELSE error_hunter_findings.status END,
                                  resolved_at = CASE WHEN $11 = 'resolved' THEN NOW() ELSE error_hunter_findings.resolved_at END
                """,
                    sweep_id,
                    finding["fingerprint"],
                    finding["check_category"],
                    finding["check_name"],
                    finding.get("cluster"),
                    finding.get("resource"),
                    finding.get("severity", "warning"),
                    classification,
                    finding.get("description", ""),
                    json.dumps(finding.get("evidence", {})),
                    initial_status,
                )
                persisted += 1
            except Exception as e:
                logger.warning(f"Failed to persist finding {finding.get('check_name')}: {e}")

        # Update sweep run
        summary = {
            "total": len(all_classified),
            "fix_now": len(fix_now),
            "create_rule": len(create_rule),
            "log_only": len(log_only),
            "screens_created": len(state.get("screens_created", [])),
            "rules_written": len(state.get("rules_written", [])),
            "runbooks_drafted": len(state.get("runbooks_drafted", [])),
            "persisted": persisted,
        }

        try:
            await pg_execute("""
                UPDATE sweep_runs
                SET status = 'completed', completed_at = NOW(),
                    total_findings = $2, fix_now_count = $3, create_rule_count = $4,
                    log_only_count = $5, screens_created = $6, rules_written = $7,
                    summary = $8
                WHERE sweep_id = $1
            """,
                sweep_id,
                len(all_classified),
                len(fix_now),
                len(create_rule),
                len(log_only),
                len(state.get("screens_created", [])),
                len(state.get("rules_written", [])),
                json.dumps(summary),
            )
        except Exception as e:
            logger.error(f"Failed to update sweep run: {e}")

        # Auto-resolve findings no longer detected.
        # If a finding was active (new/dispatched) but its fingerprint was NOT
        # found in this sweep's raw findings, the issue has cleared — resolve it.
        current_fingerprints = set(f["fingerprint"] for f in state.get("raw_findings", []))
        auto_resolved = 0
        try:
            stale = await pg_fetch(
                "SELECT id, fingerprint, check_name, resource FROM error_hunter_findings "
                "WHERE status IN ('new', 'dispatched')"
            )
            for row in stale:
                if row["fingerprint"] not in current_fingerprints:
                    await pg_execute(
                        "UPDATE error_hunter_findings SET status = 'resolved', "
                        "resolved_at = NOW() WHERE id = $1",
                        row["id"],
                    )
                    auto_resolved += 1
            if auto_resolved > 0:
                logger.info(f"[{sweep_id}] Auto-resolved {auto_resolved} findings no longer detected")
        except Exception as e:
            logger.warning(f"Auto-resolve error: {e}")

        summary["auto_resolved"] = auto_resolved

        # Log to knowledge-mcp
        try:
            await call_mcp_tool(KNOWLEDGE_MCP_URL, "log_event", {
                "event_type": "error_hunter.sweep_complete",
                "description": f"Error Hunter sweep {sweep_id}: {len(all_classified)} findings ({len(fix_now)} fix_now, {len(create_rule)} create_rule, {len(log_only)} log_only)",
                "source_agent": "error-hunter",
                "metadata": summary,
            })
        except Exception as e:
            logger.warning(f"Failed to log sweep event: {e}")

        # Notify Kernow Hub (Patrol tab) about sweep completion
        try:
            async with httpx.AsyncClient(timeout=10.0) as client:
                await client.post(
                    f"{AFFERENT_URL}/webhook/patrol-sweep",
                    json={"sweep_id": sweep_id, "summary": summary},
                    headers={"Content-Type": "application/json"},
                )
            logger.info(f"[{sweep_id}] Notified Kernow Hub patrol webhook")
        except Exception as e:
            logger.warning(f"Failed to notify Kernow Hub: {e}")

        logger.info(f"[{sweep_id}] Sweep recorded: {json.dumps(summary)}")
        return {"summary": summary, "status": "completed"}

    # ========================================================================
    # GRAPH DEFINITION
    # ========================================================================

    def create_error_hunter_workflow():
        workflow = StateGraph(SweepState)
        workflow.add_node("sweep_estate", sweep_estate)
        workflow.add_node("deduplicate", deduplicate)
        workflow.add_node("classify", classify)
        workflow.add_node("remediate", remediate)
        workflow.add_node("dispatch_remediation", dispatch_remediation)
        workflow.add_node("record_sweep", record_sweep)

        workflow.set_entry_point("sweep_estate")
        workflow.add_edge("sweep_estate", "deduplicate")
        workflow.add_edge("deduplicate", "classify")
        workflow.add_edge("classify", "remediate")
        workflow.add_edge("remediate", "dispatch_remediation")
        workflow.add_edge("dispatch_remediation", "record_sweep")
        workflow.add_edge("record_sweep", END)

        return workflow.compile()

    error_hunter_graph = create_error_hunter_workflow()

    # ========================================================================
    # SWEEP EXECUTION
    # ========================================================================

    # Track active sweep to prevent concurrent runs
    _active_sweep: Optional[str] = None

    async def run_sweep(sweep_id: str):
        global _active_sweep

        if _active_sweep:
            logger.warning(f"Sweep {sweep_id} rejected: sweep {_active_sweep} already running")
            return

        _active_sweep = sweep_id
        try:
            initial_state: SweepState = {
                "sweep_id": sweep_id,
                "sweep_time": datetime.utcnow().isoformat(),
                "raw_findings": [],
                "deduplicated_findings": [],
                "fix_now": [],
                "create_rule": [],
                "log_only": [],
                "screens_created": [],
                "rules_written": [],
                "runbooks_drafted": [],
                "remediation_results": [],
                "summary": {},
                "status": "running",
            }

            result = await error_hunter_graph.ainvoke(initial_state)
            logger.info(f"Sweep {sweep_id} completed: {result.get('summary', {})}")

        except Exception as e:
            logger.error(f"Sweep {sweep_id} failed: {e}")
            try:
                await pg_execute(
                    "UPDATE sweep_runs SET status = 'failed', completed_at = NOW(), summary = $2 WHERE sweep_id = $1",
                    sweep_id, json.dumps({"error": str(e)}),
                )
            except Exception:
                pass
        finally:
            _active_sweep = None

    # ========================================================================
    # FASTAPI APPLICATION
    # ========================================================================

    app = FastAPI(title="Error Hunter", description="Proactive estate patrol for Kernow homelab")

    @app.on_event("startup")
    async def on_startup():
        try:
            await init_db()
        except Exception as e:
            logger.warning(f"init_db failed (non-fatal): {e}")
        # Clean up sweeps stuck in 'running' from previous pod lifecycle
        try:
            await pg_execute(
                "UPDATE sweep_runs SET status = 'completed', completed_at = NOW(), "
                "summary = '{\"note\": \"terminated by pod restart\"}' "
                "WHERE status = 'running'"
            )
            logger.info("Cleaned up stale running sweeps from previous pod")
        except Exception as e:
            logger.warning(f"Stale sweep cleanup failed (non-fatal): {e}")

    @app.post("/sweep")
    async def trigger_sweep():
        """Trigger a proactive estate sweep."""
        if _active_sweep:
            raise HTTPException(status_code=409, detail=f"Sweep {_active_sweep} already in progress")

        sweep_id = f"eh-{uuid_mod.uuid4().hex[:8]}"
        asyncio.create_task(run_sweep(sweep_id))
        return {"status": "sweep_started", "sweep_id": sweep_id}

    @app.get("/sweep/{sweep_id}")
    async def get_sweep_status(sweep_id: str):
        """Get status and results of a specific sweep."""
        row = await pg_fetchrow("SELECT * FROM sweep_runs WHERE sweep_id = $1", sweep_id)
        if not row:
            raise HTTPException(status_code=404, detail="Sweep not found")
        return dict(row)

    @app.get("/findings")
    async def list_findings(
        status: str = None,
        severity: str = None,
        category: str = None,
        limit: int = 50,
    ):
        """List findings with optional filters."""
        conditions = []
        params = []
        idx = 1

        if status:
            conditions.append(f"status = ${idx}")
            params.append(status)
            idx += 1
        if severity:
            conditions.append(f"severity = ${idx}")
            params.append(severity)
            idx += 1
        if category:
            conditions.append(f"check_category = ${idx}")
            params.append(category)
            idx += 1

        where = f"WHERE {' AND '.join(conditions)}" if conditions else ""
        params.append(limit)

        rows = await pg_fetch(
            f"SELECT * FROM error_hunter_findings {where} ORDER BY last_seen DESC LIMIT ${idx}",
            *params
        )
        return {"findings": [dict(r) for r in rows], "count": len(rows)}

    @app.get("/sweeps")
    async def list_sweeps(limit: int = 10):
        """List recent sweep runs."""
        rows = await pg_fetch(
            "SELECT * FROM sweep_runs ORDER BY started_at DESC LIMIT $1", limit
        )
        return {"sweeps": [dict(r) for r in rows]}

    @app.get("/health")
    async def health():
        return {"status": "healthy", "active_sweep": _active_sweep}

    @app.get("/status")
    async def status():
        """Overall Error Hunter status."""
        last_sweep = await pg_fetchrow(
            "SELECT * FROM sweep_runs ORDER BY started_at DESC LIMIT 1"
        )
        total_findings = await pg_fetchrow(
            "SELECT COUNT(*) as total FROM error_hunter_findings WHERE status = 'new'"
        )
        return {
            "active_sweep": _active_sweep,
            "last_sweep": dict(last_sweep) if last_sweep else None,
            "pending_findings": total_findings["total"] if total_findings else 0,
        }

    # ============================================================
    # Estate Operations API
    # ============================================================

    @app.get("/api/estate/health")
    async def estate_health():
        """Current estate health summary."""
        try:
            findings = await pg_fetch("""
                SELECT severity, status, COUNT(*) as count
                FROM error_hunter_findings
                WHERE status NOT IN ('resolved', 'ignored')
                GROUP BY severity, status
            """)
            incidents = await pg_fetch("""
                SELECT severity, status, COUNT(*) as count
                FROM incidents
                WHERE status NOT IN ('resolved', 'false_positive')
                GROUP BY severity, status
            """)
            last_sweep = await pg_fetch("""
                SELECT started_at, completed_at, total_findings as findings_count, status
                FROM sweep_runs
                ORDER BY started_at DESC LIMIT 1
            """)
            return {
                "status": "healthy" if not any(f["severity"] == "critical" for f in findings) else "degraded",
                "findings_summary": [dict(r) for r in findings],
                "incidents_summary": [dict(r) for r in incidents],
                "last_sweep": dict(last_sweep[0]) if last_sweep else None,
                "timestamp": datetime.utcnow().isoformat()
            }
        except Exception as e:
            return {"status": "error", "error": str(e)}

    @app.get("/api/estate/queue")
    async def estate_queue(severity: str = None, status: str = None, limit: int = 50):
        """Unified work queue (findings + incidents)."""
        try:
            query = "SELECT * FROM estate_work_queue WHERE 1=1"
            params = []
            idx = 1
            if severity:
                query += f" AND severity = ${idx}"
                params.append(severity)
                idx += 1
            if status:
                query += f" AND status = ${idx}"
                params.append(status)
                idx += 1
            query += f" LIMIT ${idx}"
            params.append(limit)
            rows = await pg_fetch(query, *params)
            return {"items": [dict(r) for r in rows], "count": len(rows)}
        except Exception as e:
            return {"error": str(e)}

    @app.get("/api/estate/findings")
    async def estate_findings(status: str = None, severity: str = None, limit: int = 50):
        """Filtered findings list."""
        try:
            query = "SELECT * FROM error_hunter_findings WHERE 1=1"
            params = []
            idx = 1
            if status:
                query += f" AND status = ${idx}"
                params.append(status)
                idx += 1
            if severity:
                query += f" AND severity = ${idx}"
                params.append(severity)
                idx += 1
            query += f" ORDER BY first_seen DESC LIMIT ${idx}"
            params.append(limit)
            rows = await pg_fetch(query, *params)
            return {"findings": [dict(r) for r in rows], "count": len(rows)}
        except Exception as e:
            return {"error": str(e)}

    @app.get("/api/estate/incidents")
    async def estate_incidents(status: str = None, limit: int = 50):
        """Open/closed incidents."""
        try:
            query = "SELECT * FROM incidents WHERE 1=1"
            params = []
            idx = 1
            if status:
                query += f" AND status = ${idx}"
                params.append(status)
                idx += 1
            query += f" ORDER BY detected_at DESC LIMIT ${idx}"
            params.append(limit)
            rows = await pg_fetch(query, *params)
            return {"incidents": [dict(r) for r in rows], "count": len(rows)}
        except Exception as e:
            return {"error": str(e)}

    @app.get("/api/estate/sweeps")
    async def estate_sweeps(limit: int = 10):
        """Sweep history."""
        try:
            rows = await pg_fetch("""
                SELECT sweep_id as id, started_at, completed_at, total_findings as findings_count,
                       status, summary
                FROM sweep_runs
                ORDER BY started_at DESC
                LIMIT $1
            """, limit)
            return {"sweeps": [dict(r) for r in rows], "count": len(rows)}
        except Exception as e:
            return {"error": str(e)}

    @app.get("/api/estate/remediations")
    async def estate_remediations(limit: int = 20):
        """Remediation log with steps."""
        try:
            rows = await pg_fetch("""
                SELECT id, finding_id, tier, success, steps, summary,
                       started_at, completed_at, verified
                FROM error_hunter_remediations
                ORDER BY started_at DESC
                LIMIT $1
            """, limit)
            return {"remediations": [dict(r) for r in rows], "count": len(rows)}
        except Exception as e:
            return {"error": str(e)}

    @app.post("/api/estate/sweep")
    async def trigger_estate_sweep():
        """Trigger manual sweep."""
        try:
            sweep_id = f"eh-{uuid_mod.uuid4().hex[:8]}"
            asyncio.create_task(run_sweep(sweep_id))
            return {"status": "started", "message": "Sweep triggered", "sweep_id": sweep_id}
        except Exception as e:
            return {"error": str(e)}

    @app.post("/api/estate/resolve/{item_id}")
    async def resolve_item(item_id: int, request: Request, item_type: str = "finding"):
        """Mark a finding or incident as resolved, then log to knowledge base.

        Accepts optional JSON body:
          summary   — what was done to resolve
          lessons   — lessons learned (optional)
          runbook_path — path to runbook file if one was created (optional)
          domain    — infra|security|obs|dns|network|data (auto-detected if omitted)
        """
        body = {}
        try:
            raw = await request.json()
            if isinstance(raw, dict):
                body = raw
        except Exception:
            pass

        summary = body.get("summary", "")
        lessons = body.get("lessons", "")
        runbook_path = body.get("runbook_path", "")
        domain = body.get("domain", "")

        try:
            # ---- 1. Fetch item context before updating ----
            item_context = {}
            if item_type == "finding":
                rows = await pg_fetch(
                    "SELECT id, check_name, severity, cluster, resource, description, "
                    "classification, evidence::text FROM error_hunter_findings WHERE id = $1",
                    item_id,
                )
                if rows:
                    item_context = dict(rows[0])
            elif item_type == "incident":
                rows = await pg_fetch(
                    "SELECT id, alert_name, severity, source, description, "
                    "fingerprint, labels::text FROM incidents WHERE id = $1",
                    item_id,
                )
                if rows:
                    item_context = dict(rows[0])

            # ---- 2. Update DB ----
            if item_type == "finding":
                await pg_execute(
                    "UPDATE error_hunter_findings SET status = 'resolved' WHERE id = $1",
                    item_id
                )
                # Also resolve the linked incident (if any) so it doesn't re-surface in sweeps
                linked_incident = await pg_fetchrow(
                    "SELECT incident_id FROM error_hunter_findings WHERE id = $1",
                    item_id,
                )
                if linked_incident and linked_incident["incident_id"]:
                    await pg_execute(
                        """UPDATE incidents SET status = 'resolved', resolved_at = NOW(),
                           resolution_summary = $2
                           WHERE id = $1 AND status NOT IN ('resolved', 'closed', 'false_positive')""",
                        linked_incident["incident_id"],
                        summary or "Resolved via linked finding",
                    )
                    logger.info(f"Auto-resolved linked incident #{linked_incident['incident_id']} when finding #{item_id} resolved")
            elif item_type == "incident":
                await pg_execute(
                    "UPDATE incidents SET status = 'resolved', resolved_at = NOW(), "
                    "resolution_summary = $2, lessons_learned = $3 WHERE id = $1",
                    item_id, summary or None, json.dumps({"lessons": lessons}) if lessons else None,
                )

            # ---- 3. Log resolution to knowledge (async, non-blocking) ----
            asyncio.create_task(_log_resolution_to_knowledge(
                item_id=item_id,
                item_type=item_type,
                item_context=item_context,
                summary=summary,
                lessons=lessons,
                runbook_path=runbook_path,
                domain=domain,
            ))

            return {"status": "resolved", "item_type": item_type, "id": item_id}
        except Exception as e:
            return {"error": str(e)}

    async def _log_resolution_to_knowledge(
        item_id: int, item_type: str, item_context: dict,
        summary: str, lessons: str, runbook_path: str, domain: str,
    ):
        """Write resolution to Qdrant + Neo4j via knowledge-mcp. Fire-and-forget."""
        alert_name = item_context.get("alert_name") or item_context.get("check_name") or f"{item_type}-{item_id}"
        severity = item_context.get("severity", "unknown")
        description = item_context.get("description", "")

        # Auto-detect domain from alert name / description if not provided
        if not domain:
            text = (alert_name + " " + description).lower()
            if any(w in text for w in ("node", "pod", "deploy", "kube", "vm", "proxmox", "storage", "nfs", "dns", "caddy", "firewall")):
                domain = "infra"
            elif any(w in text for w in ("alert", "metric", "prometheus", "grafana", "gatus", "coroot")):
                domain = "obs"
            elif any(w in text for w in ("plex", "sonarr", "radarr", "transmission", "media")):
                domain = "media"
            elif any(w in text for w in ("network", "unifi", "omada", "cloudflare", "tunnel")):
                domain = "network"
            else:
                domain = "infra"

        # ---- A. Log structured resolution event to agent_events ----
        try:
            await call_mcp_tool(KNOWLEDGE_MCP_URL, "log_event", {
                "event_type": f"{item_type}.resolved",
                "description": f"[{severity.upper()}] {alert_name}: {summary}" if summary else f"[{severity.upper()}] {alert_name} resolved",
                "source_agent": "error-hunter",
                "metadata": {
                    "item_id": item_id,
                    "item_type": item_type,
                    "alert_name": alert_name,
                    "severity": severity,
                    "domain": domain,
                    "summary": summary,
                    "lessons": lessons,
                    "runbook_path": runbook_path,
                    "cluster": item_context.get("cluster", ""),
                    "resource": item_context.get("resource", ""),
                },
                "resolution": "completed",
            })
        except Exception as e:
            logger.warning(f"Failed to log resolution event for {item_type} #{item_id}: {e}")

        # ---- B. Match/record runbook execution if a known runbook was used ----
        if summary:
            try:
                matches = await call_mcp_tool(KNOWLEDGE_MCP_URL, "search_runbooks", {
                    "query": alert_name,
                    "limit": 1,
                    "min_score": 0.7,
                })
                results = matches.get("result", [])
                if results and len(results) > 0:
                    rb_id = str(results[0].get("id", ""))
                    if rb_id:
                        await call_mcp_tool(KNOWLEDGE_MCP_URL, "record_runbook_execution", {
                            "runbook_id": rb_id,
                            "success": True,
                        })
                        logger.info(f"Recorded runbook execution for {alert_name} -> runbook {rb_id}")
            except Exception as e:
                logger.warning(f"Failed to record runbook execution: {e}")

        # ---- C. Record lessons learned as a decision (dedup by alert_name) ----
        if lessons:
            try:
                existing = await call_mcp_tool(KNOWLEDGE_MCP_URL, "search_decisions", {
                    "query": alert_name,
                    "limit": 1,
                })
                existing_results = existing.get("result", [])
                already_recorded = False
                if existing_results:
                    for d in existing_results:
                        if alert_name.lower() in (d.get("title", "") or "").lower():
                            already_recorded = True
                            break
                if not already_recorded:
                    await call_mcp_tool(KNOWLEDGE_MCP_URL, "add_decision", {
                        "title": f"Resolution: {alert_name}",
                        "decision": summary or f"Resolved {alert_name}",
                        "rationale": lessons,
                        "context": f"Domain: {domain}. Severity: {severity}. {item_type} #{item_id}. {description[:300]}",
                    })
                    logger.info(f"Recorded lessons for {alert_name} in decisions")
            except Exception as e:
                logger.warning(f"Failed to record lessons: {e}")

    @app.post("/api/estate/remediate/{finding_id}")
    async def trigger_remediation(finding_id: int):
        """Trigger remediation for a specific finding."""
        try:
            rows = await pg_fetch(
                "SELECT * FROM error_hunter_findings WHERE id = $1", finding_id
            )
            if not rows:
                return {"error": "Finding not found"}
            finding = dict(rows[0])

            # Try Tier 1 first
            remediator = REMEDIATORS.get(finding.get("check_name", ""))
            if remediator:
                success = await remediator(finding)
                if success:
                    verified = await verify_remediation(finding)
                    return {"status": "remediated", "tier": 1, "verified": verified}

            # Tier 2: LLM-guided
            result = await llm_remediate(finding)
            return {"status": "remediated" if result["success"] else "failed",
                    "tier": 2, "steps": result.get("steps", []),
                    "summary": result.get("summary", "")}
        except Exception as e:
            return {"error": str(e)}

    @app.post("/api/estate/escalate/{item_id}")
    async def escalate_item(item_id: int, item_type: str = "finding"):
        """Escalate to human review."""
        try:
            if item_type == "finding":
                await pg_execute(
                    "UPDATE error_hunter_findings SET status = 'escalated' WHERE id = $1",
                    item_id
                )
            elif item_type == "incident":
                await pg_execute(
                    "UPDATE incidents SET status = 'escalated' WHERE id = $1",
                    item_id
                )
            return {"status": "escalated", "item_type": item_type, "id": item_id}
        except Exception as e:
            return {"error": str(e)}

    def main():
        port = int(os.environ.get("PORT", "8000"))
        uvicorn.run(app, host="0.0.0.0", port=port)

    if __name__ == "__main__":
        main()

  requirements.txt: |
    langgraph>=0.2.0
    langchain-core>=0.3.0
    asyncpg>=0.30.0
    fastapi>=0.115.0
    uvicorn>=0.34.0
    httpx>=0.28.0
    pydantic>=2.11.0
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: error-hunter
  namespace: ai-platform
  labels:
    app: error-hunter
spec:
  replicas: 1
  selector:
    matchLabels:
      app: error-hunter
  template:
    metadata:
      labels:
        app: error-hunter
    spec:
      initContainers:
        - name: install-deps
          image: python:3.14-slim
          command: ['sh', '-c', 'pip install --target=/app/deps -r /code/requirements.txt']
          volumeMounts:
            - name: code
              mountPath: /code
            - name: deps
              mountPath: /app/deps
      containers:
        - name: error-hunter
          image: python:3.14-slim
          command: ['sh', '-c', 'PYTHONPATH=/app/deps python /code/main.py']
          ports:
            - containerPort: 8000
              name: http
          env:
            - name: PORT
              value: "8000"
            - name: LITELLM_URL
              value: "http://litellm:4000"
            - name: KNOWLEDGE_MCP_URL
              value: "http://knowledge-mcp:8000"
            - name: INFRASTRUCTURE_MCP_URL
              value: "http://infrastructure-mcp:8000"
            - name: OBSERVABILITY_MCP_URL
              value: "http://observability-mcp:8000"
            - name: HOME_MCP_URL
              value: "http://home-mcp:8000"
            - name: MEDIA_MCP_URL
              value: "http://media-mcp:8000"
            - name: EXTERNAL_MCP_URL
              value: "http://external-mcp:8000"
            - name: AFFERENT_URL
              value: "http://10.10.0.22:3456"
            - name: SYNAPSE_URL
              value: "http://10.10.0.22:3460"
            - name: A2A_API_TOKEN
              valueFrom:
                secretKeyRef:
                  name: a2a-api-token
                  key: TOKEN
            - name: AFFERENT_AUTH_TOKEN
              valueFrom:
                secretKeyRef:
                  name: afferent-auth-token
                  key: AUTH_TOKEN
                  optional: true
            - name: INCIDENT_DB_URL
              valueFrom:
                secretKeyRef:
                  name: incident-db-credentials
                  key: DATABASE_URL
            - name: GITHUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: mcp-github
                  key: GITHUB_TOKEN
            - name: BESZEL_URL
              value: "https://beszel.kernow.io"
            - name: BESZEL_USERNAME
              valueFrom:
                secretKeyRef:
                  name: beszel-credentials
                  key: USERNAME
            - name: BESZEL_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: beszel-credentials
                  key: PASSWORD
            - name: PBS_HOST
              value: "https://10.10.0.151:8007"
            - name: PBS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: alert-forwarder-pbs
                  key: PASSWORD
            - name: ADGUARD_URL
              value: "http://10.10.0.1:3000"
            - name: ADGUARD_USER
              valueFrom:
                secretKeyRef:
                  name: mcp-adguard
                  key: USERNAME
            - name: ADGUARD_PASS
              valueFrom:
                secretKeyRef:
                  name: mcp-adguard
                  key: PASSWORD
            - name: PVE_RUAPEHU_TOKEN_ID
              valueFrom:
                secretKeyRef:
                  name: mcp-proxmox-ruapehu
                  key: TOKEN_ID
            - name: PVE_RUAPEHU_TOKEN_SECRET
              valueFrom:
                secretKeyRef:
                  name: mcp-proxmox-ruapehu
                  key: TOKEN_SECRET
            - name: PVE_PIHANGA_TOKEN_ID
              valueFrom:
                secretKeyRef:
                  name: mcp-proxmox-pihanga
                  key: TOKEN_ID
            - name: PVE_PIHANGA_TOKEN_SECRET
              valueFrom:
                secretKeyRef:
                  name: mcp-proxmox-pihanga
                  key: TOKEN_SECRET
          volumeMounts:
            - name: code
              mountPath: /code
            - name: deps
              mountPath: /app/deps
          resources:
            requests:
              memory: "384Mi"
              cpu: "100m"
            limits:
              memory: "768Mi"
              cpu: "500m"
          readinessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 30
            periodSeconds: 30
          livenessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 60
            periodSeconds: 60
      volumes:
        - name: code
          configMap:
            name: error-hunter-code
        - name: deps
          emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: error-hunter
  namespace: ai-platform
  labels:
    app: error-hunter
spec:
  type: NodePort
  selector:
    app: error-hunter
  ports:
    - port: 8000
      targetPort: 8000
      nodePort: 30801
      name: http
---
apiVersion: secrets.infisical.com/v1alpha1
kind: InfisicalSecret
metadata:
  name: beszel-credentials
  namespace: ai-platform
spec:
  hostAPI: https://app.infisical.com/api
  authentication:
    universalAuth:
      credentialsRef:
        secretName: universal-auth-credentials
        secretNamespace: infisical-operator-system
      secretsScope:
        projectSlug: prod-homelab-y-nij
        envSlug: prod
        secretsPath: /observability/beszel
  managedSecretReference:
    secretName: beszel-credentials
    secretNamespace: ai-platform
    secretType: Opaque
    creationPolicy: Owner
