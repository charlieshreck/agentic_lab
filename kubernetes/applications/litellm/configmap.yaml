apiVersion: v1
kind: ConfigMap
metadata:
  name: litellm-config
  namespace: ai-platform
  labels:
    app: litellm
data:
  config.yaml: |
    # LiteLLM Proxy Configuration
    # See: https://docs.litellm.ai/docs/proxy/configs
    #
    # Gemini Flash Only - FREE TIER
    # WARNING: Do not use Pro models without budget controls

    model_list:
      # Primary: Direct Gemini API (free tier)
      - model_name: gemini/gemini-2.0-flash
        litellm_params:
          model: gemini/gemini-2.0-flash
          api_key: os.environ/GEMINI_API_KEY

      # Fallback: Local Ollama (no rate limits)
      - model_name: ollama-qwen
        litellm_params:
          model: ollama/qwen2.5:7b
          api_base: http://ollama:11434

      # Embedding model
      - model_name: embeddings
        litellm_params:
          model: gemini/text-embedding-004
          api_key: os.environ/GEMINI_API_KEY

      # Aliases
      - model_name: gemini-pro
        litellm_params:
          model: gemini/gemini-2.0-flash
          api_key: os.environ/GEMINI_API_KEY

      - model_name: gemini/gemini-2.0-pro
        litellm_params:
          model: gemini/gemini-2.0-flash
          api_key: os.environ/GEMINI_API_KEY

    # Router settings - fallback to OpenRouter on 429
    router_settings:
      routing_strategy: simple-shuffle
      num_retries: 1
      retry_after: 0
      timeout: 60
      allowed_fails: 1
      fallbacks:
        - gemini/gemini-2.0-flash:
            - ollama-qwen
        - gemini-pro:
            - ollama-qwen
        - gemini/gemini-2.0-pro:
            - ollama-qwen

    # General settings
    general_settings:
      master_key: null
      drop_params: true

    # Logging - track all requests to catch abuse
    litellm_settings:
      set_verbose: false
      json_logs: true
      store_audit_logs: true
      request_timeout: 60

    # Log every request with caller info
    environment_variables:
      LITELLM_LOG_LEVEL: INFO
