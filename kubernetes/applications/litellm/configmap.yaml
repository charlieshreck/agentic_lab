apiVersion: v1
kind: ConfigMap
metadata:
  name: litellm-config
  namespace: ai-platform
  labels:
    app: litellm
data:
  config.yaml: |
    # LiteLLM Proxy Configuration
    # See: https://docs.litellm.ai/docs/proxy/configs
    #
    # OLLAMA-ONLY Configuration
    # All inference runs locally on Ollama (qwen2.5:7b)
    # Complex issues escalate to Brain Trust (Outline) for human review
    # Claude Code handles implementation after Brain Trust approval

    model_list:
      # ===== PRIMARY: Ollama qwen2.5:7b =====
      # Use for: ALL automated triage, runbook execution, MCP tool usage
      - model_name: tier1
        litellm_params:
          model: ollama/qwen2.5:7b
          api_base: http://ollama:11434

      - model_name: tier2
        litellm_params:
          model: ollama/qwen2.5:7b
          api_base: http://ollama:11434

      - model_name: ollama-qwen
        litellm_params:
          model: ollama/qwen2.5:7b
          api_base: http://ollama:11434

      # Default model alias (for backward compatibility)
      - model_name: gemini/gemini-2.0-flash
        litellm_params:
          model: ollama/qwen2.5:7b
          api_base: http://ollama:11434

      - model_name: gemini-pro
        litellm_params:
          model: ollama/qwen2.5:7b
          api_base: http://ollama:11434

      # ===== EMBEDDINGS: Local Ollama via OpenAI-compatible API =====
      # Using OpenAI provider to talk to Ollama's /v1/embeddings endpoint
      # This avoids LiteLLM's broken async handling for ollama/ prefix (issue #7451)
      - model_name: embeddings
        litellm_params:
          model: openai/nomic-embed-text
          api_base: http://ollama:11434/v1
          api_key: "not-needed"

      - model_name: text-embedding-004
        litellm_params:
          model: openai/nomic-embed-text
          api_base: http://ollama:11434/v1
          api_key: "not-needed"

    # Router settings - all routes to Ollama
    router_settings:
      routing_strategy: simple-shuffle
      num_retries: 2
      retry_after: 1
      timeout: 120
      allowed_fails: 2

    # General settings
    general_settings:
      master_key: null
      drop_params: true

    # Logging - track all requests
    litellm_settings:
      set_verbose: false
      json_logs: true
      store_audit_logs: true
      request_timeout: 120

    # Log every request with caller info
    environment_variables:
      LITELLM_LOG_LEVEL: INFO
