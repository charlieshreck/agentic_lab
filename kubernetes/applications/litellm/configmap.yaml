apiVersion: v1
kind: ConfigMap
metadata:
  name: litellm-config
  namespace: ai-platform
  labels:
    app: litellm
data:
  config.yaml: |
    # LiteLLM Proxy Configuration
    # See: https://docs.litellm.ai/docs/proxy/configs
    #
    # Gemini Flash Only - FREE TIER
    # WARNING: Do not use Pro models without budget controls

    model_list:
      # ===== TIER 1: Local/Free (Ollama) =====
      # Use for: classification, filtering, simple tasks
      - model_name: tier1
        litellm_params:
          model: ollama/qwen2.5:7b
          api_base: http://ollama:11434

      - model_name: ollama-qwen
        litellm_params:
          model: ollama/qwen2.5:7b
          api_base: http://ollama:11434

      - model_name: ollama-llama
        litellm_params:
          model: ollama/llama3.1:8b
          api_base: http://ollama:11434

      # Local embeddings (when Gemini rate limited)
      - model_name: embeddings-local
        litellm_params:
          model: ollama/nomic-embed-text
          api_base: http://ollama:11434

      # ===== TIER 2: Gemini (Free Tier, 1M Context) =====
      # Use for: analysis, summaries, large context tasks
      - model_name: tier2
        litellm_params:
          model: gemini/gemini-2.0-flash
          api_key: os.environ/GEMINI_API_KEY

      - model_name: gemini/gemini-2.0-flash
        litellm_params:
          model: gemini/gemini-2.0-flash
          api_key: os.environ/GEMINI_API_KEY

      - model_name: gemini-pro
        litellm_params:
          model: gemini/gemini-2.0-flash
          api_key: os.environ/GEMINI_API_KEY

      - model_name: gemini/gemini-2.0-pro
        litellm_params:
          model: gemini/gemini-2.0-flash
          api_key: os.environ/GEMINI_API_KEY

      # Gemini embeddings (primary)
      - model_name: embeddings
        litellm_params:
          model: gemini/text-embedding-004
          api_key: os.environ/GEMINI_API_KEY

      # ===== TIER 3: Claude (Premium, Best Quality) =====
      # Use for: validation, code gen, security, complex reasoning
      # Note: Requires ANTHROPIC_API_KEY in Infisical
      - model_name: tier3
        litellm_params:
          model: anthropic/claude-sonnet-4-20250514
          api_key: os.environ/ANTHROPIC_API_KEY

      - model_name: claude-sonnet
        litellm_params:
          model: anthropic/claude-sonnet-4-20250514
          api_key: os.environ/ANTHROPIC_API_KEY

      - model_name: claude-haiku
        litellm_params:
          model: anthropic/claude-3-5-haiku-20241022
          api_key: os.environ/ANTHROPIC_API_KEY

    # Router settings - tiered fallbacks
    router_settings:
      routing_strategy: simple-shuffle
      num_retries: 1
      retry_after: 0
      timeout: 60
      allowed_fails: 1
      fallbacks:
        # Tier 3 → Tier 2 → Tier 1
        - tier3:
            - tier2
            - tier1
        - claude-sonnet:
            - tier2
            - tier1
        - claude-haiku:
            - tier2
            - tier1
        # Tier 2 → Tier 1
        - tier2:
            - tier1
        - gemini/gemini-2.0-flash:
            - ollama-qwen
        - gemini-pro:
            - ollama-qwen
        - gemini/gemini-2.0-pro:
            - ollama-qwen
        # Embeddings fallback
        - embeddings:
            - embeddings-local

    # General settings
    general_settings:
      master_key: null
      drop_params: true

    # Logging - track all requests to catch abuse
    litellm_settings:
      set_verbose: false
      json_logs: true
      store_audit_logs: true
      request_timeout: 60

    # Log every request with caller info
    environment_variables:
      LITELLM_LOG_LEVEL: INFO
