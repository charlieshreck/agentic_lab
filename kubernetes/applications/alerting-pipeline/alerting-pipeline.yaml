---
apiVersion: v1
kind: ConfigMap
metadata:
  name: alerting-pipeline-code
  namespace: ai-platform
data:
  main.py: |
    #!/usr/bin/env python3
    """
    Alerting Pipeline - Polls MCP servers and raises AI alerts.

    This service periodically checks infrastructure health via MCP servers
    and raises alerts to the Telegram service when issues are detected.
    """
    import os
    import logging
    import asyncio
    from datetime import datetime
    from typing import Dict, List, Any, Optional

    from fastapi import FastAPI
    from pydantic import BaseModel
    import httpx
    import uvicorn

    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

    # Service URLs
    TELEGRAM_SERVICE_URL = os.environ.get("TELEGRAM_SERVICE_URL", "http://telegram-service:8000")

    # MCP Server endpoints
    MCP_SERVERS = {
        "unifi": os.environ.get("UNIFI_MCP_URL", "http://unifi-mcp:8000"),
        "truenas": os.environ.get("TRUENAS_MCP_URL", "http://truenas-mcp:8000"),
        "proxmox": os.environ.get("PROXMOX_MCP_URL", "http://proxmox-mcp:8000"),
        "opnsense": os.environ.get("OPNSENSE_MCP_URL", "http://opnsense-mcp:8000"),
        "adguard": os.environ.get("ADGUARD_MCP_URL", "http://adguard-mcp:8000"),
    }

    # Poll intervals in seconds
    POLL_INTERVAL = int(os.environ.get("POLL_INTERVAL", "300"))  # 5 minutes default
    CRITICAL_POLL_INTERVAL = int(os.environ.get("CRITICAL_POLL_INTERVAL", "60"))  # 1 minute for critical checks

    # Track raised alerts to avoid duplicates
    raised_alerts: Dict[str, datetime] = {}
    ALERT_COOLDOWN_SECONDS = 3600  # 1 hour cooldown before re-alerting on same issue

    app = FastAPI(title="Alerting Pipeline")

    class AlertRule(BaseModel):
        name: str
        source: str
        check_type: str  # threshold, presence, absence
        threshold: Optional[float] = None
        severity: str = "warning"
        description_template: str
        fix_template: Optional[str] = None

    # Define alert rules
    ALERT_RULES = [
        # UniFi alerts
        AlertRule(
            name="UniFi Device Offline",
            source="unifi",
            check_type="device_offline",
            severity="critical",
            description_template="UniFi device '{device_name}' is offline. Last seen: {last_seen}",
            fix_template="Check physical connectivity and power for device '{device_name}'"
        ),
        AlertRule(
            name="UniFi High Client Count",
            source="unifi",
            check_type="threshold",
            threshold=50,
            severity="warning",
            description_template="Access point '{ap_name}' has {client_count} clients connected (threshold: 50)",
            fix_template=None
        ),
        # TrueNAS alerts
        AlertRule(
            name="TrueNAS Pool Degraded",
            source="truenas",
            check_type="pool_health",
            severity="critical",
            description_template="ZFS pool '{pool_name}' is in degraded state: {status}",
            fix_template="Run 'zpool status {pool_name}' to check disk status. Consider replacing failed drives."
        ),
        AlertRule(
            name="TrueNAS Low Disk Space",
            source="truenas",
            check_type="threshold",
            threshold=85,
            severity="warning",
            description_template="Pool '{pool_name}' is at {usage_percent}% capacity",
            fix_template="Review and clean up data or expand storage"
        ),
        # Proxmox alerts
        AlertRule(
            name="Proxmox VM Not Running",
            source="proxmox",
            check_type="vm_status",
            severity="warning",
            description_template="VM '{vm_name}' (ID: {vm_id}) is not running. Status: {status}",
            fix_template="Start the VM using: qm start {vm_id}"
        ),
        AlertRule(
            name="Proxmox High CPU",
            source="proxmox",
            check_type="threshold",
            threshold=90,
            severity="warning",
            description_template="Node '{node_name}' CPU usage at {cpu_percent}%",
            fix_template=None
        ),
        # OPNsense alerts
        AlertRule(
            name="OPNsense Service Down",
            source="opnsense",
            check_type="service_status",
            severity="critical",
            description_template="Service '{service_name}' is not running on OPNsense",
            fix_template="SSH to OPNsense and run: service {service_name} restart"
        ),
        # AdGuard alerts
        AlertRule(
            name="AdGuard High Block Rate",
            source="adguard",
            check_type="threshold",
            threshold=70,
            severity="warning",
            description_template="AdGuard blocking {block_percent}% of queries - may indicate malware activity",
            fix_template="Review blocked domains in AdGuard admin panel"
        ),
    ]

    CLAUDE_AGENT_URL = os.environ.get("CLAUDE_AGENT_URL", "http://claude-agent:8000")

    async def query_via_claude_agent(query: str, timeout: int = 120) -> dict:
        """Query infrastructure via Claude Agent which can access MCP tools."""
        async with httpx.AsyncClient(timeout=30.0) as client:
            try:
                # Submit task
                response = await client.post(
                    f"{CLAUDE_AGENT_URL}/agent/run",
                    json={
                        "prompt": query,
                        "context": {"source": "alerting_pipeline"},
                        "max_turns": 3,
                        "working_directory": "/workspace",
                        "async_mode": True,
                        "timeout": timeout
                    }
                )
                result = response.json()
                if not result.get("success") or not result.get("task_id"):
                    return {"error": result.get("error", "Failed to queue task")}

                task_id = result.get("task_id")

                # Poll for completion
                for _ in range(timeout // 5):
                    await asyncio.sleep(5)
                    status_resp = await client.get(f"{CLAUDE_AGENT_URL}/task/{task_id}")
                    status = status_resp.json()
                    if status.get("status") == "completed":
                        return {"result": status.get("result", "")}
                    elif status.get("status") == "failed":
                        return {"error": status.get("error", "Unknown error")}

                return {"error": "Query timed out"}
            except Exception as e:
                return {"error": str(e)}

    async def raise_alert(name: str, severity: str, description: str,
                          source: str, suggested_fix: str = None, context: str = None):
        """Raise an alert via the Telegram service."""
        # Check cooldown
        alert_key = f"{source}:{name}"
        if alert_key in raised_alerts:
            elapsed = (datetime.utcnow() - raised_alerts[alert_key]).total_seconds()
            if elapsed < ALERT_COOLDOWN_SECONDS:
                logger.debug(f"Alert '{name}' still in cooldown ({elapsed:.0f}s)")
                return

        logger.info(f"Raising alert: {name} ({severity})")
        async with httpx.AsyncClient(timeout=30.0) as client:
            try:
                response = await client.post(
                    f"{TELEGRAM_SERVICE_URL}/alert/raise",
                    json={
                        "name": name,
                        "severity": severity,
                        "description": description,
                        "suggested_fix": suggested_fix,
                        "source": source,
                        "context": context
                    }
                )
                if response.status_code == 200:
                    raised_alerts[alert_key] = datetime.utcnow()
                    logger.info(f"Alert raised: {response.json()}")
                else:
                    logger.error(f"Failed to raise alert: {response.text}")
            except Exception as e:
                logger.error(f"Error raising alert: {e}")

    async def check_service_connectivity(name: str, url: str) -> bool:
        """Check if a service is reachable."""
        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                # Try common health endpoints
                for endpoint in ["/health", "/healthz", "/", "/sse"]:
                    try:
                        response = await client.get(f"{url}{endpoint}")
                        if response.status_code < 500:
                            return True
                    except:
                        continue
                return False
            except Exception:
                return False

    async def run_deep_health_check():
        """Run comprehensive health check via Claude Agent (expensive, runs less frequently)."""
        logger.info("Running deep health check via Claude Agent...")

        queries = [
            ("UniFi", "Check UniFi network status. List any devices that are offline or have issues. Report health status."),
            ("TrueNAS", "Check TrueNAS storage pools. Report any degraded pools or low disk space warnings."),
            ("Proxmox", "Check Proxmox cluster status. Report any offline nodes or high resource usage."),
        ]

        for source, query in queries:
            try:
                result = await query_via_claude_agent(query, timeout=180)
                if "error" in result:
                    logger.warning(f"{source} deep check failed: {result['error']}")
                    continue

                response = result.get("result", "").lower()

                # Simple keyword detection for issues
                alert_keywords = ["offline", "degraded", "critical", "error", "failed", "down", "unreachable"]
                if any(kw in response for kw in alert_keywords):
                    await raise_alert(
                        name=f"{source} Health Issue Detected",
                        severity="warning",
                        description=f"Health check detected potential issues:\n\n{result.get('result', '')[:500]}",
                        source=source.lower(),
                        suggested_fix=f"Review {source} status and address any issues found"
                    )
            except Exception as e:
                logger.error(f"Deep health check error for {source}: {e}")

    async def check_mcp_service_health():
        """Check if MCP services are reachable (basic connectivity check)."""
        for name, url in MCP_SERVERS.items():
            try:
                is_healthy = await check_service_connectivity(name, url)
                if not is_healthy:
                    await raise_alert(
                        name=f"MCP Service Unreachable: {name}",
                        severity="warning",
                        description=f"MCP server '{name}' at {url} is not responding",
                        source="alerting-pipeline",
                        suggested_fix=f"Check pod status: kubectl get pods -n ai-platform | grep {name}"
                    )
            except Exception as e:
                logger.error(f"Error checking {name}: {e}")

    # Deep check interval (every 30 minutes by default)
    DEEP_CHECK_INTERVAL = int(os.environ.get("DEEP_CHECK_INTERVAL", "1800"))
    last_deep_check = 0

    async def run_health_checks():
        """Run health checks - basic connectivity checks."""
        logger.info("Running basic health checks...")
        await check_mcp_service_health()
        logger.info("Basic health checks complete")

    async def polling_loop():
        """Main polling loop."""
        global last_deep_check
        import time

        logger.info(f"Starting polling loop (interval: {POLL_INTERVAL}s, deep check: {DEEP_CHECK_INTERVAL}s)")
        while True:
            try:
                # Always run basic connectivity checks
                await run_health_checks()

                # Run deep health check less frequently (uses Claude Agent - expensive)
                current_time = time.time()
                if current_time - last_deep_check >= DEEP_CHECK_INTERVAL:
                    await run_deep_health_check()
                    last_deep_check = current_time
            except Exception as e:
                logger.error(f"Polling error: {e}")
            await asyncio.sleep(POLL_INTERVAL)

    @app.on_event("startup")
    async def startup():
        asyncio.create_task(polling_loop())
        logger.info("Alerting pipeline started")

    @app.get("/health")
    async def health():
        return {
            "status": "healthy",
            "active_alerts": len(raised_alerts),
            "mcp_servers": list(MCP_SERVERS.keys())
        }

    @app.post("/check")
    async def trigger_check():
        """Manually trigger a health check."""
        await run_health_checks()
        return {"status": "checks_complete"}

    @app.get("/raised")
    async def list_raised():
        """List recently raised alerts."""
        return {
            "alerts": [
                {"key": k, "raised_at": v.isoformat()}
                for k, v in raised_alerts.items()
            ]
        }

    def main():
        port = int(os.environ.get("PORT", "8000"))
        uvicorn.run(app, host="0.0.0.0", port=port)

    if __name__ == "__main__":
        main()

  requirements.txt: |
    fastapi>=0.115.0
    uvicorn>=0.34.0
    httpx>=0.28.0
    pydantic>=2.11.0
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: alerting-pipeline
  namespace: ai-platform
  labels:
    app: alerting-pipeline
spec:
  replicas: 1
  selector:
    matchLabels:
      app: alerting-pipeline
  template:
    metadata:
      labels:
        app: alerting-pipeline
    spec:
      initContainers:
        - name: install-deps
          image: python:3.11-slim
          command: ['sh', '-c', 'pip install --target=/app/deps -r /code/requirements.txt']
          volumeMounts:
            - name: code
              mountPath: /code
            - name: deps
              mountPath: /app/deps
      containers:
        - name: alerting-pipeline
          image: python:3.11-slim
          command: ['sh', '-c', 'PYTHONPATH=/app/deps python /code/main.py']
          ports:
            - containerPort: 8000
              name: http
          env:
            - name: PORT
              value: "8000"
            - name: TELEGRAM_SERVICE_URL
              value: "http://telegram-service:8000"
            - name: UNIFI_MCP_URL
              value: "http://unifi-mcp:8000"
            - name: TRUENAS_MCP_URL
              value: "http://truenas-mcp:8000"
            - name: PROXMOX_MCP_URL
              value: "http://proxmox-mcp:8000"
            - name: OPNSENSE_MCP_URL
              value: "http://opnsense-mcp:8000"
            - name: ADGUARD_MCP_URL
              value: "http://adguard-mcp:8000"
            - name: CLAUDE_AGENT_URL
              value: "http://claude-agent:8000"
            - name: POLL_INTERVAL
              value: "300"  # 5 minutes
            - name: DEEP_CHECK_INTERVAL
              value: "1800"  # 30 minutes
          volumeMounts:
            - name: code
              mountPath: /code
            - name: deps
              mountPath: /app/deps
          resources:
            requests:
              memory: "128Mi"
              cpu: "50m"
            limits:
              memory: "256Mi"
              cpu: "200m"
          readinessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 30
            periodSeconds: 30
          livenessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 60
            periodSeconds: 60
      volumes:
        - name: code
          configMap:
            name: alerting-pipeline-code
        - name: deps
          emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: alerting-pipeline
  namespace: ai-platform
spec:
  selector:
    app: alerting-pipeline
  ports:
    - port: 8000
      targetPort: 8000
      name: http
