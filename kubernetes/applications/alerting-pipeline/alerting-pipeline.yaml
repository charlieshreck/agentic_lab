---
apiVersion: v1
kind: ConfigMap
metadata:
  name: alerting-pipeline-code
  namespace: ai-platform
data:
  main.py: |
    #!/usr/bin/env python3
    """
    Alerting Pipeline - Polls MCP servers for health data and forwards
    issues to LangGraph for AI-assisted triage and incident management.

    Output: All alerts go to LangGraph /ingest → Afferent → Claude screens.
    """
    import os
    import json
    import logging
    import asyncio
    from datetime import datetime
    from typing import Dict, List, Any, Optional

    from fastapi import FastAPI, Request
    from pydantic import BaseModel
    import httpx
    import uvicorn

    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

    # Service URLs
    LANGGRAPH_URL = os.environ.get("LANGGRAPH_URL", "http://langgraph:8000")
    A2A_API_TOKEN = os.environ.get("A2A_API_TOKEN", "")

    # Consolidated domain MCP endpoints
    MCP_SERVERS = {
        "infrastructure": os.environ.get("INFRASTRUCTURE_MCP_URL", "http://infrastructure-mcp:8000"),
        "home": os.environ.get("HOME_MCP_URL", "http://home-mcp:8000"),
        "media": os.environ.get("MEDIA_MCP_URL", "http://media-mcp:8000"),
        "observability": os.environ.get("OBSERVABILITY_MCP_URL", "http://observability-mcp:8000"),
    }

    # Poll intervals in seconds
    POLL_INTERVAL = int(os.environ.get("POLL_INTERVAL", "300"))  # 5 minutes default

    # Track raised alerts to avoid duplicates
    raised_alerts: Dict[str, datetime] = {}
    ALERT_COOLDOWN_SECONDS = 3600  # 1 hour cooldown before re-alerting on same issue

    app = FastAPI(title="Alerting Pipeline")

    class AlertRule(BaseModel):
        name: str
        source: str
        check_type: str  # threshold, presence, absence
        threshold: Optional[float] = None
        severity: str = "warning"
        description_template: str
        fix_template: Optional[str] = None

    # Define alert rules
    ALERT_RULES = [
        # UniFi alerts (via home-mcp)
        AlertRule(
            name="UniFi Device Offline",
            source="home",
            check_type="device_offline",
            severity="critical",
            description_template="UniFi device '{device_name}' is offline. Last seen: {last_seen}",
            fix_template="Check physical connectivity and power for device '{device_name}'"
        ),
        AlertRule(
            name="UniFi High Client Count",
            source="home",
            check_type="threshold",
            threshold=50,
            severity="warning",
            description_template="Access point '{ap_name}' has {client_count} clients connected (threshold: 50)",
            fix_template=None
        ),
        # TrueNAS alerts (via infrastructure-mcp)
        AlertRule(
            name="TrueNAS Pool Degraded",
            source="infrastructure",
            check_type="pool_health",
            severity="critical",
            description_template="ZFS pool '{pool_name}' is in degraded state: {status}",
            fix_template="Run 'zpool status {pool_name}' to check disk status. Consider replacing failed drives."
        ),
        AlertRule(
            name="TrueNAS Low Disk Space",
            source="infrastructure",
            check_type="threshold",
            threshold=85,
            severity="warning",
            description_template="Pool '{pool_name}' is at {usage_percent}% capacity",
            fix_template="See runbook: /home/agentic_lab/runbooks/alerts/truenas-low-disk-space.md. Quick: (1) Identify space consumers: zfs list -r {pool_name}; (2) Remove old snapshots or files; (3) Expand pool if needed."
        ),
        # Proxmox alerts (via infrastructure-mcp)
        AlertRule(
            name="Proxmox VM Not Running",
            source="infrastructure",
            check_type="vm_status",
            severity="warning",
            description_template="VM '{vm_name}' (ID: {vm_id}) is not running. Status: {status}",
            fix_template="Start the VM using: qm start {vm_id}"
        ),
        AlertRule(
            name="Proxmox High CPU",
            source="infrastructure",
            check_type="threshold",
            threshold=90,
            severity="warning",
            description_template="Node '{node_name}' CPU usage at {cpu_percent}%",
            fix_template=None
        ),
        # OPNsense alerts (via infrastructure-mcp)
        AlertRule(
            name="OPNsense Service Down",
            source="infrastructure",
            check_type="service_status",
            severity="critical",
            description_template="Service '{service_name}' is not running on OPNsense",
            fix_template="SSH to OPNsense and run: service {service_name} restart"
        ),
        # AdGuard alerts (via home-mcp)
        AlertRule(
            name="AdGuard High Block Rate",
            source="home",
            check_type="threshold",
            threshold=85,
            severity="warning",
            description_template="AdGuard blocking {block_percent}% of queries - may indicate malware activity",
            fix_template="Review blocked domains in AdGuard admin panel"
        ),
        # Functional health alerts ("Silent Killers")
        AlertRule(
            name="Media Import Stalled",
            source="media",
            check_type="queue_stall",
            severity="warning",
            description_template="{count} items stuck in *arr download queue"
        ),
        AlertRule(
            name="Tailscale VPN Down",
            source="infrastructure",
            check_type="service_status",
            severity="critical",
            description_template="Tailscale on OPNsense is offline"
        ),
        AlertRule(
            name="Cloudflare Tunnel Unhealthy",
            source="infrastructure",
            check_type="tunnel_health",
            severity="critical",
            description_template="Tunnel '{name}' status: {status}"
        ),
        AlertRule(
            name="NFS Mount Stale",
            source="infrastructure",
            check_type="canary_check",
            severity="critical",
            description_template="NFS mount canary failed — storage disconnected"
        ),
        # ntopng network traffic alerts (via observability-mcp)
        AlertRule(
            name="ntopng Flow Alert",
            source="observability",
            check_type="ntopng_alerts",
            severity="warning",
            description_template="ntopng: {alert_name} — {cli} → {srv}"
        ),
        AlertRule(
            name="ntopng Host Alert",
            source="observability",
            check_type="ntopng_host_alerts",
            severity="warning",
            description_template="ntopng host alert: {alert_name} on {host}"
        ),
    ]

    # ============================================================================
    # LangGraph Forwarding (sole output path)
    # ============================================================================

    async def forward_to_langgraph(alertname: str, severity: str, description: str,
                                    source: str, namespace: str = "ai-platform"):
        """Forward alert to LangGraph /ingest endpoint for AI-assisted triage."""
        alert_key = f"{source}:{alertname}"
        if alert_key in raised_alerts:
            elapsed = (datetime.utcnow() - raised_alerts[alert_key]).total_seconds()
            if elapsed < ALERT_COOLDOWN_SECONDS:
                logger.debug(f"Alert '{alertname}' still in cooldown ({elapsed:.0f}s)")
                return None

        logger.info(f"Forwarding to LangGraph: {alertname} ({severity})")
        async with httpx.AsyncClient(timeout=60.0) as client:
            try:
                response = await client.post(
                    f"{LANGGRAPH_URL}/ingest?source={source}",
                    json={
                        "alert_name": alertname,
                        "severity": severity,
                        "description": description,
                        "labels": {"source": source, "namespace": namespace},
                    }
                )
                if 200 <= response.status_code < 300:
                    raised_alerts[alert_key] = datetime.utcnow()
                    try:
                        result = response.json()
                    except Exception:
                        result = {"status": "accepted", "code": response.status_code}
                    logger.info(f"LangGraph ingest (HTTP {response.status_code}): {result}")
                    return result
                else:
                    logger.error(f"LangGraph ingest error (HTTP {response.status_code}): {response.text}")
                    return None
            except Exception as e:
                logger.error(f"Error forwarding to LangGraph: {e}")
                return None

    # ============================================================================
    # MCP Tool Calls (via REST bridge /api/call)
    # ============================================================================

    async def call_mcp_tool(mcp_url: str, tool_name: str, params: dict = None) -> dict:
        """Call a tool on a consolidated MCP server via REST bridge (/api/call)."""
        headers = {"Content-Type": "application/json"}
        if A2A_API_TOKEN:
            headers["Authorization"] = f"Bearer {A2A_API_TOKEN}"
        async with httpx.AsyncClient(timeout=30.0) as client:
            try:
                resp = await client.post(
                    f"{mcp_url}/api/call",
                    json={"tool": tool_name, "arguments": params or {}},
                    headers=headers,
                )
                if resp.status_code == 200:
                    result = resp.json()
                    if result.get("status") == "success":
                        output = result.get("output")
                        if isinstance(output, dict):
                            return output
                        if isinstance(output, str):
                            try:
                                return json.loads(output)
                            except (json.JSONDecodeError, ValueError):
                                return {"text": output}
                        if isinstance(output, list):
                            return {"items": output}
                        return {"result": output}
                else:
                    logger.warning(f"MCP tool call {tool_name} returned HTTP {resp.status_code}")
            except Exception as e:
                logger.error(f"MCP tool call error ({tool_name}): {e}")
        return {}

    # ============================================================================
    # Health Check Polling
    # ============================================================================

    async def run_threshold_checks():
        """Evaluate ALERT_RULES against live data from consolidated MCPs."""
        logger.info("Running threshold checks...")
        for rule in ALERT_RULES:
            try:
                mcp_url = MCP_SERVERS.get(rule.source)
                if not mcp_url:
                    continue

                if rule.check_type == "device_offline" and rule.source == "home":
                    result = await call_mcp_tool(mcp_url, "unifi_list_devices")
                    if result:
                        devices = result.get("devices", result.get("result", []))
                        if isinstance(devices, list):
                            for device in devices:
                                if isinstance(device, dict) and device.get("state", "") not in ("connected", "1"):
                                    await forward_to_langgraph(
                                        alertname=rule.name,
                                        severity=rule.severity,
                                        description=f"UniFi device '{device.get('name', 'unknown')}' is offline.",
                                        source="alerting-pipeline"
                                    )

                elif rule.check_type == "pool_health" and rule.source == "infrastructure":
                    result = await call_mcp_tool(mcp_url, "truenas_list_pools")
                    if result:
                        pools = result.get("pools", result.get("result", []))
                        if isinstance(pools, list):
                            for pool in pools:
                                if isinstance(pool, dict) and pool.get("status", "").upper() not in ("ONLINE", "HEALTHY"):
                                    await forward_to_langgraph(
                                        alertname=rule.name,
                                        severity=rule.severity,
                                        description=f"ZFS pool '{pool.get('name', 'unknown')}' status: {pool.get('status', 'unknown')}",
                                        source="alerting-pipeline"
                                    )

                elif rule.check_type == "vm_status" and rule.source == "infrastructure":
                    result = await call_mcp_tool(mcp_url, "proxmox_list_vms")
                    if result:
                        vms = result.get("vms", result.get("result", []))
                        if isinstance(vms, list):
                            for vm in vms:
                                if isinstance(vm, dict) and vm.get("status", "") != "running":
                                    await forward_to_langgraph(
                                        alertname=rule.name,
                                        severity=rule.severity,
                                        description=f"VM '{vm.get('name', 'unknown')}' (ID: {vm.get('vmid', '?')}) status: {vm.get('status', 'unknown')}",
                                        source="alerting-pipeline"
                                    )

                elif rule.check_type == "service_status" and rule.source == "infrastructure":
                    result = await call_mcp_tool(mcp_url, "get_services")
                    if result:
                        services = result.get("services", result.get("result", []))
                        if isinstance(services, list):
                            for svc in services:
                                if isinstance(svc, dict) and not svc.get("running", True):
                                    await forward_to_langgraph(
                                        alertname=rule.name,
                                        severity=rule.severity,
                                        description=f"Service '{svc.get('name', 'unknown')}' is not running on OPNsense",
                                        source="alerting-pipeline"
                                    )

            except Exception as e:
                logger.error(f"Threshold check error for {rule.name}: {e}")

    async def run_functional_checks():
        """Detect 'Silent Killers' — services that are up but broken."""
        logger.info("Running functional health checks...")

        # 1. Media Import Stall — downloads finish but import stalls
        try:
            media_url = MCP_SERVERS.get("media")
            if media_url:
                sonarr_q = await call_mcp_tool(media_url, "sonarr_get_queue") or {}
                radarr_q = await call_mcp_tool(media_url, "radarr_get_queue") or {}
                sonarr_records = sonarr_q.get("records", sonarr_q.get("result", []))
                radarr_records = radarr_q.get("records", radarr_q.get("result", []))
                if not isinstance(sonarr_records, list):
                    sonarr_records = []
                if not isinstance(radarr_records, list):
                    radarr_records = []
                stalled = [i for i in (sonarr_records + radarr_records)
                           if isinstance(i, dict) and i.get("status", "").lower() in ("warning", "stalled")]
                if stalled:
                    await forward_to_langgraph(
                        alertname="MediaImportStalled",
                        severity="warning",
                        description=f"{len(stalled)} items stuck in download queue (stalled imports detected).",
                        source="alerting-pipeline"
                    )
        except Exception as e:
            logger.error(f"Media import check error: {e}")

        # 2. Tailscale VPN — OPNsense Tailscale service
        try:
            infra_url = MCP_SERVERS.get("infrastructure")
            if infra_url:
                ts_status = await call_mcp_tool(infra_url, "get_tailscale_status") or {}
                status_val = ts_status.get("status", ts_status.get("result", ""))
                if isinstance(status_val, str) and status_val.lower() not in ("running", "healthy", ""):
                    await forward_to_langgraph(
                        alertname="TailscaleDown",
                        severity="critical",
                        description="Tailscale service on OPNsense is NOT running. External VPN access is offline.",
                        source="alerting-pipeline"
                    )
        except Exception as e:
            logger.error(f"Tailscale check error: {e}")

        # 3. Cloudflare Tunnel — tunnel status
        try:
            infra_url = MCP_SERVERS.get("infrastructure")
            if infra_url:
                cf_tunnels = await call_mcp_tool(infra_url, "cloudflare_list_tunnels") or {}
                tunnels = cf_tunnels.get("tunnels", cf_tunnels.get("result", []))
                if isinstance(tunnels, list):
                    for tunnel in tunnels:
                        if isinstance(tunnel, dict) and tunnel.get("status", "").lower() not in ("healthy", "active", ""):
                            await forward_to_langgraph(
                                alertname="CloudflareTunnelDown",
                                severity="critical",
                                description=f"Cloudflare tunnel '{tunnel.get('name', 'unknown')}' status: {tunnel.get('status')}. External access may be broken.",
                                source="alerting-pipeline"
                            )
        except Exception as e:
            logger.error(f"Cloudflare tunnel check error: {e}")

        # 4. NFS Ghost Mount — check canary job status
        try:
            infra_url = MCP_SERVERS.get("infrastructure")
            if infra_url:
                jobs = await call_mcp_tool(infra_url, "kubectl_get_jobs", {
                    "namespace": "media",
                    "cluster": "production"
                }) or {}
                job_list = jobs.get("jobs", jobs.get("result", []))
                if isinstance(job_list, list):
                    canary_jobs = [j for j in job_list if isinstance(j, dict) and "mount-canary" in j.get("name", "")]
                    failed_canaries = [j for j in canary_jobs if j.get("failed", 0) > 0]
                    if failed_canaries:
                        await forward_to_langgraph(
                            alertname="StaleStorageMount",
                            severity="critical",
                            description="NFS mount canary FAILED — storage mounts may be disconnected. *arr databases at risk.",
                            source="alerting-pipeline"
                        )
        except Exception as e:
            logger.error(f"NFS canary check error: {e}")

        # 5. ntopng Network Alerts — behavioral host detections only
        # Flow alerts (port scans, protocol mismatches) are too noisy for incidents.
        # Host alerts represent real behavioral detections: scans, floods, dangerous hosts.
        try:
            obs_url = MCP_SERVERS.get("observability")
            if obs_url:
                host_alerts = await call_mcp_tool(obs_url, "ntopng_get_alerts", {"alert_type": "host", "interface": "igc3", "limit": 10})
                host_text = host_alerts.get("text", "")
                if isinstance(host_text, str) and host_text.strip() and "No host alerts found" not in host_text:
                    # Only forward if there are critical or warning host alerts
                    for line in host_text.split("\n"):
                        line_lower = line.lower()
                        if line.startswith("- [") and ("critical" in line_lower or "error" in line_lower):
                            await forward_to_langgraph(
                                alertname="ntopng Host Behavioral Alert",
                                severity="critical" if "critical" in line_lower else "warning",
                                description=f"ntopng host detection on LAN: {line.strip()[:300]}",
                                source="alerting-pipeline"
                            )
                            break  # One alert per poll cycle to avoid flooding
        except Exception as e:
            logger.error(f"ntopng alert check error: {e}")

        # 6. Velero Backup — check recent backup job status
        try:
            infra_url = MCP_SERVERS.get("infrastructure")
            if infra_url:
                jobs = await call_mcp_tool(infra_url, "kubectl_get_jobs", {
                    "namespace": "velero",
                    "cluster": "production"
                }) or {}
                job_list = jobs.get("jobs", jobs.get("result", []))
                if isinstance(job_list, list):
                    failed_backups = [j for j in job_list if isinstance(j, dict) and j.get("failed", 0) > 0]
                    if failed_backups:
                        await forward_to_langgraph(
                            alertname="VeleroBackupFailed",
                            severity="warning",
                            description=f"{len(failed_backups)} Velero backup job(s) failed.",
                            source="alerting-pipeline"
                        )
        except Exception as e:
            logger.error(f"Velero backup check error: {e}")

        # 7. ArgoCD Sync Drift — detect apps that have been OutOfSync too long
        try:
            infra_url = MCP_SERVERS.get("infrastructure")
            if infra_url:
                apps = await call_mcp_tool(infra_url, "argocd_get_applications") or {}
                app_list = apps.get("result", apps) if isinstance(apps, dict) else apps
                if isinstance(app_list, list):
                    for app in app_list:
                        if isinstance(app, dict) and app.get("sync_status") == "OutOfSync":
                            app_name = app.get("name", "unknown")
                            health = app.get("health", "unknown")
                            await forward_to_langgraph(
                                alertname="ArgoCDOutOfSync",
                                severity="warning",
                                description=f"ArgoCD app '{app_name}' is OutOfSync (health: {health}). Git state does not match live cluster.",
                                source="alerting-pipeline"
                            )
        except Exception as e:
            logger.error(f"ArgoCD sync check error: {e}")

    async def check_mcp_service_health():
        """Check if MCP services are reachable."""
        for name, url in MCP_SERVERS.items():
            try:
                async with httpx.AsyncClient(timeout=10.0) as client:
                    for endpoint in ["/health", "/healthz", "/"]:
                        try:
                            response = await client.get(f"{url}{endpoint}")
                            if response.status_code < 500:
                                break
                        except:
                            continue
                    else:
                        await forward_to_langgraph(
                            alertname=f"MCP Service Unreachable: {name}",
                            severity="warning",
                            description=f"MCP server '{name}' at {url} is not responding",
                            source="alerting-pipeline"
                        )
            except Exception as e:
                logger.error(f"Error checking {name}: {e}")

    # ============================================================================
    # Polling Loop
    # ============================================================================

    async def polling_loop():
        """Main polling loop."""
        logger.info(f"Starting polling loop (interval: {POLL_INTERVAL}s)")
        while True:
            try:
                await check_mcp_service_health()
                await run_threshold_checks()
                await run_functional_checks()
            except Exception as e:
                logger.error(f"Polling error: {e}")
            await asyncio.sleep(POLL_INTERVAL)

    @app.on_event("startup")
    async def startup():
        asyncio.create_task(polling_loop())
        logger.info("Alerting pipeline started")

    # ============================================================================
    # API Endpoints
    # ============================================================================

    @app.get("/health")
    async def health():
        return {
            "status": "healthy",
            "active_alerts": len(raised_alerts),
            "mcp_servers": list(MCP_SERVERS.keys())
        }

    @app.post("/check")
    async def trigger_check():
        """Manually trigger a health check."""
        await check_mcp_service_health()
        await run_threshold_checks()
        await run_functional_checks()
        return {"status": "checks_complete"}

    @app.get("/raised")
    async def list_raised():
        """List recently raised alerts."""
        return {
            "alerts": [
                {"key": k, "raised_at": v.isoformat()}
                for k, v in raised_alerts.items()
            ]
        }

    @app.post("/alert")
    async def receive_alertmanager_webhook(request: Request):
        """Receive alerts from Prometheus AlertManager webhook and forward to LangGraph."""
        try:
            payload = await request.json()
        except Exception as e:
            logger.error(f"Failed to parse alert payload: {e}")
            return {"status": "error", "message": str(e)}

        # AlertManager sends: {"receiver": "...", "status": "...", "alerts": [...]}
        # Or sometimes just an array of alerts
        if isinstance(payload, dict) and "alerts" in payload:
            alerts = payload.get("alerts", [])
        elif isinstance(payload, list):
            alerts = payload
        elif isinstance(payload, dict):
            alerts = [payload]
        else:
            logger.error(f"Unexpected payload type: {type(payload)}")
            return {"status": "error", "message": "Invalid payload format"}

        logger.info(f"Received {len(alerts)} alerts from AlertManager")
        processed = 0

        for alert in alerts:
            status = alert.get("status", "firing")
            labels = alert.get("labels", {})
            annotations = alert.get("annotations", {})

            alertname = labels.get("alertname", "Unknown")
            severity = labels.get("severity", "warning")
            namespace = labels.get("namespace", "unknown")

            description = annotations.get("description") or annotations.get("summary") or alertname

            if status == "resolved":
                logger.info(f"Alert resolved: {alertname}")
                alert_key = f"alertmanager:{alertname}:{namespace}"
                raised_alerts.pop(alert_key, None)
                continue

            # Check cooldown
            alert_key = f"alertmanager:{alertname}:{namespace}"
            if alert_key in raised_alerts:
                elapsed = (datetime.utcnow() - raised_alerts[alert_key]).total_seconds()
                if elapsed < ALERT_COOLDOWN_SECONDS:
                    logger.debug(f"Alert '{alertname}' in cooldown ({elapsed:.0f}s)")
                    continue

            logger.info(f"Processing alert: {alertname} ({severity}) in {namespace}")

            try:
                await forward_to_langgraph(
                    alertname=alertname,
                    severity=severity,
                    description=description,
                    source="alertmanager",
                    namespace=namespace
                )
                raised_alerts[alert_key] = datetime.utcnow()
                processed += 1
            except Exception as e:
                logger.error(f"Failed to forward alert {alertname}: {e}")

        return {"status": "ok", "processed": processed, "total": len(alerts)}

    def main():
        port = int(os.environ.get("PORT", "8000"))
        uvicorn.run(app, host="0.0.0.0", port=port)

    if __name__ == "__main__":
        main()

  requirements.txt: |
    fastapi>=0.115.0
    uvicorn>=0.34.0
    httpx>=0.28.0
    pydantic>=2.11.0
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: alerting-pipeline
  namespace: ai-platform
  labels:
    app: alerting-pipeline
spec:
  replicas: 1
  selector:
    matchLabels:
      app: alerting-pipeline
  template:
    metadata:
      labels:
        app: alerting-pipeline
    spec:
      initContainers:
        - name: install-deps
          image: python:3.11-slim
          command: ['sh', '-c', 'pip install --target=/app/deps -r /code/requirements.txt']
          volumeMounts:
            - name: code
              mountPath: /code
            - name: deps
              mountPath: /app/deps
      containers:
        - name: alerting-pipeline
          image: python:3.11-slim
          command: ['sh', '-c', 'PYTHONPATH=/app/deps python /code/main.py']
          ports:
            - containerPort: 8000
              name: http
          env:
            - name: PORT
              value: "8000"
            - name: LANGGRAPH_URL
              value: "http://langgraph:8000"
            - name: INFRASTRUCTURE_MCP_URL
              value: "http://infrastructure-mcp:8000"
            - name: HOME_MCP_URL
              value: "http://home-mcp:8000"
            - name: MEDIA_MCP_URL
              value: "http://media-mcp:8000"
            - name: OBSERVABILITY_MCP_URL
              value: "http://observability-mcp:8000"
            - name: A2A_API_TOKEN
              valueFrom:
                secretKeyRef:
                  name: a2a-api-token
                  key: TOKEN
            - name: POLL_INTERVAL
              value: "300"  # 5 minutes
          volumeMounts:
            - name: code
              mountPath: /code
            - name: deps
              mountPath: /app/deps
          resources:
            requests:
              memory: "128Mi"
              cpu: "50m"
            limits:
              memory: "256Mi"
              cpu: "200m"
          readinessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 30
            periodSeconds: 30
          livenessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 60
            periodSeconds: 60
      volumes:
        - name: code
          configMap:
            name: alerting-pipeline-code
        - name: deps
          emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: alerting-pipeline
  namespace: ai-platform
spec:
  type: ClusterIP
  selector:
    app: alerting-pipeline
  ports:
    - port: 8000
      targetPort: 8000
      name: http
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: alerting-pipeline
  namespace: ai-platform
spec:
  ingressClassName: traefik
  rules:
    - host: alerting-pipeline.agentic.kernow.io
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: alerting-pipeline
                port:
                  number: 8000
