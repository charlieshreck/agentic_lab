---
apiVersion: v1
kind: ConfigMap
metadata:
  name: alerting-pipeline-code
  namespace: ai-platform
data:
  main.py: |
    #!/usr/bin/env python3
    """
    Alerting Pipeline - Polls MCP servers and raises AI alerts.

    This service periodically checks infrastructure health via MCP servers
    and raises alerts to the Matrix bot when issues are detected.
    """
    import os
    import logging
    import asyncio
    from datetime import datetime
    from typing import Dict, List, Any, Optional

    from fastapi import FastAPI
    from pydantic import BaseModel
    import httpx
    import uvicorn

    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

    # Service URLs
    MATRIX_BOT_URL = os.environ.get("MATRIX_BOT_URL", "http://matrix-bot:8000")
    LANGGRAPH_URL = os.environ.get("LANGGRAPH_URL", "http://langgraph:8000")
    LITELLM_URL = os.environ.get("LITELLM_URL", "http://litellm:4000")
    QDRANT_URL = os.environ.get("QDRANT_URL", "http://qdrant:6333")
    LOCAL_MODEL = os.environ.get("LOCAL_MODEL", "local/qwen2.5:7b")

    # MCP Server endpoints
    MCP_SERVERS = {
        "unifi": os.environ.get("UNIFI_MCP_URL", "http://unifi-mcp:8000"),
        "truenas": os.environ.get("TRUENAS_MCP_URL", "http://truenas-mcp:8000"),
        "proxmox": os.environ.get("PROXMOX_MCP_URL", "http://proxmox-mcp:8000"),
        "opnsense": os.environ.get("OPNSENSE_MCP_URL", "http://opnsense-mcp:8000"),
        "adguard": os.environ.get("ADGUARD_MCP_URL", "http://adguard-mcp:8000"),
    }

    # Runbook cache (in-memory for quick lookups)
    runbook_cache: Dict[str, dict] = {}

    # Poll intervals in seconds
    POLL_INTERVAL = int(os.environ.get("POLL_INTERVAL", "300"))  # 5 minutes default
    CRITICAL_POLL_INTERVAL = int(os.environ.get("CRITICAL_POLL_INTERVAL", "60"))  # 1 minute for critical checks

    # Track raised alerts to avoid duplicates
    raised_alerts: Dict[str, datetime] = {}
    ALERT_COOLDOWN_SECONDS = 3600  # 1 hour cooldown before re-alerting on same issue

    app = FastAPI(title="Alerting Pipeline")

    class AlertRule(BaseModel):
        name: str
        source: str
        check_type: str  # threshold, presence, absence
        threshold: Optional[float] = None
        severity: str = "warning"
        description_template: str
        fix_template: Optional[str] = None

    # Define alert rules
    ALERT_RULES = [
        # UniFi alerts
        AlertRule(
            name="UniFi Device Offline",
            source="unifi",
            check_type="device_offline",
            severity="critical",
            description_template="UniFi device '{device_name}' is offline. Last seen: {last_seen}",
            fix_template="Check physical connectivity and power for device '{device_name}'"
        ),
        AlertRule(
            name="UniFi High Client Count",
            source="unifi",
            check_type="threshold",
            threshold=50,
            severity="warning",
            description_template="Access point '{ap_name}' has {client_count} clients connected (threshold: 50)",
            fix_template=None
        ),
        # TrueNAS alerts
        AlertRule(
            name="TrueNAS Pool Degraded",
            source="truenas",
            check_type="pool_health",
            severity="critical",
            description_template="ZFS pool '{pool_name}' is in degraded state: {status}",
            fix_template="Run 'zpool status {pool_name}' to check disk status. Consider replacing failed drives."
        ),
        AlertRule(
            name="TrueNAS Low Disk Space",
            source="truenas",
            check_type="threshold",
            threshold=85,
            severity="warning",
            description_template="Pool '{pool_name}' is at {usage_percent}% capacity",
            fix_template="Review and clean up data or expand storage"
        ),
        # Proxmox alerts
        AlertRule(
            name="Proxmox VM Not Running",
            source="proxmox",
            check_type="vm_status",
            severity="warning",
            description_template="VM '{vm_name}' (ID: {vm_id}) is not running. Status: {status}",
            fix_template="Start the VM using: qm start {vm_id}"
        ),
        AlertRule(
            name="Proxmox High CPU",
            source="proxmox",
            check_type="threshold",
            threshold=90,
            severity="warning",
            description_template="Node '{node_name}' CPU usage at {cpu_percent}%",
            fix_template=None
        ),
        # OPNsense alerts
        AlertRule(
            name="OPNsense Service Down",
            source="opnsense",
            check_type="service_status",
            severity="critical",
            description_template="Service '{service_name}' is not running on OPNsense",
            fix_template="SSH to OPNsense and run: service {service_name} restart"
        ),
        # AdGuard alerts
        AlertRule(
            name="AdGuard High Block Rate",
            source="adguard",
            check_type="threshold",
            threshold=70,
            severity="warning",
            description_template="AdGuard blocking {block_percent}% of queries - may indicate malware activity",
            fix_template="Review blocked domains in AdGuard admin panel"
        ),
    ]

    CLAUDE_AGENT_URL = os.environ.get("CLAUDE_AGENT_URL", "http://claude-agent:8000")

    # ============================================================================
    # LLM Functions (via LiteLLM/Gemini)
    # ============================================================================

    async def query_llm(prompt: str, system_prompt: str = None) -> dict:
        """Query LLM via LiteLLM proxy (routes to Gemini)."""
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})

        async with httpx.AsyncClient(timeout=60.0) as client:
            try:
                response = await client.post(
                    f"{LITELLM_URL}/v1/chat/completions",
                    json={
                        "model": LOCAL_MODEL,
                        "messages": messages,
                        "temperature": 0.3,
                        "max_tokens": 1000
                    }
                )
                if response.status_code == 200:
                    data = response.json()
                    content = data.get("choices", [{}])[0].get("message", {}).get("content", "")
                    return {"result": content}
                return {"error": f"HTTP {response.status_code}: {response.text[:200]}"}
            except Exception as e:
                return {"error": str(e)}

    async def get_embedding(text: str) -> List[float]:
        """Get embedding vector via LiteLLM (Gemini text-embedding-004)."""
        async with httpx.AsyncClient(timeout=30.0) as client:
            try:
                response = await client.post(
                    f"{LITELLM_URL}/v1/embeddings",
                    json={
                        "model": "embeddings",
                        "input": text
                    }
                )
                if response.status_code == 200:
                    data = response.json()
                    return data.get("data", [{}])[0].get("embedding", [])
                return []
            except Exception as e:
                logger.error(f"Embedding error: {e}")
                return []

    # ============================================================================
    # Qdrant Runbook Functions
    # ============================================================================

    async def search_runbooks(query: str, limit: int = 3) -> List[dict]:
        """Search Qdrant for relevant runbooks based on query."""
        embedding = await get_embedding(query)
        if not embedding:
            return []

        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                response = await client.post(
                    f"{QDRANT_URL}/collections/runbooks/points/search",
                    json={
                        "vector": embedding,
                        "limit": limit,
                        "with_payload": True,
                        "score_threshold": 0.7  # Only return good matches
                    }
                )
                if response.status_code == 200:
                    data = response.json()
                    return [
                        {
                            "id": hit.get("id"),
                            "score": hit.get("score"),
                            "name": hit.get("payload", {}).get("name", "Unknown"),
                            "description": hit.get("payload", {}).get("description", ""),
                            "steps": hit.get("payload", {}).get("steps", []),
                            "fix_command": hit.get("payload", {}).get("fix_command", ""),
                            "source": hit.get("payload", {}).get("source", "unknown")
                        }
                        for hit in data.get("result", [])
                    ]
                return []
            except Exception as e:
                logger.error(f"Runbook search error: {e}")
                return []

    async def store_runbook(name: str, description: str, steps: List[str], fix_command: str = None, source: str = "claude") -> bool:
        """Store a new runbook in Qdrant for future local handling."""
        # Generate embedding for the description
        embedding = await get_embedding(f"{name} {description}")
        if not embedding:
            logger.error("Failed to generate embedding for runbook")
            return False

        import uuid as uuid_mod
        point_id = str(uuid_mod.uuid4())

        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                response = await client.put(
                    f"{QDRANT_URL}/collections/runbooks/points",
                    json={
                        "points": [{
                            "id": point_id,
                            "vector": embedding,
                            "payload": {
                                "name": name,
                                "description": description,
                                "steps": steps,
                                "fix_command": fix_command,
                                "source": source,
                                "created_at": datetime.utcnow().isoformat()
                            }
                        }]
                    }
                )
                if response.status_code == 200:
                    logger.info(f"Stored runbook: {name}")
                    return True
                logger.error(f"Failed to store runbook: {response.text}")
                return False
            except Exception as e:
                logger.error(f"Runbook store error: {e}")
                return False

    # ============================================================================
    # Claude Agent Functions (for escalation)
    # ============================================================================

    async def query_via_claude_agent(query: str, timeout: int = 120) -> dict:
        """Query infrastructure via Claude Agent which can access MCP tools."""
        async with httpx.AsyncClient(timeout=30.0) as client:
            try:
                # Submit task
                response = await client.post(
                    f"{CLAUDE_AGENT_URL}/agent/run",
                    json={
                        "prompt": query,
                        "context": {"source": "alerting_pipeline"},
                        "max_turns": 3,
                        "working_directory": "/workspace",
                        "async_mode": True,
                        "timeout": timeout
                    }
                )
                result = response.json()
                if not result.get("success") or not result.get("task_id"):
                    return {"error": result.get("error", "Failed to queue task")}

                task_id = result.get("task_id")

                # Poll for completion
                for _ in range(timeout // 5):
                    await asyncio.sleep(5)
                    status_resp = await client.get(f"{CLAUDE_AGENT_URL}/task/{task_id}")
                    status = status_resp.json()
                    if status.get("status") == "completed":
                        return {"result": status.get("result", "")}
                    elif status.get("status") == "failed":
                        return {"error": status.get("error", "Unknown error")}

                return {"error": "Query timed out"}
            except Exception as e:
                return {"error": str(e)}

    async def forward_to_langgraph(alertname: str, severity: str, description: str,
                                    source: str, namespace: str = "ai-platform"):
        """Forward alert to LangGraph for orchestrated handling."""
        alert_key = f"{source}:{alertname}"
        if alert_key in raised_alerts:
            elapsed = (datetime.utcnow() - raised_alerts[alert_key]).total_seconds()
            if elapsed < ALERT_COOLDOWN_SECONDS:
                logger.debug(f"Alert '{alertname}' still in cooldown ({elapsed:.0f}s)")
                return None

        logger.info(f"Forwarding to LangGraph: {alertname} ({severity})")
        async with httpx.AsyncClient(timeout=60.0) as client:
            try:
                response = await client.post(
                    f"{LANGGRAPH_URL}/alert",
                    json={
                        "id": f"alert-{source}-{datetime.utcnow().strftime('%Y%m%d%H%M%S')}",
                        "alertname": alertname,
                        "severity": severity,
                        "description": description,
                        "namespace": namespace,
                        "labels": {"source": source},
                        "annotations": {}
                    }
                )
                if response.status_code == 200:
                    raised_alerts[alert_key] = datetime.utcnow()
                    result = response.json()
                    logger.info(f"LangGraph processing: {result}")
                    return result
                else:
                    logger.error(f"LangGraph error: {response.text}")
                    return None
            except Exception as e:
                logger.error(f"Error forwarding to LangGraph: {e}")
                return None

    async def raise_alert(name: str, severity: str, description: str,
                          source: str, suggested_fix: str = None, context: str = None):
        """Raise an alert via the Matrix bot service."""
        # Check cooldown
        alert_key = f"{source}:{name}"
        if alert_key in raised_alerts:
            elapsed = (datetime.utcnow() - raised_alerts[alert_key]).total_seconds()
            if elapsed < ALERT_COOLDOWN_SECONDS:
                logger.debug(f"Alert '{name}' still in cooldown ({elapsed:.0f}s)")
                return

        logger.info(f"Raising alert: {name} ({severity})")
        async with httpx.AsyncClient(timeout=30.0) as client:
            try:
                response = await client.post(
                    f"{MATRIX_BOT_URL}/alert",
                    json={
                        "title": name,
                        "severity": severity,
                        "message": description,
                        "suggested_fix": suggested_fix,
                        "source": source,
                        "context": context
                    }
                )
                if response.status_code == 200:
                    raised_alerts[alert_key] = datetime.utcnow()
                    logger.info(f"Alert raised: {response.json()}")
                else:
                    logger.error(f"Failed to raise alert: {response.text}")
            except Exception as e:
                logger.error(f"Error raising alert: {e}")

    async def check_service_connectivity(name: str, url: str) -> bool:
        """Check if a service is reachable."""
        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                # Try common health endpoints
                for endpoint in ["/health", "/healthz", "/", "/sse"]:
                    try:
                        response = await client.get(f"{url}{endpoint}")
                        if response.status_code < 500:
                            return True
                    except:
                        continue
                return False
            except Exception:
                return False

    async def analyze_issue(issue_description: str) -> dict:
        """Use LLM to analyze an issue and determine severity/category."""
        system_prompt = (
            "You are a homelab infrastructure analyst. Analyze the issue and respond in JSON format with these fields: "
            "severity (critical/warning/info), category (network/storage/compute/service), summary (brief description), "
            "needs_escalation (true/false), reason (why escalation is needed or not). "
            "Only set needs_escalation to true if the issue is complex or requires investigation."
        )

        result = await query_llm(issue_description, system_prompt)
        if "error" in result:
            return {"needs_escalation": True, "reason": "LLM unavailable"}

        try:
            import json
            # Try to parse JSON from response
            response = result.get("result", "{}")
            # Handle markdown code blocks
            if "```json" in response:
                response = response.split("```json")[1].split("```")[0]
            elif "```" in response:
                response = response.split("```")[1].split("```")[0]
            return json.loads(response.strip())
        except:
            return {"needs_escalation": True, "reason": "Could not parse local analysis"}

    async def handle_issue_with_runbook(issue: str, runbook: dict) -> bool:
        """Handle a known issue using an existing runbook."""
        logger.info(f"Handling issue with runbook: {runbook.get('name')}")

        # Notify user that we're using an existing runbook
        await raise_alert(
            name=f"Known Issue: {runbook.get('name', 'Unknown')}",
            severity="info",
            description=f"Detected known issue. Applying existing runbook.\n\nIssue: {issue[:200]}",
            source="gemini-llm",
            suggested_fix=runbook.get("fix_command") or "\n".join(runbook.get("steps", [])),
            context=f"Runbook match score: {runbook.get('score', 0):.2f}"
        )
        return True

    async def escalate_to_claude(issue: str, source: str) -> dict:
        """Escalate unknown issue to Claude Agent for investigation and runbook creation."""
        logger.info(f"Escalating to Claude Agent: {issue[:100]}...")

        escalation_prompt = (
            f"Investigate this infrastructure issue and create a runbook for future handling.\n\n"
            f"Issue: {issue}\n"
            f"Source: {source}\n\n"
            f"Please: 1) Analyze the issue, 2) Provide a diagnosis, 3) Suggest specific fix steps, "
            f"4) Format your response so it can be saved as a runbook.\n\n"
            f"End your response with a JSON block containing: runbook_name, runbook_description, "
            f"runbook_steps (array), and fix_command (optional)."
        )

        result = await query_via_claude_agent(escalation_prompt, timeout=300)
        return result

    async def run_deep_health_check():
        """Run health check and forward issues to LangGraph for orchestrated handling."""
        logger.info("Running health check (forwarding to LangGraph)...")

        # Use Claude Agent to gather status from MCP servers, then forward issues to LangGraph
        health_queries = [
            ("UniFi", "network", "Check UniFi network - list any offline devices or issues"),
            ("TrueNAS", "storage", "Check TrueNAS pools - report degraded or low space"),
            ("Proxmox", "compute", "Check Proxmox nodes - report offline or high resource usage"),
        ]

        for source, category, query in health_queries:
            try:
                # Step 1: Get current status via Claude Agent (has MCP access)
                status_result = await query_via_claude_agent(
                    f"Briefly check {source} status. Just list any issues found, nothing else.",
                    timeout=120
                )

                if "error" in status_result:
                    logger.warning(f"{source} status check failed: {status_result['error']}")
                    continue

                status_text = status_result.get("result", "")

                # Step 2: Check if there are any issues
                alert_keywords = ["offline", "degraded", "critical", "error", "failed", "down", "unreachable", "warning"]
                if not any(kw in status_text.lower() for kw in alert_keywords):
                    logger.info(f"{source}: No issues detected")
                    continue

                # Step 3: Determine severity based on keywords
                severity = "warning"
                critical_keywords = ["offline", "degraded", "critical", "failed", "down"]
                if any(kw in status_text.lower() for kw in critical_keywords):
                    severity = "critical"

                # Step 4: Forward to LangGraph for orchestrated handling
                # LangGraph will: assess → search runbooks → generate solutions → request approval
                logger.info(f"{source}: Issue detected, forwarding to LangGraph...")
                result = await forward_to_langgraph(
                    alertname=f"{source} Health Issue",
                    severity=severity,
                    description=status_text,
                    source=source.lower(),
                    namespace=category
                )

                if result:
                    logger.info(f"{source}: LangGraph handling - status: {result.get('status')}, solutions: {result.get('solutions_count', 0)}")
                else:
                    # Fallback to direct Telegram alert if LangGraph fails
                    logger.warning(f"{source}: LangGraph unavailable, sending direct alert")
                    await raise_alert(
                        name=f"{source} Health Issue",
                        severity=severity,
                        description=status_text,
                        source=source.lower()
                    )

            except Exception as e:
                logger.error(f"Health check error for {source}: {e}")

    async def check_mcp_service_health():
        """Check if MCP services are reachable (basic connectivity check)."""
        for name, url in MCP_SERVERS.items():
            try:
                is_healthy = await check_service_connectivity(name, url)
                if not is_healthy:
                    await raise_alert(
                        name=f"MCP Service Unreachable: {name}",
                        severity="warning",
                        description=f"MCP server '{name}' at {url} is not responding",
                        source="alerting-pipeline",
                        suggested_fix=f"Check pod status: kubectl get pods -n ai-platform | grep {name}"
                    )
            except Exception as e:
                logger.error(f"Error checking {name}: {e}")

    # Deep check interval (every 30 minutes by default)
    DEEP_CHECK_INTERVAL = int(os.environ.get("DEEP_CHECK_INTERVAL", "1800"))
    last_deep_check = 0

    async def run_health_checks():
        """Run health checks - basic connectivity checks."""
        logger.info("Running basic health checks...")
        await check_mcp_service_health()
        logger.info("Basic health checks complete")

    async def polling_loop():
        """Main polling loop."""
        global last_deep_check
        import time

        logger.info(f"Starting polling loop (interval: {POLL_INTERVAL}s, deep check: {DEEP_CHECK_INTERVAL}s)")
        while True:
            try:
                # Always run basic connectivity checks
                await run_health_checks()

                # Run deep health check less frequently (uses Claude Agent - expensive)
                current_time = time.time()
                if current_time - last_deep_check >= DEEP_CHECK_INTERVAL:
                    await run_deep_health_check()
                    last_deep_check = current_time
            except Exception as e:
                logger.error(f"Polling error: {e}")
            await asyncio.sleep(POLL_INTERVAL)

    @app.on_event("startup")
    async def startup():
        asyncio.create_task(polling_loop())
        logger.info("Alerting pipeline started")

    @app.get("/health")
    async def health():
        return {
            "status": "healthy",
            "active_alerts": len(raised_alerts),
            "mcp_servers": list(MCP_SERVERS.keys())
        }

    @app.post("/check")
    async def trigger_check():
        """Manually trigger a health check."""
        await run_health_checks()
        return {"status": "checks_complete"}

    @app.get("/raised")
    async def list_raised():
        """List recently raised alerts."""
        return {
            "alerts": [
                {"key": k, "raised_at": v.isoformat()}
                for k, v in raised_alerts.items()
            ]
        }

    def main():
        port = int(os.environ.get("PORT", "8000"))
        uvicorn.run(app, host="0.0.0.0", port=port)

    if __name__ == "__main__":
        main()

  requirements.txt: |
    fastapi>=0.115.0
    uvicorn>=0.34.0
    httpx>=0.28.0
    pydantic>=2.11.0
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: alerting-pipeline
  namespace: ai-platform
  labels:
    app: alerting-pipeline
spec:
  replicas: 1
  selector:
    matchLabels:
      app: alerting-pipeline
  template:
    metadata:
      labels:
        app: alerting-pipeline
    spec:
      initContainers:
        - name: install-deps
          image: python:3.11-slim
          command: ['sh', '-c', 'pip install --target=/app/deps -r /code/requirements.txt']
          volumeMounts:
            - name: code
              mountPath: /code
            - name: deps
              mountPath: /app/deps
      containers:
        - name: alerting-pipeline
          image: python:3.11-slim
          command: ['sh', '-c', 'PYTHONPATH=/app/deps python /code/main.py']
          ports:
            - containerPort: 8000
              name: http
          env:
            - name: PORT
              value: "8000"
            - name: MATRIX_BOT_URL
              value: "http://matrix-bot:8000"
            - name: LANGGRAPH_URL
              value: "http://langgraph:8000"
            - name: UNIFI_MCP_URL
              value: "http://unifi-mcp:8000"
            - name: TRUENAS_MCP_URL
              value: "http://truenas-mcp:8000"
            - name: PROXMOX_MCP_URL
              value: "http://proxmox-mcp:8000"
            - name: OPNSENSE_MCP_URL
              value: "http://opnsense-mcp:8000"
            - name: ADGUARD_MCP_URL
              value: "http://adguard-mcp:8000"
            - name: CLAUDE_AGENT_URL
              value: "http://claude-agent:8000"
            - name: LITELLM_URL
              value: "http://litellm:4000"
            - name: QDRANT_URL
              value: "http://qdrant:6333"
            - name: LOCAL_MODEL
              value: "local/qwen2.5:7b"
            - name: POLL_INTERVAL
              value: "300"  # 5 minutes
            - name: DEEP_CHECK_INTERVAL
              value: "1800"  # 30 minutes
          volumeMounts:
            - name: code
              mountPath: /code
            - name: deps
              mountPath: /app/deps
          resources:
            requests:
              memory: "128Mi"
              cpu: "50m"
            limits:
              memory: "256Mi"
              cpu: "200m"
          readinessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 30
            periodSeconds: 30
          livenessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 60
            periodSeconds: 60
      volumes:
        - name: code
          configMap:
            name: alerting-pipeline-code
        - name: deps
          emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: alerting-pipeline
  namespace: ai-platform
spec:
  selector:
    app: alerting-pipeline
  ports:
    - port: 8000
      targetPort: 8000
      name: http
