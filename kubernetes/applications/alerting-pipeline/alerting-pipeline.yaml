---
apiVersion: v1
kind: ConfigMap
metadata:
  name: alerting-pipeline-code
  namespace: ai-platform
data:
  main.py: |
    #!/usr/bin/env python3
    """
    Alerting Pipeline - Polls MCP servers and raises AI alerts.

    This service periodically checks infrastructure health via MCP servers
    and raises alerts to the Matrix bot when issues are detected.
    """
    import os
    import logging
    import asyncio
    from datetime import datetime
    from typing import Dict, List, Any, Optional

    from fastapi import FastAPI, Request
    from pydantic import BaseModel
    import httpx
    import uvicorn

    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

    # Service URLs
    MATRIX_BOT_URL = os.environ.get("MATRIX_BOT_URL", "http://matrix-bot:8000")
    LANGGRAPH_URL = os.environ.get("LANGGRAPH_URL", "http://langgraph:8000")
    LITELLM_URL = os.environ.get("LITELLM_URL", "http://litellm:4000")
    QDRANT_URL = os.environ.get("QDRANT_URL", "http://qdrant:6333")
    LOCAL_MODEL = os.environ.get("LOCAL_MODEL", "local/qwen2.5:7b")

    # Consolidated domain MCP endpoints
    MCP_SERVERS = {
        "infrastructure": os.environ.get("INFRASTRUCTURE_MCP_URL", "http://infrastructure-mcp:8000"),
        "home": os.environ.get("HOME_MCP_URL", "http://home-mcp:8000"),
        "media": os.environ.get("MEDIA_MCP_URL", "http://media-mcp:8000"),
    }

    # Runbook cache (in-memory for quick lookups)
    runbook_cache: Dict[str, dict] = {}

    # Poll intervals in seconds
    POLL_INTERVAL = int(os.environ.get("POLL_INTERVAL", "300"))  # 5 minutes default
    CRITICAL_POLL_INTERVAL = int(os.environ.get("CRITICAL_POLL_INTERVAL", "60"))  # 1 minute for critical checks

    # Track raised alerts to avoid duplicates
    raised_alerts: Dict[str, datetime] = {}
    ALERT_COOLDOWN_SECONDS = 3600  # 1 hour cooldown before re-alerting on same issue

    app = FastAPI(title="Alerting Pipeline")

    class AlertRule(BaseModel):
        name: str
        source: str
        check_type: str  # threshold, presence, absence
        threshold: Optional[float] = None
        severity: str = "warning"
        description_template: str
        fix_template: Optional[str] = None

    # Define alert rules (updated for consolidated domain MCPs)
    ALERT_RULES = [
        # UniFi alerts (now via home-mcp)
        AlertRule(
            name="UniFi Device Offline",
            source="home",
            check_type="device_offline",
            severity="critical",
            description_template="UniFi device '{device_name}' is offline. Last seen: {last_seen}",
            fix_template="Check physical connectivity and power for device '{device_name}'"
        ),
        AlertRule(
            name="UniFi High Client Count",
            source="home",
            check_type="threshold",
            threshold=50,
            severity="warning",
            description_template="Access point '{ap_name}' has {client_count} clients connected (threshold: 50)",
            fix_template=None
        ),
        # TrueNAS alerts (now via infrastructure-mcp)
        AlertRule(
            name="TrueNAS Pool Degraded",
            source="infrastructure",
            check_type="pool_health",
            severity="critical",
            description_template="ZFS pool '{pool_name}' is in degraded state: {status}",
            fix_template="Run 'zpool status {pool_name}' to check disk status. Consider replacing failed drives."
        ),
        AlertRule(
            name="TrueNAS Low Disk Space",
            source="infrastructure",
            check_type="threshold",
            threshold=85,
            severity="warning",
            description_template="Pool '{pool_name}' is at {usage_percent}% capacity",
            fix_template="Review and clean up data or expand storage"
        ),
        # Proxmox alerts (now via infrastructure-mcp)
        AlertRule(
            name="Proxmox VM Not Running",
            source="infrastructure",
            check_type="vm_status",
            severity="warning",
            description_template="VM '{vm_name}' (ID: {vm_id}) is not running. Status: {status}",
            fix_template="Start the VM using: qm start {vm_id}"
        ),
        AlertRule(
            name="Proxmox High CPU",
            source="infrastructure",
            check_type="threshold",
            threshold=90,
            severity="warning",
            description_template="Node '{node_name}' CPU usage at {cpu_percent}%",
            fix_template=None
        ),
        # OPNsense alerts (now via infrastructure-mcp)
        AlertRule(
            name="OPNsense Service Down",
            source="infrastructure",
            check_type="service_status",
            severity="critical",
            description_template="Service '{service_name}' is not running on OPNsense",
            fix_template="SSH to OPNsense and run: service {service_name} restart"
        ),
        # AdGuard alerts (now via home-mcp)
        AlertRule(
            name="AdGuard High Block Rate",
            source="home",
            check_type="threshold",
            threshold=85,
            severity="warning",
            description_template="AdGuard blocking {block_percent}% of queries - may indicate malware activity",
            fix_template="Review blocked domains in AdGuard admin panel"
        ),
        # Functional health alerts (Gemini Phase 1.5 — "Silent Killers")
        AlertRule(
            name="Media Import Stalled",
            source="media",
            check_type="queue_stall",
            severity="warning",
            description_template="{count} items stuck in *arr download queue"
        ),
        AlertRule(
            name="Tailscale VPN Down",
            source="infrastructure",
            check_type="service_status",
            severity="critical",
            description_template="Tailscale on OPNsense is offline"
        ),
        AlertRule(
            name="Cloudflare Tunnel Unhealthy",
            source="infrastructure",
            check_type="tunnel_health",
            severity="critical",
            description_template="Tunnel '{name}' status: {status}"
        ),
        AlertRule(
            name="NFS Mount Stale",
            source="infrastructure",
            check_type="canary_check",
            severity="critical",
            description_template="NFS mount canary failed — storage disconnected"
        ),
    ]

    CLAUDE_AGENT_URL = os.environ.get("CLAUDE_AGENT_URL", "http://claude-agent:8000")

    # ============================================================================
    # LLM Functions (via LiteLLM/Gemini)
    # ============================================================================

    async def query_llm(prompt: str, system_prompt: str = None) -> dict:
        """Query LLM via LiteLLM proxy (routes to Gemini)."""
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})

        async with httpx.AsyncClient(timeout=60.0) as client:
            try:
                response = await client.post(
                    f"{LITELLM_URL}/v1/chat/completions",
                    json={
                        "model": LOCAL_MODEL,
                        "messages": messages,
                        "temperature": 0.3,
                        "max_tokens": 1000
                    }
                )
                if response.status_code == 200:
                    data = response.json()
                    content = data.get("choices", [{}])[0].get("message", {}).get("content", "")
                    return {"result": content}
                return {"error": f"HTTP {response.status_code}: {response.text[:200]}"}
            except Exception as e:
                return {"error": str(e)}

    async def get_embedding(text: str) -> List[float]:
        """Get embedding vector via LiteLLM (Gemini text-embedding-004)."""
        async with httpx.AsyncClient(timeout=30.0) as client:
            try:
                response = await client.post(
                    f"{LITELLM_URL}/v1/embeddings",
                    json={
                        "model": "embeddings",
                        "input": text
                    }
                )
                if response.status_code == 200:
                    data = response.json()
                    return data.get("data", [{}])[0].get("embedding", [])
                return []
            except Exception as e:
                logger.error(f"Embedding error: {e}")
                return []

    # ============================================================================
    # Qdrant Runbook Functions
    # ============================================================================

    async def search_runbooks(query: str, limit: int = 3) -> List[dict]:
        """Search Qdrant for relevant runbooks based on query."""
        embedding = await get_embedding(query)
        if not embedding:
            return []

        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                response = await client.post(
                    f"{QDRANT_URL}/collections/runbooks/points/search",
                    json={
                        "vector": embedding,
                        "limit": limit,
                        "with_payload": True,
                        "score_threshold": 0.7  # Only return good matches
                    }
                )
                if response.status_code == 200:
                    data = response.json()
                    return [
                        {
                            "id": hit.get("id"),
                            "score": hit.get("score"),
                            "name": hit.get("payload", {}).get("name", "Unknown"),
                            "description": hit.get("payload", {}).get("description", ""),
                            "steps": hit.get("payload", {}).get("steps", []),
                            "fix_command": hit.get("payload", {}).get("fix_command", ""),
                            "source": hit.get("payload", {}).get("source", "unknown")
                        }
                        for hit in data.get("result", [])
                    ]
                return []
            except Exception as e:
                logger.error(f"Runbook search error: {e}")
                return []

    async def store_runbook(name: str, description: str, steps: List[str], fix_command: str = None, source: str = "claude") -> bool:
        """Store a new runbook in Qdrant for future local handling."""
        # Generate embedding for the description
        embedding = await get_embedding(f"{name} {description}")
        if not embedding:
            logger.error("Failed to generate embedding for runbook")
            return False

        import uuid as uuid_mod
        point_id = str(uuid_mod.uuid4())

        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                response = await client.put(
                    f"{QDRANT_URL}/collections/runbooks/points",
                    json={
                        "points": [{
                            "id": point_id,
                            "vector": embedding,
                            "payload": {
                                "name": name,
                                "description": description,
                                "steps": steps,
                                "fix_command": fix_command,
                                "source": source,
                                "created_at": datetime.utcnow().isoformat()
                            }
                        }]
                    }
                )
                if response.status_code == 200:
                    logger.info(f"Stored runbook: {name}")
                    return True
                logger.error(f"Failed to store runbook: {response.text}")
                return False
            except Exception as e:
                logger.error(f"Runbook store error: {e}")
                return False

    # ============================================================================
    # Claude Agent Functions (for escalation)
    # ============================================================================

    async def query_via_claude_agent(query: str, timeout: int = 120) -> dict:
        """Query infrastructure via Claude Agent which can access MCP tools."""
        async with httpx.AsyncClient(timeout=30.0) as client:
            try:
                # Submit task
                response = await client.post(
                    f"{CLAUDE_AGENT_URL}/agent/run",
                    json={
                        "prompt": query,
                        "context": {"source": "alerting_pipeline"},
                        "max_turns": 3,
                        "working_directory": "/workspace",
                        "async_mode": True,
                        "timeout": timeout
                    }
                )
                result = response.json()
                if not result.get("success") or not result.get("task_id"):
                    return {"error": result.get("error", "Failed to queue task")}

                task_id = result.get("task_id")

                # Poll for completion
                for _ in range(timeout // 5):
                    await asyncio.sleep(5)
                    status_resp = await client.get(f"{CLAUDE_AGENT_URL}/task/{task_id}")
                    status = status_resp.json()
                    if status.get("status") == "completed":
                        return {"result": status.get("result", "")}
                    elif status.get("status") == "failed":
                        return {"error": status.get("error", "Unknown error")}

                return {"error": "Query timed out"}
            except Exception as e:
                return {"error": str(e)}

    async def forward_to_langgraph(alertname: str, severity: str, description: str,
                                    source: str, namespace: str = "ai-platform"):
        """Forward alert to LangGraph /ingest endpoint."""
        alert_key = f"{source}:{alertname}"
        if alert_key in raised_alerts:
            elapsed = (datetime.utcnow() - raised_alerts[alert_key]).total_seconds()
            if elapsed < ALERT_COOLDOWN_SECONDS:
                logger.debug(f"Alert '{alertname}' still in cooldown ({elapsed:.0f}s)")
                return None

        logger.info(f"Forwarding to LangGraph /ingest: {alertname} ({severity})")
        async with httpx.AsyncClient(timeout=60.0) as client:
            try:
                response = await client.post(
                    f"{LANGGRAPH_URL}/ingest?source={source}",
                    json={
                        "alert_name": alertname,
                        "severity": severity,
                        "description": description,
                        "labels": {"source": source, "namespace": namespace},
                    }
                )
                if 200 <= response.status_code < 300:
                    raised_alerts[alert_key] = datetime.utcnow()
                    try:
                        result = response.json()
                    except Exception:
                        result = {"status": "accepted", "code": response.status_code}
                    logger.info(f"LangGraph ingest (HTTP {response.status_code}): {result}")
                    return result
                else:
                    logger.error(f"LangGraph ingest error (HTTP {response.status_code}): {response.text}")
                    return None
            except Exception as e:
                logger.error(f"Error forwarding to LangGraph: {e}")
                return None

    async def raise_alert(name: str, severity: str, description: str,
                          source: str, suggested_fix: str = None, context: str = None):
        """Raise an alert via the Matrix bot service."""
        # Check cooldown
        alert_key = f"{source}:{name}"
        if alert_key in raised_alerts:
            elapsed = (datetime.utcnow() - raised_alerts[alert_key]).total_seconds()
            if elapsed < ALERT_COOLDOWN_SECONDS:
                logger.debug(f"Alert '{name}' still in cooldown ({elapsed:.0f}s)")
                return

        logger.info(f"Raising alert: {name} ({severity})")
        async with httpx.AsyncClient(timeout=30.0) as client:
            try:
                response = await client.post(
                    f"{MATRIX_BOT_URL}/alert",
                    json={
                        "title": name,
                        "severity": severity,
                        "message": description,
                        "suggested_fix": suggested_fix,
                        "source": source,
                        "context": context
                    }
                )
                if response.status_code == 200:
                    raised_alerts[alert_key] = datetime.utcnow()
                    logger.info(f"Alert raised: {response.json()}")
                else:
                    logger.error(f"Failed to raise alert: {response.text}")
            except Exception as e:
                logger.error(f"Error raising alert: {e}")

    async def check_service_connectivity(name: str, url: str) -> bool:
        """Check if a service is reachable."""
        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                # Try common health endpoints
                for endpoint in ["/health", "/healthz", "/", "/sse"]:
                    try:
                        response = await client.get(f"{url}{endpoint}")
                        if response.status_code < 500:
                            return True
                    except:
                        continue
                return False
            except Exception:
                return False

    async def analyze_issue(issue_description: str) -> dict:
        """Use LLM to analyze an issue and determine severity/category."""
        system_prompt = (
            "You are a homelab infrastructure analyst. Analyze the issue and respond in JSON format with these fields: "
            "severity (critical/warning/info), category (network/storage/compute/service), summary (brief description), "
            "needs_escalation (true/false), reason (why escalation is needed or not). "
            "Only set needs_escalation to true if the issue is complex or requires investigation."
        )

        result = await query_llm(issue_description, system_prompt)
        if "error" in result:
            return {"needs_escalation": True, "reason": "LLM unavailable"}

        try:
            import json
            # Try to parse JSON from response
            response = result.get("result", "{}")
            # Handle markdown code blocks
            if "```json" in response:
                response = response.split("```json")[1].split("```")[0]
            elif "```" in response:
                response = response.split("```")[1].split("```")[0]
            return json.loads(response.strip())
        except:
            return {"needs_escalation": True, "reason": "Could not parse local analysis"}

    async def handle_issue_with_runbook(issue: str, runbook: dict) -> bool:
        """Handle a known issue using an existing runbook."""
        logger.info(f"Handling issue with runbook: {runbook.get('name')}")

        # Notify user that we're using an existing runbook
        await raise_alert(
            name=f"Known Issue: {runbook.get('name', 'Unknown')}",
            severity="info",
            description=f"Detected known issue. Applying existing runbook.\n\nIssue: {issue[:200]}",
            source="gemini-llm",
            suggested_fix=runbook.get("fix_command") or "\n".join(runbook.get("steps", [])),
            context=f"Runbook match score: {runbook.get('score', 0):.2f}"
        )
        return True

    async def escalate_to_claude(issue: str, source: str) -> dict:
        """Escalate unknown issue to Claude Agent for investigation and runbook creation."""
        logger.info(f"Escalating to Claude Agent: {issue[:100]}...")

        escalation_prompt = (
            f"Investigate this infrastructure issue and create a runbook for future handling.\n\n"
            f"Issue: {issue}\n"
            f"Source: {source}\n\n"
            f"Please: 1) Analyze the issue, 2) Provide a diagnosis, 3) Suggest specific fix steps, "
            f"4) Format your response so it can be saved as a runbook.\n\n"
            f"End your response with a JSON block containing: runbook_name, runbook_description, "
            f"runbook_steps (array), and fix_command (optional)."
        )

        result = await query_via_claude_agent(escalation_prompt, timeout=300)
        return result

    async def call_mcp_tool(mcp_url: str, tool_name: str, params: dict = None) -> dict:
        """Call a tool on a consolidated MCP server."""
        async with httpx.AsyncClient(timeout=30.0) as client:
            try:
                resp = await client.post(
                    f"{mcp_url}/call_tool",
                    json={"tool_name": tool_name, "arguments": params or {}}
                )
                if resp.status_code == 200:
                    return resp.json()
            except Exception as e:
                logger.error(f"MCP tool call error ({tool_name}): {e}")
        return {}

    async def run_threshold_checks():
        """Evaluate ALERT_RULES against live data from consolidated MCPs."""
        logger.info("Running threshold checks...")
        for rule in ALERT_RULES:
            try:
                mcp_url = MCP_SERVERS.get(rule.source)
                if not mcp_url:
                    continue

                if rule.check_type == "device_offline" and rule.source == "home":
                    result = await call_mcp_tool(mcp_url, "unifi_list_devices")
                    if result:
                        devices = result.get("devices", result.get("result", []))
                        if isinstance(devices, list):
                            for device in devices:
                                if isinstance(device, dict) and device.get("state", "") not in ("connected", "1"):
                                    await raise_alert(
                                        name=rule.name,
                                        severity=rule.severity,
                                        description=f"UniFi device '{device.get('name', 'unknown')}' is offline.",
                                        source=rule.source,
                                        suggested_fix=rule.fix_template
                                    )

                elif rule.check_type == "pool_health" and rule.source == "infrastructure":
                    result = await call_mcp_tool(mcp_url, "truenas_list_pools")
                    if result:
                        pools = result.get("pools", result.get("result", []))
                        if isinstance(pools, list):
                            for pool in pools:
                                if isinstance(pool, dict) and pool.get("status", "").upper() not in ("ONLINE", "HEALTHY"):
                                    await raise_alert(
                                        name=rule.name,
                                        severity=rule.severity,
                                        description=f"ZFS pool '{pool.get('name', 'unknown')}' status: {pool.get('status', 'unknown')}",
                                        source=rule.source,
                                        suggested_fix=rule.fix_template
                                    )

                elif rule.check_type == "vm_status" and rule.source == "infrastructure":
                    result = await call_mcp_tool(mcp_url, "proxmox_list_vms")
                    if result:
                        vms = result.get("vms", result.get("result", []))
                        if isinstance(vms, list):
                            for vm in vms:
                                if isinstance(vm, dict) and vm.get("status", "") != "running":
                                    await raise_alert(
                                        name=rule.name,
                                        severity=rule.severity,
                                        description=f"VM '{vm.get('name', 'unknown')}' (ID: {vm.get('vmid', '?')}) status: {vm.get('status', 'unknown')}",
                                        source=rule.source,
                                        suggested_fix=rule.fix_template
                                    )

                elif rule.check_type == "service_status" and rule.source == "infrastructure":
                    result = await call_mcp_tool(mcp_url, "get_services")
                    if result:
                        services = result.get("services", result.get("result", []))
                        if isinstance(services, list):
                            for svc in services:
                                if isinstance(svc, dict) and not svc.get("running", True):
                                    await raise_alert(
                                        name=rule.name,
                                        severity=rule.severity,
                                        description=f"Service '{svc.get('name', 'unknown')}' is not running on OPNsense",
                                        source=rule.source,
                                        suggested_fix=rule.fix_template
                                    )

            except Exception as e:
                logger.error(f"Threshold check error for {rule.name}: {e}")

    async def run_functional_checks():
        """Gemini Phase 1.5: Detect 'Silent Killers' — services that are up but broken."""
        logger.info("Running functional health checks...")

        # 1. Media Import Stall — downloads finish but import stalls
        try:
            media_url = MCP_SERVERS.get("media")
            if media_url:
                sonarr_q = await call_mcp_tool(media_url, "sonarr_get_queue") or {}
                radarr_q = await call_mcp_tool(media_url, "radarr_get_queue") or {}
                sonarr_records = sonarr_q.get("records", sonarr_q.get("result", []))
                radarr_records = radarr_q.get("records", radarr_q.get("result", []))
                if not isinstance(sonarr_records, list):
                    sonarr_records = []
                if not isinstance(radarr_records, list):
                    radarr_records = []
                stalled = [i for i in (sonarr_records + radarr_records)
                           if isinstance(i, dict) and i.get("status", "").lower() in ("warning", "stalled")]
                if stalled:
                    await raise_alert(
                        name="MediaImportStalled",
                        severity="warning",
                        description=f"{len(stalled)} items stuck in download queue (stalled imports detected).",
                        source="media-pipeline"
                    )
        except Exception as e:
            logger.error(f"Media import check error: {e}")

        # 2. Tailscale VPN — OPNsense Tailscale service
        try:
            infra_url = MCP_SERVERS.get("infrastructure")
            if infra_url:
                ts_status = await call_mcp_tool(infra_url, "get_tailscale_status") or {}
                status_val = ts_status.get("status", ts_status.get("result", ""))
                if isinstance(status_val, str) and status_val.lower() not in ("running", "healthy", ""):
                    await raise_alert(
                        name="TailscaleDown",
                        severity="critical",
                        description="Tailscale service on OPNsense is NOT running. External VPN access is offline.",
                        source="network-inspector"
                    )
        except Exception as e:
            logger.error(f"Tailscale check error: {e}")

        # 3. Cloudflare Tunnel — tunnel status
        try:
            infra_url = MCP_SERVERS.get("infrastructure")
            if infra_url:
                cf_tunnels = await call_mcp_tool(infra_url, "cloudflare_list_tunnels") or {}
                tunnels = cf_tunnels.get("tunnels", cf_tunnels.get("result", []))
                if isinstance(tunnels, list):
                    for tunnel in tunnels:
                        if isinstance(tunnel, dict) and tunnel.get("status", "").lower() not in ("healthy", "active", ""):
                            await raise_alert(
                                name="CloudflareTunnelDown",
                                severity="critical",
                                description=f"Cloudflare tunnel '{tunnel.get('name', 'unknown')}' status: {tunnel.get('status')}. External access may be broken.",
                                source="network-inspector"
                            )
        except Exception as e:
            logger.error(f"Cloudflare tunnel check error: {e}")

        # 4. NFS Ghost Mount — check canary job status
        try:
            infra_url = MCP_SERVERS.get("infrastructure")
            if infra_url:
                jobs = await call_mcp_tool(infra_url, "kubectl_get_jobs", {
                    "namespace": "media",
                    "cluster": "production"
                }) or {}
                job_list = jobs.get("jobs", jobs.get("result", []))
                if isinstance(job_list, list):
                    canary_jobs = [j for j in job_list if isinstance(j, dict) and "mount-canary" in j.get("name", "")]
                    failed_canaries = [j for j in canary_jobs if j.get("failed", 0) > 0]
                    if failed_canaries:
                        await raise_alert(
                            name="StaleStorageMount",
                            severity="critical",
                            description="NFS mount canary FAILED — storage mounts may be disconnected. *arr databases at risk.",
                            source="storage-inspector",
                            suggested_fix="Check NFS mounts on TrueNAS HDD (10.10.0.50) and Media (10.10.0.60)."
                        )
        except Exception as e:
            logger.error(f"NFS canary check error: {e}")

        # 5. Velero Backup — check recent backup job status
        try:
            infra_url = MCP_SERVERS.get("infrastructure")
            if infra_url:
                jobs = await call_mcp_tool(infra_url, "kubectl_get_jobs", {
                    "namespace": "velero",
                    "cluster": "production"
                }) or {}
                job_list = jobs.get("jobs", jobs.get("result", []))
                if isinstance(job_list, list):
                    failed_backups = [j for j in job_list if isinstance(j, dict) and j.get("failed", 0) > 0]
                    if failed_backups:
                        await raise_alert(
                            name="VeleroBackupFailed",
                            severity="warning",
                            description=f"{len(failed_backups)} Velero backup job(s) failed.",
                            source="backup-inspector"
                        )
        except Exception as e:
            logger.error(f"Velero backup check error: {e}")

    async def run_deep_health_check():
        """Run health check and forward issues to LangGraph for orchestrated handling."""
        logger.info("Running health check (forwarding to LangGraph)...")

        # Use Claude Agent to gather status from MCP servers, then forward issues to LangGraph
        health_queries = [
            ("Network", "network", "Check UniFi network - list any offline devices or issues"),
            ("Storage", "storage", "Check TrueNAS pools - report degraded or low space"),
            ("Compute", "compute", "Check Proxmox nodes - report offline or high resource usage"),
        ]

        for source, category, query in health_queries:
            try:
                # Step 1: Get current status via Claude Agent (has MCP access)
                status_result = await query_via_claude_agent(
                    f"Briefly check {source} status. Just list any issues found, nothing else.",
                    timeout=120
                )

                if "error" in status_result:
                    logger.warning(f"{source} status check failed: {status_result['error']}")
                    continue

                status_text = status_result.get("result", "")

                # Step 2: Check if there are any issues
                alert_keywords = ["offline", "degraded", "critical", "error", "failed", "down", "unreachable", "warning"]
                if not any(kw in status_text.lower() for kw in alert_keywords):
                    logger.info(f"{source}: No issues detected")
                    continue

                # Step 3: Determine severity based on keywords
                severity = "warning"
                critical_keywords = ["offline", "degraded", "critical", "failed", "down"]
                if any(kw in status_text.lower() for kw in critical_keywords):
                    severity = "critical"

                # Step 4: Forward to LangGraph /ingest for orchestrated handling
                logger.info(f"{source}: Issue detected, forwarding to LangGraph...")
                result = await forward_to_langgraph(
                    alertname=f"{source} Health Issue",
                    severity=severity,
                    description=status_text,
                    source=source.lower(),
                    namespace=category
                )

                if result:
                    logger.info(f"{source}: LangGraph handling - status: {result.get('status')}, solutions: {result.get('solutions_count', 0)}")
                else:
                    # Fallback to direct Telegram alert if LangGraph fails
                    logger.warning(f"{source}: LangGraph unavailable, sending direct alert")
                    await raise_alert(
                        name=f"{source} Health Issue",
                        severity=severity,
                        description=status_text,
                        source=source.lower()
                    )

            except Exception as e:
                logger.error(f"Health check error for {source}: {e}")

    async def check_mcp_service_health():
        """Check if MCP services are reachable (basic connectivity check)."""
        for name, url in MCP_SERVERS.items():
            try:
                is_healthy = await check_service_connectivity(name, url)
                if not is_healthy:
                    await raise_alert(
                        name=f"MCP Service Unreachable: {name}",
                        severity="warning",
                        description=f"MCP server '{name}' at {url} is not responding",
                        source="alerting-pipeline",
                        suggested_fix=f"Check pod status: kubectl get pods -n ai-platform | grep {name}"
                    )
            except Exception as e:
                logger.error(f"Error checking {name}: {e}")

    # Deep check interval (every 30 minutes by default)
    DEEP_CHECK_INTERVAL = int(os.environ.get("DEEP_CHECK_INTERVAL", "1800"))
    last_deep_check = 0

    async def run_health_checks():
        """Run health checks - basic connectivity checks."""
        logger.info("Running basic health checks...")
        await check_mcp_service_health()
        logger.info("Basic health checks complete")

    async def polling_loop():
        """Main polling loop."""
        global last_deep_check
        import time

        logger.info(f"Starting polling loop (interval: {POLL_INTERVAL}s, deep check: {DEEP_CHECK_INTERVAL}s)")
        while True:
            try:
                # Always run basic connectivity checks
                await run_health_checks()

                # Run threshold checks using ALERT_RULES against consolidated MCPs
                await run_threshold_checks()

                # Run functional health checks (Gemini Phase 1.5 - "Silent Killers")
                await run_functional_checks()

                # Run deep health check less frequently (uses Claude Agent - expensive)
                current_time = time.time()
                if current_time - last_deep_check >= DEEP_CHECK_INTERVAL:
                    await run_deep_health_check()
                    last_deep_check = current_time
            except Exception as e:
                logger.error(f"Polling error: {e}")
            await asyncio.sleep(POLL_INTERVAL)

    @app.on_event("startup")
    async def startup():
        asyncio.create_task(polling_loop())
        logger.info("Alerting pipeline started")

    @app.get("/health")
    async def health():
        return {
            "status": "healthy",
            "active_alerts": len(raised_alerts),
            "mcp_servers": list(MCP_SERVERS.keys())
        }

    @app.post("/check")
    async def trigger_check():
        """Manually trigger a health check."""
        await run_health_checks()
        return {"status": "checks_complete"}

    @app.get("/raised")
    async def list_raised():
        """List recently raised alerts."""
        return {
            "alerts": [
                {"key": k, "raised_at": v.isoformat()}
                for k, v in raised_alerts.items()
            ]
        }

    @app.post("/alert")
    async def receive_alertmanager_webhook(request: Request):
        """Receive alerts from Prometheus AlertManager webhook."""
        try:
            payload = await request.json()
        except Exception as e:
            logger.error(f"Failed to parse alert payload: {e}")
            return {"status": "error", "message": str(e)}

        # AlertManager sends: {"receiver": "...", "status": "...", "alerts": [...]}
        # Or sometimes just an array of alerts
        if isinstance(payload, dict) and "alerts" in payload:
            alerts = payload.get("alerts", [])
        elif isinstance(payload, list):
            alerts = payload
        elif isinstance(payload, dict):
            alerts = [payload]  # Single alert as dict
        else:
            logger.error(f"Unexpected payload type: {type(payload)}")
            return {"status": "error", "message": "Invalid payload format"}

        logger.info(f"Received {len(alerts)} alerts from AlertManager")
        processed = 0

        for alert in alerts:
            status = alert.get("status", "firing")
            labels = alert.get("labels", {})
            annotations = alert.get("annotations", {})

            alertname = labels.get("alertname", "Unknown")
            severity = labels.get("severity", "warning")
            namespace = labels.get("namespace", "unknown")
            cluster = labels.get("cluster", "unknown")

            description = annotations.get("description") or annotations.get("summary") or alertname

            # Skip resolved alerts for now
            if status == "resolved":
                logger.info(f"Alert resolved: {alertname}")
                # Remove from raised_alerts if present
                alert_key = f"alertmanager:{alertname}:{namespace}"
                raised_alerts.pop(alert_key, None)
                continue

            # Check cooldown
            alert_key = f"alertmanager:{alertname}:{namespace}"
            if alert_key in raised_alerts:
                elapsed = (datetime.utcnow() - raised_alerts[alert_key]).total_seconds()
                if elapsed < ALERT_COOLDOWN_SECONDS:
                    logger.debug(f"Alert '{alertname}' in cooldown ({elapsed:.0f}s)")
                    continue

            logger.info(f"Processing alert: {alertname} ({severity}) from {cluster}/{namespace}")

            # Forward to LangGraph /ingest for AI-assisted triage
            try:
                await forward_to_langgraph(
                    alertname=alertname,
                    severity=severity,
                    description=description,
                    source="alertmanager",
                    namespace=namespace
                )
                raised_alerts[alert_key] = datetime.utcnow()
                processed += 1
            except Exception as e:
                logger.error(f"Failed to forward alert {alertname}: {e}")
                # Fallback: send directly to Matrix bot
                try:
                    await raise_alert(
                        name=alertname,
                        severity=severity,
                        description=f"[{cluster}/{namespace}] {description}",
                        source="alertmanager",
                        suggested_fix=annotations.get("runbook_url")
                    )
                    raised_alerts[alert_key] = datetime.utcnow()
                    processed += 1
                except Exception as e2:
                    logger.error(f"Fallback alert also failed: {e2}")

        return {"status": "ok", "processed": processed, "total": len(alerts)}

    def main():
        port = int(os.environ.get("PORT", "8000"))
        uvicorn.run(app, host="0.0.0.0", port=port)

    if __name__ == "__main__":
        main()

  requirements.txt: |
    fastapi>=0.115.0
    uvicorn>=0.34.0
    httpx>=0.28.0
    pydantic>=2.11.0
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: alerting-pipeline
  namespace: ai-platform
  labels:
    app: alerting-pipeline
spec:
  replicas: 1
  selector:
    matchLabels:
      app: alerting-pipeline
  template:
    metadata:
      labels:
        app: alerting-pipeline
    spec:
      initContainers:
        - name: install-deps
          image: python:3.11-slim
          command: ['sh', '-c', 'pip install --target=/app/deps -r /code/requirements.txt']
          volumeMounts:
            - name: code
              mountPath: /code
            - name: deps
              mountPath: /app/deps
      containers:
        - name: alerting-pipeline
          image: python:3.11-slim
          command: ['sh', '-c', 'PYTHONPATH=/app/deps python /code/main.py']
          ports:
            - containerPort: 8000
              name: http
          env:
            - name: PORT
              value: "8000"
            - name: MATRIX_BOT_URL
              value: "http://matrix-bot:8000"
            - name: LANGGRAPH_URL
              value: "http://langgraph:8000"
            - name: INFRASTRUCTURE_MCP_URL
              value: "http://infrastructure-mcp:8000"
            - name: HOME_MCP_URL
              value: "http://home-mcp:8000"
            - name: MEDIA_MCP_URL
              value: "http://media-mcp:8000"
            - name: CLAUDE_AGENT_URL
              value: "http://claude-agent:8000"
            - name: LITELLM_URL
              value: "http://litellm:4000"
            - name: QDRANT_URL
              value: "http://qdrant:6333"
            - name: LOCAL_MODEL
              value: "local/qwen2.5:7b"
            - name: POLL_INTERVAL
              value: "300"  # 5 minutes
            - name: DEEP_CHECK_INTERVAL
              value: "1800"  # 30 minutes
          volumeMounts:
            - name: code
              mountPath: /code
            - name: deps
              mountPath: /app/deps
          resources:
            requests:
              memory: "128Mi"
              cpu: "50m"
            limits:
              memory: "256Mi"
              cpu: "200m"
          readinessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 30
            periodSeconds: 30
          livenessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 60
            periodSeconds: 60
      volumes:
        - name: code
          configMap:
            name: alerting-pipeline-code
        - name: deps
          emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: alerting-pipeline
  namespace: ai-platform
spec:
  type: NodePort
  selector:
    app: alerting-pipeline
  ports:
    - port: 8000
      targetPort: 8000
      nodePort: 31102
      name: http
