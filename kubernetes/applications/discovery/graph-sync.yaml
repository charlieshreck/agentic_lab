apiVersion: batch/v1
kind: CronJob
metadata:
  name: graph-sync
  namespace: ai-platform
  labels:
    app: graph-sync
    component: discovery
spec:
  schedule: "15 */4 * * *"  # Every 4 hours, 15 min after discovery
  concurrencyPolicy: Forbid  # Prevent overlapping runs
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 3
  jobTemplate:
    spec:
      backoffLimit: 2
      activeDeadlineSeconds: 240  # 4 minute timeout
      template:
        metadata:
          labels:
            app: graph-sync
        spec:
          restartPolicy: Never
          containers:
          - name: graph-sync
            image: python:3.11-slim
            env:
            - name: NEO4J_URL
              value: "http://neo4j:7474"
            - name: NEO4J_USER
              value: "neo4j"
            - name: NEO4J_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: neo4j-credentials
                  key: NEO4J_PASSWORD
            # Consolidated MCP endpoints (cluster-internal)
            - name: INFRASTRUCTURE_MCP_URL
              value: "http://infrastructure-mcp:8000"
            - name: KNOWLEDGE_MCP_URL
              value: "http://knowledge-mcp:8000"
            - name: OBSERVABILITY_MCP_URL
              value: "http://observability-mcp:8000"
            - name: HOME_MCP_URL
              value: "http://home-mcp:8000"
            - name: GATUS_URL
              value: "http://10.30.0.20:32080"  # Monit cluster NodePort
            resources:
              requests:
                memory: "128Mi"
                cpu: "100m"
              limits:
                memory: "256Mi"
                cpu: "500m"
            command:
            - python
            - -c
            - |
              import os
              import json
              import asyncio
              import logging
              from datetime import datetime, timedelta
              from base64 import b64encode
              from urllib.request import Request, urlopen
              from urllib.error import URLError, HTTPError

              logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
              logger = logging.getLogger(__name__)

              # Configuration
              NEO4J_URL = os.environ.get("NEO4J_URL", "http://neo4j:7474")
              NEO4J_USER = os.environ.get("NEO4J_USER", "neo4j")
              NEO4J_PASSWORD = os.environ.get("NEO4J_PASSWORD", "")  # Don't strip - Neo4j was initialized with newline

              # Consolidated MCP endpoints
              MCP_SERVERS = {
                  "infrastructure": os.environ.get("INFRASTRUCTURE_MCP_URL", "http://infrastructure-mcp:8000"),
                  "knowledge": os.environ.get("KNOWLEDGE_MCP_URL", "http://knowledge-mcp:8000"),
                  "observability": os.environ.get("OBSERVABILITY_MCP_URL", "http://observability-mcp:8000"),
                  "home": os.environ.get("HOME_MCP_URL", "http://home-mcp:8000"),
              }

              # Direct API access (cross-cluster)
              GATUS_URL = os.environ.get("GATUS_URL", "http://10.30.0.20:32080")

              def neo4j_query(cypher: str, params: dict = None) -> dict:
                  """Execute Cypher query via Neo4j HTTP API."""
                  url = f"{NEO4J_URL}/db/neo4j/tx/commit"
                  auth = b64encode(f"{NEO4J_USER}:{NEO4J_PASSWORD}".encode()).decode()

                  body = {
                      "statements": [{
                          "statement": cypher,
                          "parameters": params or {}
                      }]
                  }

                  req = Request(
                      url,
                      data=json.dumps(body).encode(),
                      headers={
                          "Content-Type": "application/json",
                          "Authorization": f"Basic {auth}"
                      },
                      method="POST"
                  )

                  try:
                      with urlopen(req, timeout=30) as resp:
                          return json.loads(resp.read().decode())
                  except (URLError, HTTPError) as e:
                      logger.error(f"Neo4j query failed: {e}")
                      return {"errors": [str(e)]}

              def call_mcp_tool(server: str, tool_name: str, arguments: dict = None) -> dict:
                  """Call MCP tool via JSON-RPC with SSE response parsing."""
                  url = f"{MCP_SERVERS.get(server, '')}/mcp"

                  body = {
                      "jsonrpc": "2.0",
                      "id": 1,
                      "method": "tools/call",
                      "params": {
                          "name": tool_name,
                          "arguments": arguments or {}
                      }
                  }

                  req = Request(
                      url,
                      data=json.dumps(body).encode(),
                      headers={
                          "Content-Type": "application/json",
                          "Accept": "application/json, text/event-stream"
                      },
                      method="POST"
                  )

                  try:
                      with urlopen(req, timeout=30) as resp:
                          raw = resp.read().decode()
                          # Parse SSE format: "event: message\ndata: {...}"
                          for line in raw.split('\n'):
                              if line.startswith('data: '):
                                  result = json.loads(line[6:])  # Skip "data: " prefix
                                  if "result" in result and "content" in result["result"]:
                                      content = result["result"]["content"]
                                      if content and len(content) > 0:
                                          text = content[0].get("text", "{}")
                                          return json.loads(text) if text.startswith(('{', '[')) else {"text": text}
                                  return result.get("result", {})
                          # Fallback: try parsing as plain JSON (non-SSE response)
                          try:
                              result = json.loads(raw)
                              if "result" in result and "content" in result["result"]:
                                  content = result["result"]["content"]
                                  if content and len(content) > 0:
                                      text = content[0].get("text", "{}")
                                      return json.loads(text) if text.startswith(('{', '[')) else {"text": text}
                              return result.get("result", {})
                          except json.JSONDecodeError:
                              pass
                          return {}
                  except (URLError, HTTPError) as e:
                      logger.warning(f"MCP call {server}/{tool_name} failed: {e}")
                      return {}
                  except json.JSONDecodeError as e:
                      logger.warning(f"MCP call {server}/{tool_name} JSON parse failed: {e}")
                      return {}

              def extract_list(response, *keys):
                  """Extract a list from MCP tool response, handling various formats."""
                  if isinstance(response, list):
                      return response
                  if isinstance(response, dict):
                      # Try 'result' key first (common MCP tool wrapper)
                      if "result" in response and isinstance(response["result"], list):
                          return response["result"]
                      # Try specified keys
                      for key in keys:
                          if key in response and isinstance(response[key], list):
                              return response[key]
                  return []

              def call_rest_api(base_url: str, endpoint: str) -> dict:
                  """Call REST API endpoint directly."""
                  url = f"{base_url}{endpoint}"
                  req = Request(url, headers={"Accept": "application/json"}, method="GET")
                  try:
                      with urlopen(req, timeout=30) as resp:
                          return json.loads(resp.read().decode())
                  except (URLError, HTTPError) as e:
                      logger.warning(f"REST call {url} failed: {e}")
                      return {}
                  except json.JSONDecodeError as e:
                      logger.warning(f"REST call {url} JSON parse failed: {e}")
                      return {}

              def sync_proxmox_vms():
                  """Sync VMs from Proxmox to Neo4j."""
                  logger.info("Syncing Proxmox VMs...")

                  # Get VMs list via consolidated infrastructure-mcp
                  vms_response = call_mcp_tool("infrastructure", "proxmox_list_vms", {"params": {"response_format": "json"}})
                  vms = extract_list(vms_response, "vms")

                  if not vms:
                      logger.warning("No VMs returned from Proxmox MCP")
                      return 0

                  count = 0
                  for vm in vms:
                      vmid = vm.get("vmid")
                      name = vm.get("name", f"vm-{vmid}")
                      status = vm.get("status", "unknown")
                      node = vm.get("node", "unknown")

                      # Create VM node
                      cypher = """
                      MERGE (v:VM {vmid: $vmid})
                      SET v.name = $name,
                          v.status = $status,
                          v.node = $node,
                          v.type = $type,
                          v.last_seen = datetime(),
                          v.source = 'proxmox'
                      WITH v
                      MERGE (h:Host {hostname: $node})
                      SET h.type = 'hypervisor'
                      MERGE (h)-[:HOSTS]->(v)
                      RETURN v.vmid
                      """

                      result = neo4j_query(cypher, {
                          "vmid": str(vmid),
                          "name": name,
                          "status": status,
                          "node": node,
                          "type": vm.get("type", "qemu")
                      })

                      if not result.get("errors"):
                          count += 1

                  logger.info(f"Synced {count} VMs from Proxmox")
                  return count

              def sync_unifi_devices():
                  """Sync UniFi network devices to Neo4j."""
                  logger.info("Syncing UniFi devices...")

                  # Get devices (APs, switches) via consolidated home-mcp
                  devices_response = call_mcp_tool("home", "unifi_list_devices")
                  devices = extract_list(devices_response, "devices", "result")

                  count = 0
                  for device in devices:
                      mac = device.get("mac", "").lower()
                      if not mac:
                          continue

                      name = device.get("name", device.get("hostname", f"device-{mac}"))
                      device_type = device.get("type", "unknown")
                      model = device.get("model", "unknown")
                      ip = device.get("ip", "")

                      # Determine node label based on type
                      label = "AccessPoint" if device_type in ["uap", "ap"] else "Switch" if device_type in ["usw", "sw"] else "NetworkDevice"

                      cypher = f"""
                      MERGE (d:{label} {{mac: $mac}})
                      SET d.name = $name,
                          d.model = $model,
                          d.ip = $ip,
                          d.status = $status,
                          d.last_seen = datetime(),
                          d.source = 'unifi'
                      RETURN d.mac
                      """

                      result = neo4j_query(cypher, {
                          "mac": mac,
                          "name": name,
                          "model": model,
                          "ip": ip,
                          "status": device.get("state", "unknown")
                      })

                      if not result.get("errors"):
                          count += 1

                  # Get clients and their AP connections via consolidated home-mcp
                  clients_response = call_mcp_tool("home", "unifi_list_clients")
                  clients = extract_list(clients_response, "clients", "result")

                  for client in clients:
                      mac = client.get("mac", "").lower()
                      ap_mac = client.get("ap_mac", "").lower()

                      if mac and ap_mac:
                          # Create CONNECTED_VIA relationship
                          cypher = """
                          MATCH (h:Host {mac: $mac})
                          MATCH (ap:AccessPoint {mac: $ap_mac})
                          MERGE (h)-[r:CONNECTED_VIA]->(ap)
                          SET r.signal = $signal,
                              r.channel = $channel,
                              r.last_seen = datetime()
                          RETURN type(r)
                          """

                          neo4j_query(cypher, {
                              "mac": mac,
                              "ap_mac": ap_mac,
                              "signal": client.get("signal", 0),
                              "channel": client.get("channel", 0)
                          })

                  logger.info(f"Synced {count} UniFi devices, processed {len(clients)} client connections")
                  return count

              def sync_truenas_storage():
                  """Sync TrueNAS storage to Neo4j."""
                  logger.info("Syncing TrueNAS storage...")

                  # Get pools via consolidated infrastructure-mcp
                  pools_response = call_mcp_tool("infrastructure", "truenas_list_pools", {"params": {"instance": "hdd", "response_format": "json"}})
                  pools = extract_list(pools_response, "pools", "result")

                  count = 0
                  for pool in pools:
                      name = pool.get("name")
                      if not name:
                          continue

                      cypher = """
                      MERGE (p:StoragePool {name: $name})
                      SET p.status = $status,
                          p.size = $size,
                          p.used = $used,
                          p.last_seen = datetime(),
                          p.source = 'truenas'
                      RETURN p.name
                      """

                      result = neo4j_query(cypher, {
                          "name": name,
                          "status": pool.get("status", "unknown"),
                          "size": pool.get("size", 0),
                          "used": pool.get("used", 0)
                      })

                      if not result.get("errors"):
                          count += 1

                  # Get datasets via consolidated infrastructure-mcp
                  datasets_response = call_mcp_tool("infrastructure", "truenas_list_datasets", {"params": {"instance": "hdd", "response_format": "json"}})
                  datasets = extract_list(datasets_response, "datasets", "result")

                  for dataset in datasets:
                      name = dataset.get("name", "")
                      pool_name = name.split("/")[0] if "/" in name else name

                      cypher = """
                      MERGE (d:Dataset {name: $name})
                      SET d.mountpoint = $mountpoint,
                          d.used = $used,
                          d.available = $available,
                          d.last_seen = datetime(),
                          d.source = 'truenas'
                      WITH d
                      MATCH (p:StoragePool {name: $pool_name})
                      MERGE (p)-[:CONTAINS]->(d)
                      RETURN d.name
                      """

                      neo4j_query(cypher, {
                          "name": name,
                          "pool_name": pool_name,
                          "mountpoint": dataset.get("mountpoint", ""),
                          "used": dataset.get("used", 0),
                          "available": dataset.get("available", 0)
                      })

                  # Get shares via consolidated infrastructure-mcp
                  shares_response = call_mcp_tool("infrastructure", "truenas_list_shares", {"params": {"instance": "hdd", "response_format": "json"}})
                  # Shares may return {nfs: [], smb: []} dict, {result: []} wrapper, or flat list
                  if isinstance(shares_response, list):
                      shares = shares_response
                  elif isinstance(shares_response, dict):
                      if "result" in shares_response and isinstance(shares_response["result"], list):
                          shares = shares_response["result"]
                      else:
                          nfs_shares = shares_response.get("nfs", [])
                          smb_shares = shares_response.get("smb", [])
                          shares = nfs_shares + smb_shares
                  else:
                      shares = []

                  for share in shares:
                      path = share.get("path", "")
                      name = share.get("name", path.split("/")[-1] if path else "unknown")

                      cypher = """
                      MERGE (s:Share {path: $path})
                      SET s.name = $name,
                          s.type = $type,
                          s.enabled = $enabled,
                          s.last_seen = datetime(),
                          s.source = 'truenas'
                      RETURN s.path
                      """

                      neo4j_query(cypher, {
                          "path": path,
                          "name": name,
                          "type": share.get("type", "nfs"),
                          "enabled": share.get("enabled", True)
                      })

                  logger.info(f"Synced {count} storage pools, {len(datasets)} datasets, {len(shares)} shares")
                  return count

              def sync_kubernetes_services():
                  """Sync Kubernetes services to Neo4j.

                  Note: infrastructure-mcp only has access to the agentic cluster.
                  Prod cluster services are covered by Coroot's service discovery.
                  """
                  logger.info("Syncing Kubernetes services...")
                  cluster = "agentic"

                  # Get services via consolidated infrastructure-mcp
                  services_response = call_mcp_tool("infrastructure", "kubectl_get_services", {"all_namespaces": True})
                  services = extract_list(services_response, "services", "result")

                  count = 0
                  for svc in services:
                      name = svc.get("name")
                      namespace = svc.get("namespace")

                      if not name or not namespace:
                          continue

                      svc_type = svc.get("type", "ClusterIP")
                      cluster_ip = svc.get("cluster_ip", "")

                      cypher = """
                      MERGE (s:Service {name: $name, namespace: $namespace, cluster: $cluster})
                      SET s.type = $type,
                          s.cluster_ip = $cluster_ip,
                          s.last_seen = datetime(),
                          s.source = 'kubernetes'
                      RETURN s.name
                      """

                      result = neo4j_query(cypher, {
                          "name": name,
                          "namespace": namespace,
                          "cluster": cluster,
                          "type": svc_type,
                          "cluster_ip": cluster_ip
                      })

                      if not result.get("errors"):
                          count += 1

                  # Get pods via consolidated infrastructure-mcp
                  pods_response = call_mcp_tool("infrastructure", "kubectl_get_pods", {"all_namespaces": True})
                  pods = extract_list(pods_response, "pods", "result")

                  pod_count = 0
                  for pod in pods:
                      name = pod.get("name")
                      namespace = pod.get("namespace")

                      if not name or not namespace:
                          continue

                      node_name = pod.get("node", "unknown")
                      phase = pod.get("status", "unknown")

                      cypher = """
                      MERGE (p:Pod {name: $name, namespace: $namespace, cluster: $cluster})
                      SET p.node = $node,
                          p.phase = $phase,
                          p.last_seen = datetime(),
                          p.source = 'kubernetes'
                      WITH p
                      MERGE (h:Host {hostname: $node})
                      MERGE (p)-[:SCHEDULED_ON]->(h)
                      RETURN p.name
                      """

                      result = neo4j_query(cypher, {
                          "name": name,
                          "namespace": namespace,
                          "cluster": cluster,
                          "node": node_name,
                          "phase": phase
                      })

                      if not result.get("errors"):
                          pod_count += 1

                  logger.info(f"Synced {count} services, {pod_count} pods from agentic cluster")
                  return count

              def sync_runbooks():
                  """Link runbooks to alerts in Neo4j."""
                  logger.info("Syncing runbook relationships...")

                  # Get runbooks from knowledge MCP REST API
                  runbooks_response = call_rest_api(MCP_SERVERS["knowledge"], "/api/runbooks?limit=100")
                  runbooks = runbooks_response.get("runbooks", [])

                  count = 0
                  for runbook_entry in runbooks:
                      # /api/runbooks returns flattened: {id, title, trigger_pattern, solution, automation_level, path, domain}
                      qdrant_id = runbook_entry.get("id", "")
                      title = runbook_entry.get("title", "")
                      trigger_pattern = runbook_entry.get("trigger_pattern", "")

                      if not title:
                          continue

                      # Create runbook node
                      cypher = """
                      MERGE (r:RunbookDocument {qdrant_id: $qdrant_id})
                      SET r.title = $title,
                          r.path = $path,
                          r.domain = $domain,
                          r.automation_level = $automation_level,
                          r.trigger_pattern = $trigger_pattern,
                          r.last_seen = datetime(),
                          r.source = 'knowledge'
                      RETURN r.title
                      """

                      result = neo4j_query(cypher, {
                          "qdrant_id": qdrant_id,
                          "title": title,
                          "path": runbook_entry.get("path", ""),
                          "domain": runbook_entry.get("domain", ""),
                          "automation_level": runbook_entry.get("automation_level", "manual"),
                          "trigger_pattern": trigger_pattern
                      })

                      if not result.get("errors"):
                          count += 1

                      # If trigger_pattern looks like an alert name, create RESOLVES relationship
                      if trigger_pattern and not trigger_pattern.startswith("*"):
                          alert_cypher = """
                          MATCH (r:RunbookDocument {qdrant_id: $qdrant_id})
                          MERGE (a:Alert {name: $alert_name})
                          MERGE (r)-[:RESOLVES]->(a)
                          RETURN a.name
                          """

                          neo4j_query(alert_cypher, {
                              "qdrant_id": qdrant_id,
                              "alert_name": trigger_pattern
                          })

                  logger.info(f"Synced {count} runbooks")
                  return count

              def sync_coroot_services():
                  """Sync service health from Coroot to Neo4j."""
                  logger.info("Syncing Coroot service health...")

                  # Get overview from consolidated observability-mcp
                  overview_response = call_mcp_tool("observability", "coroot_get_infrastructure_overview")
                  overview = overview_response

                  if not overview or not isinstance(overview, dict) or "error" in overview:
                      logger.warning(f"Coroot overview unavailable: {overview.get('error', 'no data') if isinstance(overview, dict) else 'unexpected format'}")
                      return 0

                  count = 0
                  services = overview.get("services", {})
                  for service_name, service_data in services.items():
                      # Parse namespace/service from name
                      parts = service_name.split("/")
                      namespace = parts[0] if len(parts) > 1 else "unknown"
                      name = parts[1] if len(parts) > 1 else parts[0]

                      health = service_data.get("health", "unknown")
                      anomaly_count = service_data.get("anomalies", 0)

                      cypher = """
                      MERGE (s:Service {name: $name, namespace: $namespace})
                      SET s.health = $health,
                          s.anomaly_count = $anomaly_count,
                          s.last_health_check = datetime(),
                          s.source = 'coroot'
                      RETURN s.name
                      """

                      result = neo4j_query(cypher, {
                          "name": name,
                          "namespace": namespace,
                          "health": health,
                          "anomaly_count": anomaly_count
                      })

                      if not result.get("errors"):
                          count += 1

                  # Get alerts from consolidated observability-mcp
                  alerts_response = call_mcp_tool("observability", "coroot_get_alerts")
                  alerts = extract_list(alerts_response, "alerts", "result")

                  for alert in alerts:
                      alert_name = alert.get("name", alert.get("alertname", "unknown"))
                      service = alert.get("service", "")
                      status = alert.get("status", "unknown")
                      severity = alert.get("severity", alert.get("labels", {}).get("severity", "info"))

                      cypher = """
                      MERGE (a:Alert {name: $alert_name})
                      SET a.status = $status,
                          a.severity = $severity,
                          a.service = $service,
                          a.last_seen = datetime(),
                          a.source = 'coroot'
                      RETURN a.name
                      """

                      neo4j_query(cypher, {
                          "alert_name": alert_name,
                          "status": status,
                          "severity": severity,
                          "service": service
                      })

                  logger.info(f"Synced {count} services from Coroot, {len(alerts)} alerts")
                  return count

              def sync_gatus_health():
                  """Sync endpoint health from Gatus to Neo4j."""
                  logger.info("Syncing Gatus endpoint health...")

                  # Get endpoint statuses directly from Gatus API
                  response = call_rest_api(GATUS_URL, "/api/v1/endpoints/statuses")

                  if not response or isinstance(response, dict) and "error" in response:
                      logger.warning(f"Gatus unavailable: {response}")
                      return 0

                  count = 0
                  endpoints = response if isinstance(response, list) else []

                  for endpoint in endpoints:
                      name = endpoint.get("name", "unknown")
                      group = endpoint.get("group", "default")
                      key = endpoint.get("key", f"{group}_{name}")

                      # Get latest result
                      results = endpoint.get("results", [])
                      latest = results[-1] if results else {}
                      success = latest.get("success", False)
                      status_code = latest.get("status", 0)
                      response_time = latest.get("duration", 0) / 1000000  # Convert ns to ms
                      timestamp = latest.get("timestamp", "")

                      # Calculate uptime from recent results
                      uptime = 0
                      if results:
                          successful = sum(1 for r in results if r.get("success", False))
                          uptime = round((successful / len(results)) * 100, 2)

                      cypher = """
                      MERGE (e:UptimeMonitor {key: $key})
                      SET e.name = $name,
                          e.group = $group,
                          e.healthy = $healthy,
                          e.status_code = $status_code,
                          e.response_time_ms = $response_time,
                          e.uptime_percent = $uptime,
                          e.last_check = datetime(),
                          e.source = 'gatus'
                      RETURN e.key
                      """

                      result = neo4j_query(cypher, {
                          "key": key,
                          "name": name,
                          "group": group,
                          "healthy": success,
                          "status_code": status_code,
                          "response_time": response_time,
                          "uptime": uptime
                      })

                      if not result.get("errors"):
                          count += 1

                      # Try to link to related Service
                      if group in ["Apps", "Media", "Home Automation", "Monitoring", "Notes"]:
                          cypher_link = """
                          MATCH (e:UptimeMonitor {key: $key})
                          MATCH (s:Service)
                          WHERE toLower(s.name) CONTAINS toLower($name) OR toLower($name) CONTAINS toLower(s.name)
                          MERGE (e)-[:MONITORS]->(s)
                          RETURN type(e)
                          """
                          neo4j_query(cypher_link, {"key": key, "name": name})

                  logger.info(f"Synced {count} endpoints from Gatus")
                  return count

              def sync_ha_areas():
                  """Sync area/location data from Home Assistant to Neo4j.

                  Graceful degradation: If HA-MCP fails, log warning and continue.
                  Discovery/graph-sync should not fail due to HA being unavailable.
                  """
                  logger.info("Syncing Home Assistant areas...")

                  # Get entities from HA via consolidated home-mcp
                  try:
                      entities_response = call_mcp_tool("home", "list_entities")
                      if not entities_response:
                          logger.warning("Home-MCP unavailable or returned empty response, skipping area sync")
                          return 0
                  except Exception as e:
                      logger.warning(f"Home-MCP unavailable, skipping area sync: {e}")
                      return 0

                  entities = extract_list(entities_response, "entities", "result")
                  if not entities:
                      logger.info("No entities returned from HA, skipping area sync")
                      return 0

                  synced_count = 0
                  for entity in entities:
                      area = entity.get("area", entity.get("area_id", ""))
                      # Try to extract IP from entity_id or attributes
                      entity_id = entity.get("entity_id", "")
                      attributes = entity.get("attributes", {})
                      ip = attributes.get("ip", attributes.get("ip_address", ""))

                      # Skip if no area or no IP to match
                      if not area or not ip:
                          continue

                      try:
                          # Update Neo4j Host node with location and create Location relationship
                          cypher = """
                          MATCH (h:Host {ip: $ip})
                          SET h.location = $area
                          WITH h
                          MERGE (loc:Location {name: $area})
                          MERGE (h)-[:LOCATED_IN]->(loc)
                          RETURN h.ip
                          """
                          result = neo4j_query(cypher, {"ip": ip, "area": area})

                          if result.get("results") and result["results"][0].get("data"):
                              synced_count += 1
                              logger.debug(f"Synced location '{area}' for {ip}")
                      except Exception as e:
                          logger.warning(f"Failed to sync area for {ip}: {e}")
                          continue

                  logger.info(f"HA area sync complete: {synced_count} entities updated")
                  return synced_count

              def _extract_count(result):
                  """Extract count from Neo4j query result."""
                  try:
                      return result["results"][0]["data"][0]["row"][0]
                  except (KeyError, IndexError, TypeError):
                      return 0

              def run_lifecycle_management():
                  """Update entity lifecycle status for ALL node types."""
                  logger.info("Running lifecycle management...")

                  # Node types and their stale/archive/delete thresholds
                  lifecycle_config = {
                      "Host": {"stale": "PT8H", "offline": "P2D", "archive": "P14D"},
                      "VM": {"stale": "PT8H", "offline": "P2D", "archive": "P14D"},
                      "Pod": {"delete_after": "P1D"},
                      "Service": {"stale": "PT12H", "offline": "P3D", "delete_after": "P7D"},
                      "Alert": {"delete_after": "P7D"},
                      "UptimeMonitor": {"stale": "PT12H", "delete_after": "P7D"},
                      "RunbookDocument": {"stale": "P1D", "delete_after": "P7D"},
                      "SmartDevice": {"stale": "P1D", "offline": "P7D", "delete_after": "P30D"},
                      "NAS": {"stale": "PT8H", "offline": "P2D"},
                      "Share": {"stale": "P1D", "delete_after": "P14D"},
                      "AccessPoint": {"stale": "PT8H", "offline": "P2D", "archive": "P14D"},
                      "Switch": {"stale": "PT8H", "offline": "P2D", "archive": "P14D"},
                  }

                  total_stale = 0
                  total_offline = 0
                  total_deleted = 0
                  total_archived = 0

                  for label, thresholds in lifecycle_config.items():
                      # Mark stale
                      if "stale" in thresholds:
                          result = neo4j_query(f"""
                              MATCH (n:{label})
                              WHERE n.last_seen < datetime() - duration('{thresholds["stale"]}')
                                AND (n.status IS NULL OR n.status = 'online')
                              SET n.status = 'stale'
                              RETURN count(n) as cnt
                          """)
                          cnt = _extract_count(result)
                          total_stale += cnt

                      # Mark offline
                      if "offline" in thresholds:
                          result = neo4j_query(f"""
                              MATCH (n:{label} {{status: 'stale'}})
                              WHERE n.last_seen < datetime() - duration('{thresholds["offline"]}')
                              SET n.status = 'offline'
                              RETURN count(n) as cnt
                          """)
                          cnt = _extract_count(result)
                          total_offline += cnt

                      # Delete old nodes
                      if "delete_after" in thresholds:
                          result = neo4j_query(f"""
                              MATCH (n:{label})
                              WHERE n.last_seen < datetime() - duration('{thresholds["delete_after"]}')
                              DETACH DELETE n
                              RETURN count(n) as cnt
                          """)
                          cnt = _extract_count(result)
                          total_deleted += cnt
                          if cnt > 0:
                              logger.info(f"  Deleted {cnt} stale {label} nodes")

                      # Archive (relabel) long-offline
                      if "archive" in thresholds:
                          result = neo4j_query(f"""
                              MATCH (n:{label} {{status: 'offline'}})
                              WHERE n.last_seen < datetime() - duration('{thresholds["archive"]}')
                              SET n:Archived{label}
                              REMOVE n:{label}
                              RETURN count(n) as cnt
                          """)
                          cnt = _extract_count(result)
                          total_archived += cnt
                          if cnt > 0:
                              logger.info(f"  Archived {cnt} {label} nodes")

                  # Clean up orphan nodes with no relationships
                  for label in ["Location", "Network"]:
                      result = neo4j_query(f"""
                          MATCH (n:{label})
                          WHERE NOT (n)--()
                          DELETE n
                          RETURN count(n) as cnt
                      """)
                      cnt = _extract_count(result)
                      if cnt > 0:
                          logger.info(f"  Deleted {cnt} orphan {label} nodes")
                          total_deleted += cnt

                  logger.info(f"Lifecycle: {total_stale} stale, {total_offline} offline, {total_deleted} deleted, {total_archived} archived")

              def main():
                  logger.info("Starting graph-sync job")
                  start_time = datetime.now()

                  # Verify Neo4j connection
                  test_result = neo4j_query("RETURN 1 as test")
                  if test_result.get("errors"):
                      logger.error(f"Neo4j connection failed: {test_result}")
                      return 1

                  logger.info("Neo4j connection verified")

                  # Sync from each source
                  results = {}

                  try:
                      results["proxmox_vms"] = sync_proxmox_vms()
                  except Exception as e:
                      logger.error(f"Proxmox sync failed: {e}")
                      results["proxmox_vms"] = 0

                  try:
                      results["unifi_devices"] = sync_unifi_devices()
                  except Exception as e:
                      logger.error(f"UniFi sync failed: {e}")
                      results["unifi_devices"] = 0

                  try:
                      results["truenas_storage"] = sync_truenas_storage()
                  except Exception as e:
                      logger.error(f"TrueNAS sync failed: {e}")
                      results["truenas_storage"] = 0

                  try:
                      results["kubernetes"] = sync_kubernetes_services()
                  except Exception as e:
                      logger.error(f"Kubernetes sync failed: {e}")
                      results["kubernetes"] = 0

                  try:
                      results["runbooks"] = sync_runbooks()
                  except Exception as e:
                      logger.error(f"Runbooks sync failed: {e}")
                      results["runbooks"] = 0

                  try:
                      results["coroot_services"] = sync_coroot_services()
                  except Exception as e:
                      logger.error(f"Coroot sync failed: {e}")
                      results["coroot_services"] = 0

                  try:
                      results["gatus_health"] = sync_gatus_health()
                  except Exception as e:
                      logger.error(f"Gatus sync failed: {e}")
                      results["gatus_health"] = 0

                  try:
                      results["ha_areas"] = sync_ha_areas()
                  except Exception as e:
                      logger.error(f"HA area sync failed: {e}")
                      results["ha_areas"] = 0

                  # Run lifecycle management
                  try:
                      run_lifecycle_management()
                  except Exception as e:
                      logger.error(f"Lifecycle management failed: {e}")

                  elapsed = (datetime.now() - start_time).total_seconds()
                  logger.info(f"Graph sync completed in {elapsed:.1f}s: {results}")
                  return 0

              if __name__ == "__main__":
                  exit(main())
