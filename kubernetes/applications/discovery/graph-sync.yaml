apiVersion: batch/v1
kind: CronJob
metadata:
  name: graph-sync
  namespace: ai-platform
  labels:
    app: graph-sync
    component: discovery
spec:
  schedule: "*/5 * * * *"  # Every 5 minutes
  concurrencyPolicy: Forbid  # Prevent overlapping runs
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 3
  jobTemplate:
    spec:
      backoffLimit: 2
      activeDeadlineSeconds: 240  # 4 minute timeout
      template:
        metadata:
          labels:
            app: graph-sync
        spec:
          restartPolicy: Never
          containers:
          - name: graph-sync
            image: python:3.11-slim
            env:
            - name: NEO4J_URL
              value: "http://neo4j:7474"
            - name: NEO4J_USER
              value: "neo4j"
            - name: NEO4J_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: neo4j-credentials
                  key: NEO4J_PASSWORD
            # MCP endpoints (cluster-internal)
            - name: PROXMOX_MCP_URL
              value: "http://proxmox-mcp:8000"
            - name: UNIFI_MCP_URL
              value: "http://unifi-mcp:8000"
            - name: TRUENAS_MCP_URL
              value: "http://truenas-mcp:8000"
            - name: INFRASTRUCTURE_MCP_URL
              value: "http://infrastructure-mcp:8000"
            - name: KNOWLEDGE_MCP_URL
              value: "http://knowledge-mcp:8000"
            - name: COROOT_MCP_URL
              value: "http://coroot-mcp:8000"
            - name: GATUS_URL
              value: "http://10.30.0.20:32080"  # Monit cluster NodePort
            resources:
              requests:
                memory: "128Mi"
                cpu: "100m"
              limits:
                memory: "256Mi"
                cpu: "500m"
            command:
            - python
            - -c
            - |
              import os
              import json
              import asyncio
              import logging
              from datetime import datetime, timedelta
              from base64 import b64encode
              from urllib.request import Request, urlopen
              from urllib.error import URLError, HTTPError

              logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
              logger = logging.getLogger(__name__)

              # Configuration
              NEO4J_URL = os.environ.get("NEO4J_URL", "http://neo4j:7474")
              NEO4J_USER = os.environ.get("NEO4J_USER", "neo4j")
              NEO4J_PASSWORD = os.environ.get("NEO4J_PASSWORD", "")  # Don't strip - Neo4j was initialized with newline

              # MCP endpoints
              MCP_SERVERS = {
                  "proxmox": os.environ.get("PROXMOX_MCP_URL", "http://proxmox-mcp:8000"),
                  "unifi": os.environ.get("UNIFI_MCP_URL", "http://unifi-mcp:8000"),
                  "truenas": os.environ.get("TRUENAS_MCP_URL", "http://truenas-mcp:8000"),
                  "infrastructure": os.environ.get("INFRASTRUCTURE_MCP_URL", "http://infrastructure-mcp:8000"),
                  "knowledge": os.environ.get("KNOWLEDGE_MCP_URL", "http://knowledge-mcp:8000"),
                  "coroot": os.environ.get("COROOT_MCP_URL", "http://coroot-mcp:8000"),
              }

              # Direct API access (cross-cluster)
              GATUS_URL = os.environ.get("GATUS_URL", "http://10.30.0.20:32080")

              def neo4j_query(cypher: str, params: dict = None) -> dict:
                  """Execute Cypher query via Neo4j HTTP API."""
                  url = f"{NEO4J_URL}/db/neo4j/tx/commit"
                  auth = b64encode(f"{NEO4J_USER}:{NEO4J_PASSWORD}".encode()).decode()

                  body = {
                      "statements": [{
                          "statement": cypher,
                          "parameters": params or {}
                      }]
                  }

                  req = Request(
                      url,
                      data=json.dumps(body).encode(),
                      headers={
                          "Content-Type": "application/json",
                          "Authorization": f"Basic {auth}"
                      },
                      method="POST"
                  )

                  try:
                      with urlopen(req, timeout=30) as resp:
                          return json.loads(resp.read().decode())
                  except (URLError, HTTPError) as e:
                      logger.error(f"Neo4j query failed: {e}")
                      return {"errors": [str(e)]}

              def call_mcp_tool(server: str, tool_name: str, arguments: dict = None) -> dict:
                  """Call MCP tool via JSON-RPC."""
                  url = f"{MCP_SERVERS.get(server, '')}/mcp"

                  body = {
                      "jsonrpc": "2.0",
                      "id": 1,
                      "method": "tools/call",
                      "params": {
                          "name": tool_name,
                          "arguments": arguments or {}
                      }
                  }

                  req = Request(
                      url,
                      data=json.dumps(body).encode(),
                      headers={"Content-Type": "application/json"},
                      method="POST"
                  )

                  try:
                      with urlopen(req, timeout=30) as resp:
                          result = json.loads(resp.read().decode())
                          if "result" in result and "content" in result["result"]:
                              content = result["result"]["content"]
                              if content and len(content) > 0:
                                  text = content[0].get("text", "{}")
                                  return json.loads(text) if text.startswith('{') or text.startswith('[') else {"text": text}
                          return result.get("result", {})
                  except (URLError, HTTPError) as e:
                      logger.warning(f"MCP call {server}/{tool_name} failed: {e}")
                      return {}
                  except json.JSONDecodeError as e:
                      logger.warning(f"MCP call {server}/{tool_name} JSON parse failed: {e}")
                      return {}

              def call_rest_api(base_url: str, endpoint: str) -> dict:
                  """Call REST API endpoint directly."""
                  url = f"{base_url}{endpoint}"
                  req = Request(url, headers={"Accept": "application/json"}, method="GET")
                  try:
                      with urlopen(req, timeout=30) as resp:
                          return json.loads(resp.read().decode())
                  except (URLError, HTTPError) as e:
                      logger.warning(f"REST call {url} failed: {e}")
                      return {}
                  except json.JSONDecodeError as e:
                      logger.warning(f"REST call {url} JSON parse failed: {e}")
                      return {}

              def sync_proxmox_vms():
                  """Sync VMs from Proxmox to Neo4j."""
                  logger.info("Syncing Proxmox VMs...")

                  # Get VMs list
                  vms_response = call_mcp_tool("proxmox", "list_vms")
                  vms = vms_response.get("vms", [])

                  if not vms:
                      logger.warning("No VMs returned from Proxmox MCP")
                      return 0

                  count = 0
                  for vm in vms:
                      vmid = vm.get("vmid")
                      name = vm.get("name", f"vm-{vmid}")
                      status = vm.get("status", "unknown")
                      node = vm.get("node", "unknown")

                      # Create VM node
                      cypher = """
                      MERGE (v:VM {vmid: $vmid})
                      SET v.name = $name,
                          v.status = $status,
                          v.node = $node,
                          v.type = $type,
                          v.last_seen = datetime(),
                          v.source = 'proxmox'
                      WITH v
                      MERGE (h:Host {hostname: $node})
                      SET h.type = 'hypervisor'
                      MERGE (h)-[:HOSTS]->(v)
                      RETURN v.vmid
                      """

                      result = neo4j_query(cypher, {
                          "vmid": str(vmid),
                          "name": name,
                          "status": status,
                          "node": node,
                          "type": vm.get("type", "qemu")
                      })

                      if not result.get("errors"):
                          count += 1

                  logger.info(f"Synced {count} VMs from Proxmox")
                  return count

              def sync_unifi_devices():
                  """Sync UniFi network devices to Neo4j."""
                  logger.info("Syncing UniFi devices...")

                  # Get devices (APs, switches)
                  devices_response = call_mcp_tool("unifi", "list_devices")
                  devices = devices_response.get("devices", [])

                  count = 0
                  for device in devices:
                      mac = device.get("mac", "").lower()
                      if not mac:
                          continue

                      name = device.get("name", device.get("hostname", f"device-{mac}"))
                      device_type = device.get("type", "unknown")
                      model = device.get("model", "unknown")
                      ip = device.get("ip", "")

                      # Determine node label based on type
                      label = "AccessPoint" if device_type in ["uap", "ap"] else "Switch" if device_type in ["usw", "sw"] else "NetworkDevice"

                      cypher = f"""
                      MERGE (d:{label} {{mac: $mac}})
                      SET d.name = $name,
                          d.model = $model,
                          d.ip = $ip,
                          d.status = $status,
                          d.last_seen = datetime(),
                          d.source = 'unifi'
                      RETURN d.mac
                      """

                      result = neo4j_query(cypher, {
                          "mac": mac,
                          "name": name,
                          "model": model,
                          "ip": ip,
                          "status": device.get("state", "unknown")
                      })

                      if not result.get("errors"):
                          count += 1

                  # Get clients and their AP connections
                  clients_response = call_mcp_tool("unifi", "list_clients")
                  clients = clients_response.get("clients", [])

                  for client in clients:
                      mac = client.get("mac", "").lower()
                      ap_mac = client.get("ap_mac", "").lower()

                      if mac and ap_mac:
                          # Create CONNECTED_VIA relationship
                          cypher = """
                          MATCH (h:Host {mac: $mac})
                          MATCH (ap:AccessPoint {mac: $ap_mac})
                          MERGE (h)-[r:CONNECTED_VIA]->(ap)
                          SET r.signal = $signal,
                              r.channel = $channel,
                              r.last_seen = datetime()
                          RETURN type(r)
                          """

                          neo4j_query(cypher, {
                              "mac": mac,
                              "ap_mac": ap_mac,
                              "signal": client.get("signal", 0),
                              "channel": client.get("channel", 0)
                          })

                  logger.info(f"Synced {count} UniFi devices, processed {len(clients)} client connections")
                  return count

              def sync_truenas_storage():
                  """Sync TrueNAS storage to Neo4j."""
                  logger.info("Syncing TrueNAS storage...")

                  # Get pools
                  pools_response = call_mcp_tool("truenas", "list_pools")
                  pools = pools_response.get("pools", [])

                  count = 0
                  for pool in pools:
                      name = pool.get("name")
                      if not name:
                          continue

                      cypher = """
                      MERGE (p:StoragePool {name: $name})
                      SET p.status = $status,
                          p.size = $size,
                          p.used = $used,
                          p.last_seen = datetime(),
                          p.source = 'truenas'
                      RETURN p.name
                      """

                      result = neo4j_query(cypher, {
                          "name": name,
                          "status": pool.get("status", "unknown"),
                          "size": pool.get("size", 0),
                          "used": pool.get("used", 0)
                      })

                      if not result.get("errors"):
                          count += 1

                  # Get datasets
                  datasets_response = call_mcp_tool("truenas", "list_datasets")
                  datasets = datasets_response.get("datasets", [])

                  for dataset in datasets:
                      name = dataset.get("name", "")
                      pool_name = name.split("/")[0] if "/" in name else name

                      cypher = """
                      MERGE (d:Dataset {name: $name})
                      SET d.mountpoint = $mountpoint,
                          d.used = $used,
                          d.available = $available,
                          d.last_seen = datetime(),
                          d.source = 'truenas'
                      WITH d
                      MATCH (p:StoragePool {name: $pool_name})
                      MERGE (p)-[:CONTAINS]->(d)
                      RETURN d.name
                      """

                      neo4j_query(cypher, {
                          "name": name,
                          "pool_name": pool_name,
                          "mountpoint": dataset.get("mountpoint", ""),
                          "used": dataset.get("used", 0),
                          "available": dataset.get("available", 0)
                      })

                  # Get shares
                  shares_response = call_mcp_tool("truenas", "list_shares")
                  shares = shares_response.get("shares", [])

                  for share in shares:
                      path = share.get("path", "")
                      name = share.get("name", path.split("/")[-1] if path else "unknown")

                      cypher = """
                      MERGE (s:Share {path: $path})
                      SET s.name = $name,
                          s.type = $type,
                          s.enabled = $enabled,
                          s.last_seen = datetime(),
                          s.source = 'truenas'
                      RETURN s.path
                      """

                      neo4j_query(cypher, {
                          "path": path,
                          "name": name,
                          "type": share.get("type", "nfs"),
                          "enabled": share.get("enabled", True)
                      })

                  logger.info(f"Synced {count} storage pools, {len(datasets)} datasets, {len(shares)} shares")
                  return count

              def sync_kubernetes_services():
                  """Sync Kubernetes services to Neo4j."""
                  logger.info("Syncing Kubernetes services...")

                  # Get services from infrastructure MCP
                  services_response = call_mcp_tool("infrastructure", "kubectl_get", {
                      "resource": "services",
                      "namespace": "",  # All namespaces
                      "output": "json"
                  })

                  services = []
                  if isinstance(services_response, dict) and "items" in services_response:
                      services = services_response["items"]

                  count = 0
                  for svc in services:
                      metadata = svc.get("metadata", {})
                      name = metadata.get("name")
                      namespace = metadata.get("namespace")

                      if not name or not namespace:
                          continue

                      spec = svc.get("spec", {})
                      svc_type = spec.get("type", "ClusterIP")
                      cluster_ip = spec.get("clusterIP", "")

                      cypher = """
                      MERGE (s:Service {name: $name, namespace: $namespace})
                      SET s.type = $type,
                          s.cluster_ip = $cluster_ip,
                          s.last_seen = datetime(),
                          s.source = 'kubernetes'
                      RETURN s.name
                      """

                      result = neo4j_query(cypher, {
                          "name": name,
                          "namespace": namespace,
                          "type": svc_type,
                          "cluster_ip": cluster_ip
                      })

                      if not result.get("errors"):
                          count += 1

                  # Get pods
                  pods_response = call_mcp_tool("infrastructure", "kubectl_get", {
                      "resource": "pods",
                      "namespace": "",
                      "output": "json"
                  })

                  pods = []
                  if isinstance(pods_response, dict) and "items" in pods_response:
                      pods = pods_response["items"]

                  pod_count = 0
                  for pod in pods:
                      metadata = pod.get("metadata", {})
                      name = metadata.get("name")
                      namespace = metadata.get("namespace")

                      if not name or not namespace:
                          continue

                      spec = pod.get("spec", {})
                      status = pod.get("status", {})
                      node_name = spec.get("nodeName", "unknown")
                      phase = status.get("phase", "unknown")

                      cypher = """
                      MERGE (p:Pod {name: $name, namespace: $namespace})
                      SET p.node = $node,
                          p.phase = $phase,
                          p.last_seen = datetime(),
                          p.source = 'kubernetes'
                      WITH p
                      MERGE (h:Host {hostname: $node})
                      MERGE (p)-[:SCHEDULED_ON]->(h)
                      RETURN p.name
                      """

                      result = neo4j_query(cypher, {
                          "name": name,
                          "namespace": namespace,
                          "node": node_name,
                          "phase": phase
                      })

                      if not result.get("errors"):
                          pod_count += 1

                  logger.info(f"Synced {count} services, {pod_count} pods")
                  return count

              def sync_runbooks():
                  """Link runbooks to alerts in Neo4j."""
                  logger.info("Syncing runbook relationships...")

                  # Get runbooks from knowledge MCP
                  runbooks_response = call_mcp_tool("knowledge", "search_runbooks", {"query": "*", "limit": 100})
                  runbooks = runbooks_response.get("runbooks", [])

                  count = 0
                  for runbook in runbooks:
                      title = runbook.get("title", "")
                      qdrant_id = runbook.get("id", "")
                      resolves_alerts = runbook.get("resolves_alerts", [])

                      if not title:
                          continue

                      # Create runbook node
                      cypher = """
                      MERGE (r:RunbookDocument {qdrant_id: $qdrant_id})
                      SET r.title = $title,
                          r.path = $path,
                          r.automation_level = $automation_level,
                          r.last_seen = datetime(),
                          r.source = 'knowledge'
                      RETURN r.title
                      """

                      result = neo4j_query(cypher, {
                          "qdrant_id": qdrant_id,
                          "title": title,
                          "path": runbook.get("path", ""),
                          "automation_level": runbook.get("automation_level", "manual")
                      })

                      if not result.get("errors"):
                          count += 1

                      # Create RESOLVES relationships
                      for alert_name in resolves_alerts:
                          alert_cypher = """
                          MATCH (r:RunbookDocument {qdrant_id: $qdrant_id})
                          MERGE (a:Alert {name: $alert_name})
                          MERGE (r)-[:RESOLVES]->(a)
                          RETURN a.name
                          """

                          neo4j_query(alert_cypher, {
                              "qdrant_id": qdrant_id,
                              "alert_name": alert_name
                          })

                  logger.info(f"Synced {count} runbooks")
                  return count

              def sync_coroot_services():
                  """Sync service health from Coroot to Neo4j."""
                  logger.info("Syncing Coroot service health...")

                  # Get overview from coroot-mcp REST API
                  overview_response = call_rest_api(MCP_SERVERS["coroot"], "/api/overview")
                  overview = overview_response.get("data", {})

                  if not overview or "error" in overview:
                      logger.warning(f"Coroot overview unavailable: {overview.get('error', 'no data')}")
                      return 0

                  count = 0
                  services = overview.get("services", {})
                  for service_name, service_data in services.items():
                      # Parse namespace/service from name
                      parts = service_name.split("/")
                      namespace = parts[0] if len(parts) > 1 else "unknown"
                      name = parts[1] if len(parts) > 1 else parts[0]

                      health = service_data.get("health", "unknown")
                      anomaly_count = service_data.get("anomalies", 0)

                      cypher = """
                      MERGE (s:Service {name: $name, namespace: $namespace})
                      SET s.health = $health,
                          s.anomaly_count = $anomaly_count,
                          s.last_health_check = datetime(),
                          s.source = 'coroot'
                      RETURN s.name
                      """

                      result = neo4j_query(cypher, {
                          "name": name,
                          "namespace": namespace,
                          "health": health,
                          "anomaly_count": anomaly_count
                      })

                      if not result.get("errors"):
                          count += 1

                  # Get alerts
                  alerts_response = call_rest_api(MCP_SERVERS["coroot"], "/api/alerts")
                  alerts = alerts_response.get("data", [])

                  for alert in alerts:
                      alert_name = alert.get("name", alert.get("alertname", "unknown"))
                      service = alert.get("service", "")
                      status = alert.get("status", "unknown")
                      severity = alert.get("severity", alert.get("labels", {}).get("severity", "info"))

                      cypher = """
                      MERGE (a:Alert {name: $alert_name})
                      SET a.status = $status,
                          a.severity = $severity,
                          a.service = $service,
                          a.last_seen = datetime(),
                          a.source = 'coroot'
                      RETURN a.name
                      """

                      neo4j_query(cypher, {
                          "alert_name": alert_name,
                          "status": status,
                          "severity": severity,
                          "service": service
                      })

                  logger.info(f"Synced {count} services from Coroot, {len(alerts)} alerts")
                  return count

              def sync_gatus_health():
                  """Sync endpoint health from Gatus to Neo4j."""
                  logger.info("Syncing Gatus endpoint health...")

                  # Get endpoint statuses directly from Gatus API
                  response = call_rest_api(GATUS_URL, "/api/v1/endpoints/statuses")

                  if not response or isinstance(response, dict) and "error" in response:
                      logger.warning(f"Gatus unavailable: {response}")
                      return 0

                  count = 0
                  endpoints = response if isinstance(response, list) else []

                  for endpoint in endpoints:
                      name = endpoint.get("name", "unknown")
                      group = endpoint.get("group", "default")
                      key = endpoint.get("key", f"{group}_{name}")

                      # Get latest result
                      results = endpoint.get("results", [])
                      latest = results[-1] if results else {}
                      success = latest.get("success", False)
                      status_code = latest.get("status", 0)
                      response_time = latest.get("duration", 0) / 1000000  # Convert ns to ms
                      timestamp = latest.get("timestamp", "")

                      # Calculate uptime from recent results
                      uptime = 0
                      if results:
                          successful = sum(1 for r in results if r.get("success", False))
                          uptime = round((successful / len(results)) * 100, 2)

                      cypher = """
                      MERGE (e:UptimeMonitor {key: $key})
                      SET e.name = $name,
                          e.group = $group,
                          e.healthy = $healthy,
                          e.status_code = $status_code,
                          e.response_time_ms = $response_time,
                          e.uptime_percent = $uptime,
                          e.last_check = datetime(),
                          e.source = 'gatus'
                      RETURN e.key
                      """

                      result = neo4j_query(cypher, {
                          "key": key,
                          "name": name,
                          "group": group,
                          "healthy": success,
                          "status_code": status_code,
                          "response_time": response_time,
                          "uptime": uptime
                      })

                      if not result.get("errors"):
                          count += 1

                      # Try to link to related Service
                      if group in ["Apps", "Media", "Home Automation", "Monitoring", "Notes"]:
                          cypher_link = """
                          MATCH (e:UptimeMonitor {key: $key})
                          MATCH (s:Service)
                          WHERE toLower(s.name) CONTAINS toLower($name) OR toLower($name) CONTAINS toLower(s.name)
                          MERGE (e)-[:MONITORS]->(s)
                          RETURN type(e)
                          """
                          neo4j_query(cypher_link, {"key": key, "name": name})

                  logger.info(f"Synced {count} endpoints from Gatus")
                  return count

              def run_lifecycle_management():
                  """Update entity lifecycle status."""
                  logger.info("Running lifecycle management...")

                  # Mark entities not seen in 30 minutes as stale
                  stale_result = neo4j_query("""
                      MATCH (h:Host)
                      WHERE h.last_seen < datetime() - duration('PT30M')
                        AND h.status = 'online'
                      SET h.status = 'stale'
                      RETURN count(h) as stale_count
                  """)

                  stale_count = 0
                  if stale_result.get("results"):
                      data = stale_result["results"][0].get("data", [])
                      if data:
                          stale_count = data[0].get("row", [0])[0]

                  # Mark entities stale for 24h as offline
                  offline_result = neo4j_query("""
                      MATCH (h:Host {status: 'stale'})
                      WHERE h.last_seen < datetime() - duration('P1D')
                      SET h.status = 'offline'
                      RETURN count(h) as offline_count
                  """)

                  offline_count = 0
                  if offline_result.get("results"):
                      data = offline_result["results"][0].get("data", [])
                      if data:
                          offline_count = data[0].get("row", [0])[0]

                  # Archive entities offline for 7 days
                  archive_result = neo4j_query("""
                      MATCH (h:Host {status: 'offline'})
                      WHERE h.last_seen < datetime() - duration('P7D')
                      SET h:ArchivedHost
                      REMOVE h:Host
                      RETURN count(h) as archived_count
                  """)

                  archived_count = 0
                  if archive_result.get("results"):
                      data = archive_result["results"][0].get("data", [])
                      if data:
                          archived_count = data[0].get("row", [0])[0]

                  logger.info(f"Lifecycle: {stale_count} marked stale, {offline_count} marked offline, {archived_count} archived")

              def main():
                  logger.info("Starting graph-sync job")
                  start_time = datetime.now()

                  # Verify Neo4j connection
                  test_result = neo4j_query("RETURN 1 as test")
                  if test_result.get("errors"):
                      logger.error(f"Neo4j connection failed: {test_result}")
                      return 1

                  logger.info("Neo4j connection verified")

                  # Sync from each source
                  results = {}

                  try:
                      results["proxmox_vms"] = sync_proxmox_vms()
                  except Exception as e:
                      logger.error(f"Proxmox sync failed: {e}")
                      results["proxmox_vms"] = 0

                  try:
                      results["unifi_devices"] = sync_unifi_devices()
                  except Exception as e:
                      logger.error(f"UniFi sync failed: {e}")
                      results["unifi_devices"] = 0

                  try:
                      results["truenas_storage"] = sync_truenas_storage()
                  except Exception as e:
                      logger.error(f"TrueNAS sync failed: {e}")
                      results["truenas_storage"] = 0

                  try:
                      results["kubernetes"] = sync_kubernetes_services()
                  except Exception as e:
                      logger.error(f"Kubernetes sync failed: {e}")
                      results["kubernetes"] = 0

                  try:
                      results["runbooks"] = sync_runbooks()
                  except Exception as e:
                      logger.error(f"Runbooks sync failed: {e}")
                      results["runbooks"] = 0

                  try:
                      results["coroot_services"] = sync_coroot_services()
                  except Exception as e:
                      logger.error(f"Coroot sync failed: {e}")
                      results["coroot_services"] = 0

                  try:
                      results["gatus_health"] = sync_gatus_health()
                  except Exception as e:
                      logger.error(f"Gatus sync failed: {e}")
                      results["gatus_health"] = 0

                  # Run lifecycle management
                  try:
                      run_lifecycle_management()
                  except Exception as e:
                      logger.error(f"Lifecycle management failed: {e}")

                  elapsed = (datetime.now() - start_time).total_seconds()
                  logger.info(f"Graph sync completed in {elapsed:.1f}s: {results}")
                  return 0

              if __name__ == "__main__":
                  exit(main())
