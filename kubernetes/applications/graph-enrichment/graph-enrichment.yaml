apiVersion: v1
kind: ConfigMap
metadata:
  name: graph-enrichment-code
  namespace: ai-platform
  labels:
    app: graph-enrichment
data:
  enrich.py: |
    #!/usr/bin/env python3
    """
    Graph Enrichment Pipeline

    Creates relationships in Neo4j from multiple data sources:
    - TrueNAS: Shares, Pools, Network access
    - Proxmox: VMs on Nodes
    - Home Assistant: Devices in Rooms
    - Cloudflare: Services exposed via Tunnels
    - Kubernetes: Services, PVCs, mounts
    """
    import os
    import re
    import json
    import logging
    import httpx
    from typing import Optional, Dict, List, Any

    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    logger = logging.getLogger(__name__)

    # Configuration - direct Neo4j HTTP API + consolidated MCPs
    NEO4J_URL = os.environ.get("NEO4J_URL", "http://neo4j:7474")
    NEO4J_USER = os.environ.get("NEO4J_USER", "neo4j")
    NEO4J_PASSWORD = os.environ.get("NEO4J_PASSWORD", "")
    INFRASTRUCTURE_MCP = os.environ.get("INFRASTRUCTURE_MCP_URL", "http://infrastructure-mcp:8000")
    HOME_MCP = os.environ.get("HOME_MCP_URL", "http://home-mcp:8000")

    client = httpx.Client(timeout=30.0)

    def neo4j_query(cypher: str) -> Dict:
        """Execute a read Cypher query via Neo4j HTTP API."""
        try:
            from base64 import b64encode
            auth = b64encode(f"{NEO4J_USER}:{NEO4J_PASSWORD}".encode()).decode()
            resp = client.post(
                f"{NEO4J_URL}/db/neo4j/tx/commit",
                json={"statements": [{"statement": cypher}]},
                headers={"Authorization": f"Basic {auth}"}
            )
            result = resp.json()
            # Convert Neo4j HTTP format to simple format for compatibility
            if result.get("results") and result["results"][0].get("data"):
                rows = [row["row"] for row in result["results"][0]["data"]]
                columns = result["results"][0].get("columns", [])
                return {"status": "ok", "data": rows, "columns": columns}
            return {"status": "ok", "data": []}
        except Exception as e:
            logger.error(f"Neo4j query failed: {e}")
            return {"status": "error", "error": str(e)}

    def neo4j_write(cypher: str) -> bool:
        """Execute a write query via Neo4j HTTP API."""
        try:
            from base64 import b64encode
            auth = b64encode(f"{NEO4J_USER}:{NEO4J_PASSWORD}".encode()).decode()
            resp = client.post(
                f"{NEO4J_URL}/db/neo4j/tx/commit",
                json={"statements": [{"statement": cypher}]},
                headers={"Authorization": f"Basic {auth}"}
            )
            result = resp.json()
            return not result.get("errors")
        except Exception as e:
            logger.error(f"Neo4j write failed: {e}")
            return False

    def call_mcp_tool(base_url: str, tool_name: str, arguments: Dict = None) -> Optional[Dict]:
        """Call MCP tool via JSON-RPC with SSE response parsing."""
        try:
            resp = client.post(
                f"{base_url}/mcp",
                json={
                    "jsonrpc": "2.0",
                    "id": 1,
                    "method": "tools/call",
                    "params": {"name": tool_name, "arguments": arguments or {}}
                },
                headers={"Content-Type": "application/json", "Accept": "application/json, text/event-stream"}
            )
            raw = resp.text
            for line in raw.split('\n'):
                if line.startswith('data: '):
                    result = json.loads(line[6:])
                    if "result" in result and "content" in result["result"]:
                        content = result["result"]["content"]
                        if content and len(content) > 0:
                            text = content[0].get("text", "{}")
                            return json.loads(text) if text.startswith(('{', '[')) else {"text": text}
                    return result.get("result", {})
            try:
                result = json.loads(raw)
                if "result" in result and "content" in result["result"]:
                    content = result["result"]["content"]
                    if content and len(content) > 0:
                        text = content[0].get("text", "{}")
                        return json.loads(text) if text.startswith(('{', '[')) else {"text": text}
                return result.get("result", {})
            except json.JSONDecodeError:
                pass
            return None
        except Exception as e:
            logger.error(f"MCP call {tool_name} failed: {e}")
            return None

    # ============================================================
    # ENRICHMENT FUNCTIONS
    # ============================================================

    def enrich_truenas_shares():
        """Link Shares to TrueNAS instances and accessible networks."""
        logger.info("Enriching TrueNAS shares...")

        for instance in ["hdd", "media"]:
            # Get shares via consolidated infrastructure-mcp
            shares_resp = call_mcp_tool(INFRASTRUCTURE_MCP, "truenas_list_shares", {"instance": instance})
            if not shares_resp:
                logger.warning(f"Could not get shares for {instance}")
                continue

            result = shares_resp

            instance_name = f"TrueNAS-{instance.upper()}"

            # Create TrueNAS instance node
            neo4j_write(f"""
                MERGE (n:NAS {{name: '{instance_name}', instance: '{instance}'}})
                SET n.last_seen = datetime()
            """)

            # Process NFS shares
            for share in result.get("nfs", []):
                share_path = share.get("path", "")
                share_name = share_path.split("/")[-1] if share_path else f"nfs-{share.get('id')}"
                networks = share.get("networks", [])

                # Link share to NAS
                neo4j_write(f"""
                    MATCH (s:Share {{path: '{share_path}'}})
                    MATCH (n:NAS {{name: '{instance_name}'}})
                    MERGE (s)-[:EXPORTED_BY]->(n)
                """)

                # Link share to accessible networks
                for network in networks:
                    net_name = network.replace("/", "_").replace(".", "_")
                    neo4j_write(f"""
                        MERGE (net:Network {{cidr: '{network}', name: '{net_name}'}})
                        WITH net
                        MATCH (s:Share {{path: '{share_path}'}})
                        MERGE (s)-[:ACCESSIBLE_FROM]->(net)
                    """)

                logger.info(f"  Linked share {share_name} to {instance_name}, networks: {networks}")

            # Process SMB shares
            for share in result.get("smb", []):
                share_path = share.get("path", "")
                share_name = share.get("name", "")

                neo4j_write(f"""
                    MERGE (s:Share {{name: '{share_name}', path: '{share_path}', type: 'smb'}})
                    SET s.last_seen = datetime()
                    WITH s
                    MATCH (n:NAS {{name: '{instance_name}'}})
                    MERGE (s)-[:EXPORTED_BY]->(n)
                """)
                logger.info(f"  Linked SMB share {share_name} to {instance_name}")

    def enrich_truenas_pools():
        """Link Pools to TrueNAS instances and their disks."""
        logger.info("Enriching TrueNAS pools...")

        # Get pools via consolidated infrastructure-mcp
        pools_resp = call_mcp_tool(INFRASTRUCTURE_MCP, "truenas_list_pools")
        if not pools_resp:
            logger.warning("Could not get pools from TrueNAS MCP")
            return

        for pool in pools_resp.get("pools", []):
            instance = pool.get("instance", "hdd")
            instance_name = f"TrueNAS-{instance.upper()}"
            pool_name = pool.get("name")
            pool_status = pool.get("status", "UNKNOWN")
            pool_size = pool.get("size", 0) or 0
            pool_free = pool.get("free", 0) or 0

            # Ensure NAS node exists
            neo4j_write(f"""
                MERGE (n:NAS {{name: '{instance_name}', instance: '{instance}'}})
                SET n.last_seen = datetime()
            """)

            # Create/update pool node and link to NAS
            neo4j_write(f"""
                MERGE (p:StoragePool {{name: '{pool_name}'}})
                SET p.status = '{pool_status}',
                    p.size_bytes = {pool_size},
                    p.free_bytes = {pool_free},
                    p.instance = '{instance}',
                    p.last_seen = datetime()
                WITH p
                MATCH (n:NAS {{name: '{instance_name}'}})
                MERGE (p)-[:HOSTED_ON]->(n)
            """)

            logger.info(f"  Linked pool {pool_name} to {instance_name}")

    def enrich_proxmox_vms():
        """Link VMs to Proxmox nodes and create service links."""
        logger.info("Enriching Proxmox VMs...")

        # Get VMs via consolidated infrastructure-mcp
        vms_resp = call_mcp_tool(INFRASTRUCTURE_MCP, "proxmox_list_vms")
        if not vms_resp:
            logger.warning("Could not get VMs from Proxmox MCP")
            return

        # VM name to service mapping
        vm_service_map = {
            "plex": ["plex"],
            "truenas-hdd": ["TrueNAS-HDD"],
            "truenas-media": ["TrueNAS-MEDIA"],
        }

        for vm in vms_resp.get("vms", []):
            vm_name = vm.get("name", "")
            vm_id = vm.get("vmid")
            node = vm.get("node", "")
            status = vm.get("status", "unknown")
            cpus = vm.get("cpus", 0) or 0
            mem_bytes = vm.get("maxmem", 0) or 0
            vm_ip = vm.get("ip", "")

            if not vm_id:
                continue

            # Create/update VM node
            neo4j_write(f"""
                MERGE (v:VM {{vmid: {vm_id}}})
                SET v.name = '{vm_name}',
                    v.status = '{status}',
                    v.cpus = {cpus},
                    v.memory_bytes = {mem_bytes},
                    v.ip = '{vm_ip}',
                    v.last_seen = datetime()
            """)

            # Create Proxmox node and link VM
            neo4j_write(f"""
                MERGE (n:ProxmoxNode {{name: '{node}'}})
                SET n.last_seen = datetime()
                WITH n
                MATCH (v:VM {{vmid: {vm_id}}})
                MERGE (v)-[:RUNS_ON]->(n)
            """)

            # Link VM to services it hosts
            vm_name_lower = vm_name.lower()
            for vm_key, services in vm_service_map.items():
                if vm_key in vm_name_lower:
                    for svc in services:
                        neo4j_write(f"""
                            MATCH (v:VM {{vmid: {vm_id}}})
                            MERGE (s:Service {{name: '{svc}'}})
                            SET s.last_seen = datetime()
                            MERGE (s)-[:RUNS_ON_VM]->(v)
                        """)
                        # If it's a NAS, also link NAS node to VM
                        if "truenas" in vm_key:
                            neo4j_write(f"""
                                MATCH (v:VM {{vmid: {vm_id}}})
                                MATCH (n:NAS {{name: '{svc}'}})
                                MERGE (n)-[:RUNS_ON_VM]->(v)
                            """)
                        logger.info(f"    Linked service {svc} to VM {vm_name}")

            logger.info(f"  Linked VM {vm_name} (ID:{vm_id}) to node {node}")

    def enrich_home_assistant_locations():
        """Link devices to rooms based on Home Assistant entity names."""
        logger.info("Enriching Home Assistant device locations...")

        # Room extraction patterns
        room_patterns = {
            r"living.?room": "living_room",
            r"kitchen": "kitchen",
            r"bedroom": "bedroom",
            r"dining.?room": "dining_room",
            r"play.?room": "play_room",
            r"study": "study",
            r"garage": "garage",
            r"laundry": "laundry",
            r"hall": "hall",
            r"stair": "stairs",
            r"ensuite": "ensuite",
            r"shower": "shower_room",
            r"loo|toilet|cloakroom": "toilet",
            r"patio|door.?light": "entrance",
            r"albie": "albies_room",
            r"vienna": "viennas_room",
        }

        # Process lights and switches via consolidated home-mcp
        for domain in ["light", "switch"]:
            entities_resp = call_mcp_tool(HOME_MCP, "list_entities", {"domain": domain})
            if not entities_resp:
                logger.warning(f"Could not get {domain} entities from Home MCP")
                continue

            entities = entities_resp.get("data", [])
            logger.info(f"  Processing {len(entities)} {domain} entities")

            for entity in entities:
                entity_id = entity.get("entity_id", "")
                name = entity.get("name", "")

                if not name:
                    continue

                # Extract room from name
                name_lower = name.lower()
                room = None
                for pattern, room_name in room_patterns.items():
                    if re.search(pattern, name_lower):
                        room = room_name
                        break

                if room:
                    # Create device node
                    safe_name = name.replace("'", "\\'").replace('"', '\\"')
                    neo4j_write(f"""
                        MERGE (d:SmartDevice {{entity_id: '{entity_id}'}})
                        SET d.name = '{safe_name}',
                            d.type = '{domain}',
                            d.last_seen = datetime()
                    """)

                    # Create room and link
                    neo4j_write(f"""
                        MERGE (r:Location {{name: '{room}'}})
                        SET r.type = 'room', r.last_seen = datetime()
                        WITH r
                        MATCH (d:SmartDevice {{entity_id: '{entity_id}'}})
                        MERGE (d)-[:LOCATED_IN]->(r)
                    """)

                    logger.info(f"    Linked {name} to room {room}")

    def enrich_service_storage():
        """Link services to storage they use (Plex -> Shares, etc.)."""
        logger.info("Enriching service storage relationships...")

        # Known service-to-storage mappings
        service_storage = {
            "plex": ["Pleximetry", "Plexopathy"],
            "sonarr": ["Tarriance"],
            "radarr": ["Tarriance"],
            "transmission": ["Tarriance"],
            "sabnzbd": ["Tarriance"],
            "minio": ["MinIO"],
            "pbs": ["pbs"],
        }

        for service, shares in service_storage.items():
            for share in shares:
                neo4j_write(f"""
                    MERGE (svc:Service {{name: '{service}'}})
                    SET svc.last_seen = datetime()
                    WITH svc
                    MATCH (s:Share {{name: '{share}'}})
                    MERGE (svc)-[:USES_STORAGE]->(s)
                """)
                logger.info(f"  Linked service {service} to share {share}")

    def enrich_network_topology():
        """Create network topology relationships."""
        logger.info("Enriching network topology...")

        # Define networks
        networks = [
            {"name": "prod", "cidr": "10.10.0.0/24", "purpose": "Production services"},
            {"name": "agentic", "cidr": "10.20.0.0/24", "purpose": "AI platform"},
            {"name": "monitoring", "cidr": "10.30.0.0/24", "purpose": "Monitoring stack"},
            {"name": "iot", "cidr": "10.40.0.0/24", "purpose": "IoT devices"},
        ]

        for net in networks:
            neo4j_write(f"""
                MERGE (n:Network {{cidr: '{net["cidr"]}'}})
                SET n.name = '{net["name"]}',
                    n.purpose = '{net["purpose"]}',
                    n.last_seen = datetime()
            """)

        # Link hosts to networks based on IP
        neo4j_write("""
            MATCH (h:Host)
            WHERE h.ip STARTS WITH '10.10.0'
            MATCH (n:Network {name: 'prod'})
            MERGE (h)-[:CONNECTED_TO]->(n)
        """)
        neo4j_write("""
            MATCH (h:Host)
            WHERE h.ip STARTS WITH '10.20.0'
            MATCH (n:Network {name: 'agentic'})
            MERGE (h)-[:CONNECTED_TO]->(n)
        """)
        neo4j_write("""
            MATCH (h:Host)
            WHERE h.ip STARTS WITH '10.30.0'
            MATCH (n:Network {name: 'monitoring'})
            MERGE (h)-[:CONNECTED_TO]->(n)
        """)

    def enrich_plex_full_chain():
        """Create the full Plex relationship chain as a reference example.

        This demonstrates all relationships that should exist:
        - Plex (Service) → USES_STORAGE → Shares
        - Shares → EXPORTED_BY → NAS
        - Shares → ON_POOL → StoragePool
        - NAS → RUNS_ON_VM → VM
        - VM → RUNS_ON → ProxmoxNode
        - VM → CONNECTED_TO → Network
        """
        logger.info("Creating full Plex relationship chain...")

        # 1. Create Plex service node with rich metadata
        neo4j_write("""
            MERGE (svc:Service {name: 'plex'})
            SET svc.type = 'media_server',
                svc.description = 'Plex Media Server',
                svc.url = 'https://plex.kernow.io',
                svc.port = 32400,
                svc.last_seen = datetime()
        """)

        # 2. Plex shares with specific TrueNAS instances
        plex_storage = [
            {"share": "Pleximetry", "nas": "TrueNAS-HDD", "pool": "Taupo", "type": "nfs"},
            {"share": "Plexopathy", "nas": "TrueNAS-MEDIA", "pool": "Truro", "type": "nfs"},
        ]

        for storage in plex_storage:
            share_name = storage["share"]
            nas_name = storage["nas"]
            pool_name = storage["pool"]
            share_type = storage["type"]

            # Create share with metadata
            neo4j_write(f"""
                MERGE (s:Share {{name: '{share_name}'}})
                SET s.type = '{share_type}',
                    s.last_seen = datetime()
            """)

            # Link Plex to share
            neo4j_write(f"""
                MATCH (svc:Service {{name: 'plex'}})
                MATCH (s:Share {{name: '{share_name}'}})
                MERGE (svc)-[:USES_STORAGE]->(s)
            """)

            # Link share to NAS
            neo4j_write(f"""
                MERGE (n:NAS {{name: '{nas_name}'}})
                SET n.last_seen = datetime()
                WITH n
                MATCH (s:Share {{name: '{share_name}'}})
                MERGE (s)-[:EXPORTED_BY]->(n)
            """)

            # Link share to pool
            neo4j_write(f"""
                MERGE (p:StoragePool {{name: '{pool_name}'}})
                SET p.last_seen = datetime()
                WITH p
                MATCH (s:Share {{name: '{share_name}'}})
                MERGE (s)-[:ON_POOL]->(p)
            """)

            # Link pool to NAS
            neo4j_write(f"""
                MATCH (p:StoragePool {{name: '{pool_name}'}})
                MATCH (n:NAS {{name: '{nas_name}'}})
                MERGE (p)-[:HOSTED_ON]->(n)
            """)

            logger.info(f"  {share_name} → {nas_name} → {pool_name}")

        # 3. Link TrueNAS instances to their VMs
        nas_vms = [
            {"nas": "TrueNAS-HDD", "vmid": 500, "vm_name": "truenas-hdd", "node": "Ruapehu"},
            {"nas": "TrueNAS-MEDIA", "vmid": 600, "vm_name": "truenas-media", "node": "Ruapehu"},
        ]

        for item in nas_vms:
            neo4j_write(f"""
                MERGE (v:VM {{vmid: {item['vmid']}}})
                SET v.name = '{item['vm_name']}', v.last_seen = datetime()
                WITH v
                MATCH (n:NAS {{name: '{item['nas']}'}})
                MERGE (n)-[:RUNS_ON_VM]->(v)
            """)

            neo4j_write(f"""
                MERGE (pn:ProxmoxNode {{name: '{item['node']}'}})
                SET pn.last_seen = datetime()
                WITH pn
                MATCH (v:VM {{vmid: {item['vmid']}}})
                MERGE (v)-[:RUNS_ON]->(pn)
            """)

        # 4. Plex VM (ID 450)
        neo4j_write("""
            MERGE (v:VM {vmid: 450})
            SET v.name = 'plex',
                v.ip = '10.10.0.50',
                v.last_seen = datetime()
            WITH v
            MATCH (svc:Service {name: 'plex'})
            MERGE (svc)-[:RUNS_ON_VM]->(v)
        """)

        neo4j_write("""
            MERGE (pn:ProxmoxNode {name: 'Ruapehu'})
            SET pn.last_seen = datetime()
            WITH pn
            MATCH (v:VM {vmid: 450})
            MERGE (v)-[:RUNS_ON]->(pn)
        """)

        # 5. Connect VMs to prod network
        neo4j_write("""
            MERGE (net:Network {name: 'prod', cidr: '10.10.0.0/24'})
            SET net.last_seen = datetime()
            WITH net
            MATCH (v:VM) WHERE v.vmid IN [450, 500, 600]
            MERGE (v)-[:CONNECTED_TO]->(net)
        """)

        # 6. Link NFS shares to prod network (they're accessible from prod)
        neo4j_write("""
            MATCH (net:Network {name: 'prod'})
            MATCH (s:Share) WHERE s.name IN ['Pleximetry', 'Plexopathy']
            MERGE (s)-[:ACCESSIBLE_FROM]->(net)
        """)

        logger.info("  Full Plex chain complete: Service → Shares → NAS → VMs → ProxmoxNode")

    def enrich_alert_runbook_links():
        """Link alerts to their resolving runbooks."""
        logger.info("Enriching alert-runbook links...")

        # Query existing alerts and runbooks
        alerts = neo4j_query("MATCH (a:Alert) RETURN a.name as name")
        runbooks = neo4j_query("MATCH (r:RunbookDocument) RETURN r.title as title, r.path as path")

        if alerts.get("status") != "ok" or runbooks.get("status") != "ok":
            return

        # Simple keyword matching
        for alert_row in alerts.get("data", []):
            alert_name = alert_row[0] if alert_row else None
            if not alert_name:
                continue

            alert_lower = alert_name.lower()
            for runbook_row in runbooks.get("data", []):
                title = runbook_row[0] if runbook_row else ""
                if not title:
                    continue

                title_lower = title.lower()
                # Check for keyword overlap
                alert_words = set(re.findall(r'\w+', alert_lower))
                title_words = set(re.findall(r'\w+', title_lower))

                if len(alert_words & title_words) >= 2:
                    safe_title = title.replace("'", "\\'")
                    safe_alert = alert_name.replace("'", "\\'")
                    neo4j_write(f"""
                        MATCH (a:Alert {{name: '{safe_alert}'}})
                        MATCH (r:RunbookDocument {{title: '{safe_title}'}})
                        MERGE (a)-[:RESOLVED_BY]->(r)
                    """)
                    logger.info(f"  Linked alert '{alert_name}' to runbook '{title}'")

    def main():
        logger.info("=" * 60)
        logger.info("Starting Graph Enrichment Pipeline")
        logger.info("=" * 60)

        # Run all enrichment functions
        enrich_truenas_shares()
        enrich_truenas_pools()
        enrich_proxmox_vms()
        enrich_home_assistant_locations()
        enrich_service_storage()
        enrich_network_topology()
        enrich_plex_full_chain()  # Full Plex example with all relationships
        enrich_alert_runbook_links()

        # Log summary
        summary = neo4j_query("""
            MATCH ()-[r]->()
            RETURN type(r) as rel, count(*) as cnt
            ORDER BY cnt DESC
        """)

        logger.info("=" * 60)
        logger.info("Enrichment Complete - Relationship Summary:")
        if summary.get("status") == "ok":
            for row in summary.get("data", []):
                logger.info(f"  {row[0]}: {row[1]}")
        logger.info("=" * 60)

    if __name__ == "__main__":
        main()

  requirements.txt: |
    httpx>=0.28.0
---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: graph-enrichment
  namespace: ai-platform
  labels:
    app: graph-enrichment
spec:
  schedule: "45 */4 * * *"  # Every 4 hours, 45 min after discovery
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 3
  jobTemplate:
    spec:
      backoffLimit: 2
      template:
        metadata:
          labels:
            app: graph-enrichment
        spec:
          restartPolicy: OnFailure
          initContainers:
            - name: install-deps
              image: python:3.12-slim
              command:
                - sh
                - -c
                - pip install --target=/app/deps -r /code/requirements.txt
              volumeMounts:
                - name: code
                  mountPath: /code
                - name: deps
                  mountPath: /app/deps
          containers:
            - name: enrichment
              image: python:3.12-slim
              command:
                - sh
                - -c
                - PYTHONPATH=/app/deps python /code/enrich.py
              env:
                - name: NEO4J_URL
                  value: "http://neo4j:7474"
                - name: NEO4J_USER
                  value: "neo4j"
                - name: NEO4J_PASSWORD
                  valueFrom:
                    secretKeyRef:
                      name: neo4j-credentials
                      key: NEO4J_PASSWORD
                - name: INFRASTRUCTURE_MCP_URL
                  value: "http://infrastructure-mcp:8000"
                - name: HOME_MCP_URL
                  value: "http://home-mcp:8000"
              volumeMounts:
                - name: code
                  mountPath: /code
                - name: deps
                  mountPath: /app/deps
              resources:
                requests:
                  memory: "128Mi"
                  cpu: "50m"
                limits:
                  memory: "512Mi"
                  cpu: "500m"
          volumes:
            - name: code
              configMap:
                name: graph-enrichment-code
            - name: deps
              emptyDir: {}
